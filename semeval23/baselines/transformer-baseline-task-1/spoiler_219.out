Thu Aug  3 21:00:02 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100 80G...  On   | 00000000:C1:00.0 Off |                    0 |
| N/A   31C    P0    42W / 300W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
08/03/2023 21:00:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
08/03/2023 21:00:10 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-06,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=../../output/task1-deberta-v2-xlarge-mnli-4/runs/Aug03_21-00-10_gpu-pr1-02,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=8.0,
optim=adamw_torch,
optim_args=None,
output_dir=../../output/task1-deberta-v2-xlarge-mnli-4,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=task1-deberta-v2-xlarge-mnli-4,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=epoch,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:715] 2023-08-03 21:00:11,085 >> loading configuration file config.json from cache at /work/y53kang/.cache/huggingface/hub/models--microsoft--deberta-v2-xlarge-mnli/snapshots/5272422ce68b8d61766079390b96b033a64414d2/config.json
[INFO|configuration_utils.py:771] 2023-08-03 21:00:11,095 >> Model config DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge-mnli",
  "architectures": [
    "DebertaV2ForSequenceClassification"
  ],
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.1,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "finetuning_task": "text-classification",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1536,
  "id2label": {
    "0": "CONTRADICTION",
    "1": "NEUTRAL",
    "2": "ENTAILMENT"
  },
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "label2id": {
    "CONTRADICTION": 0,
    "ENTAILMENT": 2,
    "NEUTRAL": 1
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pooling": {
    "dropout": 0,
    "hidden_act": "gelu"
  },
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.32.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

{'id': 10, 'context': 'Analysis: This may be the most brutal number in the CBO reportThis may be the most brutal number in the CBO report - Plenty has been made of the big Congressional Budget Office finding that 24 million people could lose their insurance under Republicans\' Obamacare replacement over the next decade. That\'s higher than expected and poses a clear and massive hurdle for Republicans as they attempt to convince dozens of skeptical members. But there\'s another number that paints a particularly dire picture for the GOP\'s alternative — especially in light of President Trump\'s populist rhetoric. According to the CBO, 64-year olds making $26,500 per year would see their premiums increase by an estimated 750 percent by 2026. While they are on track to pay $1,700 under the current law, the CBO projects the American Health Care Act would force them to pay $14,600. Even if you grant that inflation will allow them to make slightly more money by 2026, that\'s still about half of their income going to health care. Here\'s how that looks as a percentage of income (h/t Philip Bump): As skeptics of the law noted, that suggests the reason premiums as a whole will eventually decline is because older, poorer people simply won\'t be able to afford it. The legislation reduces premiums substantially for younger people but increases them substantially for older people — and especially poorer, older people — according to the CBO. The CBO\'s estimate of the premium increases for older, poorer Americans is actually worse than a previous one from AARP. Here\'s what AARP, which has announced its opposition to the bill, said last week: ... Our estimates find that, taken together, premiums for older adults could increase by as much as $3,600 for a 55-year old earning $25,000 a year, $7,000 for a 64-year old earning $25,000 a year and up to $8,400 for a 64-year old earning $15,000 a year. If you\'re a Republican looking at these numbers, you have to be concerned — just from a self-preservation standpoint. Republican leaders have made great pains to argue that the CBO\'s estimate of the millions who will lose insurance is faulty, or even that it\'s a necessary side effect of returning to a more free-market approach to health care. But the GOP\'s counter-argument is predicated on the idea that its alternative would at least allow people access to coverage. Paying such a substantial portion of one\'s income on health insurance doesn\'t meet that goal — if, in fact, the CBO\'s estimate is anywhere close to accurate. It also affects a group of voters who are integral to the Trump Coalition. Less-formally educated and lower-income white Americans were the backbone of the electoral shift that allowed President Trump to be elected, and he has promised them the world: Coverage that is even more affordable than the Affordable Care Act and "insurance for everybody." On top of all that, older people are much more likely to vote than younger people, especially in a midterm election like 2018. So basically, Trump\'s win was built on older, poorer people, for whom this law appears to drive up premiums and drive down the insurance rate, while benefiting the younger and wealthier. That\'s not something that will assure skeptical Republicans at all — even if they can get past that 24 million number.', 'tags': 'phrase'}
['multi', 'passage', 'phrase']
08/03/2023 21:00:11 - INFO - __main__ - setting problem type to single label classification
[INFO|configuration_utils.py:715] 2023-08-03 21:00:11,137 >> loading configuration file config.json from cache at /work/y53kang/.cache/huggingface/hub/models--microsoft--deberta-v2-xlarge-mnli/snapshots/5272422ce68b8d61766079390b96b033a64414d2/config.json
[INFO|configuration_utils.py:771] 2023-08-03 21:00:11,138 >> Model config DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge-mnli",
  "architectures": [
    "DebertaV2ForSequenceClassification"
  ],
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.1,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1536,
  "id2label": {
    "0": "CONTRADICTION",
    "1": "NEUTRAL",
    "2": "ENTAILMENT"
  },
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "label2id": {
    "CONTRADICTION": 0,
    "ENTAILMENT": 2,
    "NEUTRAL": 1
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pooling": {
    "dropout": 0,
    "hidden_act": "gelu"
  },
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.32.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

[INFO|tokenization_utils_base.py:1844] 2023-08-03 21:00:11,144 >> loading file spm.model from cache at /work/y53kang/.cache/huggingface/hub/models--microsoft--deberta-v2-xlarge-mnli/snapshots/5272422ce68b8d61766079390b96b033a64414d2/spm.model
[INFO|tokenization_utils_base.py:1844] 2023-08-03 21:00:11,144 >> loading file tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1844] 2023-08-03 21:00:11,144 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1844] 2023-08-03 21:00:11,144 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1844] 2023-08-03 21:00:11,144 >> loading file tokenizer_config.json from cache at /work/y53kang/.cache/huggingface/hub/models--microsoft--deberta-v2-xlarge-mnli/snapshots/5272422ce68b8d61766079390b96b033a64414d2/tokenizer_config.json
[INFO|configuration_utils.py:715] 2023-08-03 21:00:11,145 >> loading configuration file config.json from cache at /work/y53kang/.cache/huggingface/hub/models--microsoft--deberta-v2-xlarge-mnli/snapshots/5272422ce68b8d61766079390b96b033a64414d2/config.json
[INFO|configuration_utils.py:771] 2023-08-03 21:00:11,145 >> Model config DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge-mnli",
  "architectures": [
    "DebertaV2ForSequenceClassification"
  ],
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.1,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1536,
  "id2label": {
    "0": "CONTRADICTION",
    "1": "NEUTRAL",
    "2": "ENTAILMENT"
  },
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "label2id": {
    "CONTRADICTION": 0,
    "ENTAILMENT": 2,
    "NEUTRAL": 1
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pooling": {
    "dropout": 0,
    "hidden_act": "gelu"
  },
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.32.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

[INFO|tokenization_utils.py:426] 2023-08-03 21:00:11,537 >> Adding [MASK] to the vocabulary
[WARNING|logging.py:284] 2023-08-03 21:00:11,537 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:715] 2023-08-03 21:00:11,538 >> loading configuration file config.json from cache at /work/y53kang/.cache/huggingface/hub/models--microsoft--deberta-v2-xlarge-mnli/snapshots/5272422ce68b8d61766079390b96b033a64414d2/config.json
[INFO|configuration_utils.py:771] 2023-08-03 21:00:11,539 >> Model config DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge-mnli",
  "architectures": [
    "DebertaV2ForSequenceClassification"
  ],
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.1,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1536,
  "id2label": {
    "0": "CONTRADICTION",
    "1": "NEUTRAL",
    "2": "ENTAILMENT"
  },
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "label2id": {
    "CONTRADICTION": 0,
    "ENTAILMENT": 2,
    "NEUTRAL": 1
  },
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pooling": {
    "dropout": 0,
    "hidden_act": "gelu"
  },
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.32.0.dev0",
  "type_vocab_size": 0,
  "vocab_size": 128100
}

[WARNING|logging.py:284] 2023-08-03 21:00:11,745 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:2638] 2023-08-03 21:00:11,887 >> loading weights file pytorch_model.bin from cache at /work/y53kang/.cache/huggingface/hub/models--microsoft--deberta-v2-xlarge-mnli/snapshots/5272422ce68b8d61766079390b96b033a64414d2/pytorch_model.bin
[INFO|modeling_utils.py:3370] 2023-08-03 21:00:17,631 >> All model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.

[INFO|modeling_utils.py:3378] 2023-08-03 21:00:17,631 >> All the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at microsoft/deberta-v2-xlarge-mnli.
If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.
08/03/2023 21:00:17 - WARNING - __main__ - The label2id key in the model config.json is not equal to the label2id key of this run. You can ignore this if you are doing finetuning.
Running tokenizer on train dataset:   0%|          | 0/3200 [00:00<?, ? examples/s]Running tokenizer on train dataset:  31%|███▏      | 1000/3200 [00:01<00:03, 572.47 examples/s]Running tokenizer on train dataset:  62%|██████▎   | 2000/3200 [00:03<00:02, 569.52 examples/s]Running tokenizer on train dataset:  94%|█████████▍| 3000/3200 [00:05<00:00, 590.53 examples/s]Running tokenizer on train dataset: 100%|██████████| 3200/3200 [00:05<00:00, 598.09 examples/s]                                                                                               08/03/2023 21:00:23 - INFO - __main__ - Shuffling the training dataset
Running tokenizer on validation dataset:   0%|          | 0/400 [00:00<?, ? examples/s]Running tokenizer on validation dataset: 100%|██████████| 400/400 [00:00<00:00, 541.68 examples/s]                                                                                                  08/03/2023 21:00:23 - INFO - __main__ - Sample 2619 of the training set: {'id': 794, 'context': 'Wondering why that millennial won\'t take your phone call? Here\'s why ...Wondering why that millennial won\'t take your phone call? Here\'s why ... - When I was seven, I joined hen I was seven, I joined the Brownies . At the time, becoming part of Baden-Powell’s famous organisation felt very grown up. However, when I reflect on the experience, I realise that being a Brownie is worth doing only if you start when you’re very young. It’s full of peculiar rituals, and only now that I am 31, and no one incentivises me to do anything by offering me a special badge, do I recognise how odd some of them were We were given badges for applying bandages to our ungrazed knees, remembering to keep a sewing kit in the pockets of our culottes, and successfully using a payphone to make a call. I’ll always remember the day 24 Brownies were walked into the nearest village, and each given a 10pto call our mums and tell them that we were still at Brownies, everything was fine and that we would see them in 40 minutes. From that moment, I saw telephone conversations as exciting events. They were fraught with romance, glamour and danger, and could only be approached if you were under strict supervision, unless you were a proper adult. I would have been overwhelmed if anyone had predicted that when I was an adult, I would have a portable phone for my exclusive use. I think I would have collapsed with shock if I had been told that I could use it to read books, play games, argue with strangers and identify vaguely recognisable songs that were being played in bars. If someone had said that I’d barely bother to make calls at all, I’d have told them they were a liar. But it’s true. More than three-quarters of all adults in the UK own smartphones, but 25% don’t use them to make calls Can we blame millennials for the demise of the phone call? Most of the twenty- and thirtysomethings I socialise with would rather suck Donald Trump’s toe than make or receive a call in order to have a chat. We’re in touch with each other constantly, but written communication allows us to participate in the conversation at the pace we choose. We don’t have to worry about misreading anyone’s tone when the words are laid out in black and white. However, it seems that phone-call phobia definitely isn’t confined to a single generation. Jeremy Corbyn’s aides complained that they were unable to reach their boss on the telephone this week to discuss "traingate" because he was busy making jam Perhaps it’s not the phone calls themselves we object to, but the feeling of being ambushed by them Many millennials struggle with mental health, and we’ve been dubbed the "anxious generation" . As an anxiety sufferer, nothing fills me with dread and terror like a phone call from a withheld number. The trouble is that the issue exacerbates itself. Because we don’t call for chats, I assume that whenever anyone does try to reach me on the telephone, it’s because of a bad, sad, serious emergency and I need to be reached urgently, possibly for a telling off. So I fling the phone from my hand, as if it’s a live snake, and then get a voicemail from someone who was just ringing for a catch-up. "But it’s not urgent, we can just do it on WhatsApp." Perhaps it’s not phone calls themselves we object to, but the feeling of being ambushed by them. One worker in their 20s told the Wall Street Journal: "Calling someone without emailing first can make it seem as though you’re prioritising your needs over theirs." That’s right. The millennial attitude towards phone calls is actually about manners. We’ve grown up with so many methods of communication available to us, and we’ve gravitated towards the least intrusive ones because we know how it feels to be digitally prodded on a range of different channels. Speaking on the telephone is an event, and we don’t want to avoid it – we just need to be sure that both parties have a chance to prepare for it. We want a chance to compose and edit our thoughts, in the way we do when we’re writing them down. There is an element of mindfulness involved too. Thanks to our phones, we’re never doing nothing, and if we’re surprised by a phone call we might struggle to give the caller our full attention. If we know when to expect it, we can at least try to focus in a way that we’re not always capable of during other moments of our lives. It’s fair to say that young people can still see the value of a phone call, but perhaps we understand it as something serious and significant, to be used in much more specific contexts and shared with a select group of people. We all have an inner circle who would be allowed to interrupt us for a chat when we’re in the middle of making jam. Everyone else will have to make do with a message.', 'label': 1, 'sentence': 'Wondering why that millennial won\'t take your phone call? Here\'s why ...Wondering why that millennial won\'t take your phone call? Here\'s why ... - When I was seven, I joined hen I was seven, I joined the Brownies . At the time, becoming part of Baden-Powell’s famous organisation felt very grown up. However, when I reflect on the experience, I realise that being a Brownie is worth doing only if you start when you’re very young. It’s full of peculiar rituals, and only now that I am 31, and no one incentivises me to do anything by offering me a special badge, do I recognise how odd some of them were We were given badges for applying bandages to our ungrazed knees, remembering to keep a sewing kit in the pockets of our culottes, and successfully using a payphone to make a call. I’ll always remember the day 24 Brownies were walked into the nearest village, and each given a 10pto call our mums and tell them that we were still at Brownies, everything was fine and that we would see them in 40 minutes. From that moment, I saw telephone conversations as exciting events. They were fraught with romance, glamour and danger, and could only be approached if you were under strict supervision, unless you were a proper adult. I would have been overwhelmed if anyone had predicted that when I was an adult, I would have a portable phone for my exclusive use. I think I would have collapsed with shock if I had been told that I could use it to read books, play games, argue with strangers and identify vaguely recognisable songs that were being played in bars. If someone had said that I’d barely bother to make calls at all, I’d have told them they were a liar. But it’s true. More than three-quarters of all adults in the UK own smartphones, but 25% don’t use them to make calls Can we blame millennials for the demise of the phone call? Most of the twenty- and thirtysomethings I socialise with would rather suck Donald Trump’s toe than make or receive a call in order to have a chat. We’re in touch with each other constantly, but written communication allows us to participate in the conversation at the pace we choose. We don’t have to worry about misreading anyone’s tone when the words are laid out in black and white. However, it seems that phone-call phobia definitely isn’t confined to a single generation. Jeremy Corbyn’s aides complained that they were unable to reach their boss on the telephone this week to discuss "traingate" because he was busy making jam Perhaps it’s not the phone calls themselves we object to, but the feeling of being ambushed by them Many millennials struggle with mental health, and we’ve been dubbed the "anxious generation" . As an anxiety sufferer, nothing fills me with dread and terror like a phone call from a withheld number. The trouble is that the issue exacerbates itself. Because we don’t call for chats, I assume that whenever anyone does try to reach me on the telephone, it’s because of a bad, sad, serious emergency and I need to be reached urgently, possibly for a telling off. So I fling the phone from my hand, as if it’s a live snake, and then get a voicemail from someone who was just ringing for a catch-up. "But it’s not urgent, we can just do it on WhatsApp." Perhaps it’s not phone calls themselves we object to, but the feeling of being ambushed by them. One worker in their 20s told the Wall Street Journal: "Calling someone without emailing first can make it seem as though you’re prioritising your needs over theirs." That’s right. The millennial attitude towards phone calls is actually about manners. We’ve grown up with so many methods of communication available to us, and we’ve gravitated towards the least intrusive ones because we know how it feels to be digitally prodded on a range of different channels. Speaking on the telephone is an event, and we don’t want to avoid it – we just need to be sure that both parties have a chance to prepare for it. We want a chance to compose and edit our thoughts, in the way we do when we’re writing them down. There is an element of mindfulness involved too. Thanks to our phones, we’re never doing nothing, and if we’re surprised by a phone call we might struggle to give the caller our full attention. If we know when to expect it, we can at least try to focus in a way that we’re not always capable of during other moments of our lives. It’s fair to say that young people can still see the value of a phone call, but perhaps we understand it as something serious and significant, to be used in much more specific contexts and shared with a select group of people. We all have an inner circle who would be allowed to interrupt us for a chat when we’re in the middle of making jam. Everyone else will have to make do with a message.', 'input_ids': [1, 34236, 314, 15, 33190, 474, 25, 38, 155, 29, 661, 398, 44, 651, 25, 12, 314, 677, 59767, 227, 314, 15, 33190, 474, 25, 38, 155, 29, 661, 398, 44, 651, 25, 12, 314, 677, 91, 220, 16, 28, 1617, 6, 16, 2055, 25115, 16, 28, 1617, 6, 16, 2055, 5, 80733, 65, 356, 5, 66, 6, 1845, 203, 9, 45492, 18, 110784, 20, 12, 1844, 4197, 914, 117, 2780, 62, 4, 407, 6, 76, 16, 3260, 21, 5, 241, 6, 16, 8335, 15, 157, 10, 63537, 13, 976, 381, 103, 75, 17, 303, 76, 17, 20, 115, 117, 607, 4, 64, 20, 12, 275, 9, 17402, 15866, 6, 7, 103, 136, 15, 16, 214, 5980, 7, 104, 54, 93902, 12, 89, 8, 71, 509, 35, 1354, 89, 10, 511, 9968, 6, 71, 16, 10205, 100, 4665, 85, 9, 88, 77, 82, 77, 482, 22270, 14, 3998, 49397, 8, 57, 1364, 22419, 39518, 8223, 6, 12251, 8, 285, 10, 8489, 3159, 11, 5, 6933, 9, 57, 13411, 69886, 12, 6, 7, 2734, 213, 10, 534, 14318, 8, 102, 10, 398, 4, 16, 20, 175, 221, 900, 5, 149, 1070, 80733, 77, 2775, 92, 5, 7356, 2486, 6, 7, 182, 482, 10, 257, 40685, 398, 57, 34325, 7, 565, 88, 15, 42, 77, 188, 33, 80733, 6, 478, 28, 1087, 7, 15, 42, 80, 137, 88, 11, 1298, 498, 4, 695, 15, 887, 6, 16, 798, 4190, 5177, 27, 2098, 693, 4, 187, 77, 34968, 19, 6729, 6, 22354, 7, 4554, 6, 7, 132, 103, 26, 6980, 75, 17, 77, 240, 6269, 8711, 6, 1998, 17, 77, 10, 1877, 2477, 4, 16, 80, 30, 74, 9660, 75, 720, 73, 7306, 15, 76, 16, 28, 41, 2477, 6, 16, 80, 30, 10, 4995, 661, 14, 50, 2899, 118, 4, 16, 171, 16, 80, 30, 13429, 19, 4961, 75, 16, 73, 74, 544, 15, 16, 132, 118, 22, 8, 341, 802, 6, 368, 646, 6, 4894, 19, 10865, 7, 2005, 24318, 42715, 2053, 15, 77, 157, 1119, 11, 3801, 4, 110, 483, 73, 129, 15, 16, 20, 147, 4838, 7392, 8, 102, 1892, 33, 46, 6, 16, 20, 147, 30, 544, 88, 49, 77, 10, 27129, 4, 167, 22, 20, 12, 684, 4, 1089, 97, 225, 18, 32934, 9, 46, 2314, 11, 5, 952, 186, 9330, 6, 47, 8368, 153, 20, 38, 118, 88, 8, 102, 1892, 854, 42, 5157, 24136, 14, 5, 17862, 9, 5, 661, 398, 44, 945, 9, 5, 3449, 18, 7, 5765, 12676, 12, 16, 70375, 19, 80, 617, 40962, 4139, 1291, 20, 12, 8, 208, 97, 102, 31, 784, 10, 398, 11, 288, 8, 30, 10, 3293, 4, 82, 20, 115, 11, 1350, 19, 182, 81, 2739, 6, 47, 905, 1663, 955, 120, 8, 2698, 11, 5, 2265, 33, 5, 3685, 42, 679, 4, 82, 153, 20, 38, 30, 8, 2219, 56, 104848, 720, 20, 12, 3721, 76, 5, 714, 24, 3842, 61, 11, 668, 7, 602, 4, 407, 6, 22, 717, 15, 661, 18, 13738, 48005, 1131, 663, 20, 38, 13263, 8, 10, 501, 2084, 4, 9562, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
08/03/2023 21:00:23 - INFO - __main__ - Sample 456 of the training set: {'id': 982, 'context': 'Who are the European Golden Boy winners since 2003 – and what happened to them?Who are the European Golden Boy winners since 2003 – and what happened to them? - 12 min ago Topics comments With this year’s 40-man shortlist for the annual award having been announced, Joe Brewin and Greg Lea look at where its previous winners wound up 2003: Rafael van der Vaart (Ajax) " In 2008 he was snapped up by Real Madrid for €13m, spent two trophyless seasons in the Spanish capital and then moved to Tottenham, where he was a popular figure for two campaigns before re-joining Hamburg . . Van der Vaart was earmarked for greatness from very early on in his career. Having honed his skills on the streets of Amsterdam close to the trailer park he grew up in, he was scooped up by Ajax aged 10 and moulded into the technically excellent player who would leave them 12 years later for Hamburg. The Dutchman was a first-team regular by the age of 17 and won the Eredivisie twice, in 2002 and 2004, establishing himself as a goalscoring midfielder with 14 league strikes in 2001/02 and another 18 the following campaign (from an injury-curtailed total of just 41 appearances altogether). Van der Vaart gets a shot off before being closed down by Scotland\'s Barry Ferguson in 2003 Those injuries took their toll. Van der Vaart’s stock fell before his surprising 2005 switch to Hamburg – much to Johan Cruyff’s disgust, among others – but his move to northern Germany was a wise one. In 2008 he was snapped up by Real Madrid for €13m, spent two trophyless seasons in the Spanish capital and then moved to Tottenham , where he was a popular figure for two campaigns before re-joining Hamburg. That was the beginning of the end: Van der Vaart’s stock was falling in line with his club’s, and they followed up a seventh-place finish in 2013 by twice almost getting relegated. A move to Real Betis was even worse – he barely featured and left the La Liga strugglers after a year. He’s now in Denmark with FC Midtjylland after being linked with Reading during the summer. 2004: Wayne Rooney (Man United) " Major tournament success with England has always been elusive, though, despite Rooney breaking the all-time scoring record in 2015 and becoming his country’s most-capped outfielder in September 2016 . . Oh, for these days again Wazza. Having followed up two seasons in Everton’s first team with an excellent Euro 2004 for England, it was only a matter of time before the country’s golden boy moved on to bigger things. Newcastle wanted him; Manchester United got him for a shade over £25m – at the time a record for a player under 20. Under Fergie’s wing he achieved a Premier League-best haul of 11 goals in his debut campaign, then duly followed that with 10 more seasons of double figures (including two above the 25-mark), firing United to five title wins and a Champions League trinket in the process. Major tournament success with England has always been elusive, though, despite Rooney breaking the all-time scoring record in 2015 and becoming his country’s most-capped outfielder in September 2016. Both of those latter achievements have only masked a worryingly slide, however: Rooney hasn’t been quite right since the late Fergie years but has surely hit his nadir now, aged 30. Having played so poorly for so long and been recently dropped by Jose Mourinho, this time the decline looks terminal. 2005: Lionel Messi (Barcelona) " Messi at this point wasn’t the free-scoring freak of today, but his dribbling skills dropped jaws and he was already considered among the world’s best when he was barely out of his teens . . "I’d never seen anything like it from a teenager," cooed Fabio Capello after watching world football’s new boy wonder dismantle his experienced Juventus side in a summer 2005 pre-season friendly in Barcelona . "At the end of the game, I went up to Frank Rijkaard and asked to loan him for the season, because they already had three non-EU players [Messi was due to receive his Spanish passport the following month]. He just laughed and said: ‘No chance’." He’d finally arrived properly. Barça already knew what they had in the pint-sized prodigy, of course, but now everyone else was seeing it too. Messi at this point wasn’t the free-scoring freak of today, but his dribbling skills dropped jaws and he was already considered among the world’s best when he was barely out of his teens. Messi shows off the Golden Ball alongside Ronaldinho and Samuel Eto\'o, who finished first and third in the World Player of the Year "Best in the world? I’m not even the best at Barça," chuckled Brazilian great Ronaldinho to FFT in late 2005 – but he really wasn’t joking. The buck-toothed trickster’s brilliance was mesmeric but frustratingly fleeting, unlike Messi who\'s lasted the course and somehow got better with age. Eight La Liga titles with Barcelona and another four in the Champions League only tell a portion of the story of a player who\'s scored 461 goals in 539 games for his only club. At 29 he’s still just as frightening. 2006: Cesc Fabregas (Arsenal) Chucked into Arsenal ’s first team at 16 three years earlier, Fabregas became a regular in Arsene Wenger’s side from the following season onwards, playing alongside the seasoned likes of Gilberto Silva and Patrick Vieira. The Spaniard arguably goes down as Wenger’s greatest student to date; a player who was forced to take on a great weight of responsibility after Vieira’s departure to Juventus in 2005 and duly responded with consistently impressive performances. By the time of his Golden Boy win in 2006, aged 19, he was already over three-quarters of the way to 100 appearances in the Premier League. But the trophies never came. Fabregas had lifted the FA Cup in 2005 but didn’t get to taste success with Arsenal again, so duly returned home to Barcelona in 2011 to win La Liga (but not the Champions League) in 2013. His homecoming hardly went to plan, though, and a 2014 switch to Chelsea was right for all parties. Things haven’t really been the same since the first half of his debut season at Stamford Bridge. Next: Better than Kleberson? (OK, fine)', 'label': 0, 'sentence': 'Who are the European Golden Boy winners since 2003 – and what happened to them?Who are the European Golden Boy winners since 2003 – and what happened to them? - 12 min ago Topics comments With this year’s 40-man shortlist for the annual award having been announced, Joe Brewin and Greg Lea look at where its previous winners wound up 2003: Rafael van der Vaart (Ajax) " In 2008 he was snapped up by Real Madrid for €13m, spent two trophyless seasons in the Spanish capital and then moved to Tottenham, where he was a popular figure for two campaigns before re-joining Hamburg . . Van der Vaart was earmarked for greatness from very early on in his career. Having honed his skills on the streets of Amsterdam close to the trailer park he grew up in, he was scooped up by Ajax aged 10 and moulded into the technically excellent player who would leave them 12 years later for Hamburg. The Dutchman was a first-team regular by the age of 17 and won the Eredivisie twice, in 2002 and 2004, establishing himself as a goalscoring midfielder with 14 league strikes in 2001/02 and another 18 the following campaign (from an injury-curtailed total of just 41 appearances altogether). Van der Vaart gets a shot off before being closed down by Scotland\'s Barry Ferguson in 2003 Those injuries took their toll. Van der Vaart’s stock fell before his surprising 2005 switch to Hamburg – much to Johan Cruyff’s disgust, among others – but his move to northern Germany was a wise one. In 2008 he was snapped up by Real Madrid for €13m, spent two trophyless seasons in the Spanish capital and then moved to Tottenham , where he was a popular figure for two campaigns before re-joining Hamburg. That was the beginning of the end: Van der Vaart’s stock was falling in line with his club’s, and they followed up a seventh-place finish in 2013 by twice almost getting relegated. A move to Real Betis was even worse – he barely featured and left the La Liga strugglers after a year. He’s now in Denmark with FC Midtjylland after being linked with Reading during the summer. 2004: Wayne Rooney (Man United) " Major tournament success with England has always been elusive, though, despite Rooney breaking the all-time scoring record in 2015 and becoming his country’s most-capped outfielder in September 2016 . . Oh, for these days again Wazza. Having followed up two seasons in Everton’s first team with an excellent Euro 2004 for England, it was only a matter of time before the country’s golden boy moved on to bigger things. Newcastle wanted him; Manchester United got him for a shade over £25m – at the time a record for a player under 20. Under Fergie’s wing he achieved a Premier League-best haul of 11 goals in his debut campaign, then duly followed that with 10 more seasons of double figures (including two above the 25-mark), firing United to five title wins and a Champions League trinket in the process. Major tournament success with England has always been elusive, though, despite Rooney breaking the all-time scoring record in 2015 and becoming his country’s most-capped outfielder in September 2016. Both of those latter achievements have only masked a worryingly slide, however: Rooney hasn’t been quite right since the late Fergie years but has surely hit his nadir now, aged 30. Having played so poorly for so long and been recently dropped by Jose Mourinho, this time the decline looks terminal. 2005: Lionel Messi (Barcelona) " Messi at this point wasn’t the free-scoring freak of today, but his dribbling skills dropped jaws and he was already considered among the world’s best when he was barely out of his teens . . "I’d never seen anything like it from a teenager," cooed Fabio Capello after watching world football’s new boy wonder dismantle his experienced Juventus side in a summer 2005 pre-season friendly in Barcelona . "At the end of the game, I went up to Frank Rijkaard and asked to loan him for the season, because they already had three non-EU players [Messi was due to receive his Spanish passport the following month]. He just laughed and said: ‘No chance’." He’d finally arrived properly. Barça already knew what they had in the pint-sized prodigy, of course, but now everyone else was seeing it too. Messi at this point wasn’t the free-scoring freak of today, but his dribbling skills dropped jaws and he was already considered among the world’s best when he was barely out of his teens. Messi shows off the Golden Ball alongside Ronaldinho and Samuel Eto\'o, who finished first and third in the World Player of the Year "Best in the world? I’m not even the best at Barça," chuckled Brazilian great Ronaldinho to FFT in late 2005 – but he really wasn’t joking. The buck-toothed trickster’s brilliance was mesmeric but frustratingly fleeting, unlike Messi who\'s lasted the course and somehow got better with age. Eight La Liga titles with Barcelona and another four in the Champions League only tell a portion of the story of a player who\'s scored 461 goals in 539 games for his only club. At 29 he’s still just as frightening. 2006: Cesc Fabregas (Arsenal) Chucked into Arsenal ’s first team at 16 three years earlier, Fabregas became a regular in Arsene Wenger’s side from the following season onwards, playing alongside the seasoned likes of Gilberto Silva and Patrick Vieira. The Spaniard arguably goes down as Wenger’s greatest student to date; a player who was forced to take on a great weight of responsibility after Vieira’s departure to Juventus in 2005 and duly responded with consistently impressive performances. By the time of his Golden Boy win in 2006, aged 19, he was already over three-quarters of the way to 100 appearances in the Premier League. But the trophies never came. Fabregas had lifted the FA Cup in 2005 but didn’t get to taste success with Arsenal again, so duly returned home to Barcelona in 2011 to win La Liga (but not the Champions League) in 2013. His homecoming hardly went to plan, though, and a 2014 switch to Chelsea was right for all parties. Things haven’t really been the same since the first half of his debut season at Stamford Bridge. Next: Better than Kleberson? (OK, fine)', 'input_ids': [1, 1548, 24, 5, 1333, 4308, 6193, 5396, 262, 4832, 119, 7, 79, 1397, 8, 88, 44, 11553, 24, 5, 1333, 4308, 6193, 5396, 262, 4832, 119, 7, 79, 1397, 8, 88, 44, 91, 515, 5118, 577, 15385, 1469, 292, 32, 145, 20, 12, 17654, 957, 34491, 14, 5, 1523, 2079, 326, 74, 1481, 6, 3048, 21424, 282, 7, 7198, 22592, 207, 33, 141, 107, 1161, 5396, 7330, 62, 4832, 43, 23565, 3884, 4634, 9558, 4560, 36, 291, 65590, 53, 55, 84, 2848, 58, 28, 14800, 62, 35, 2558, 9283, 14, 29596, 678, 99, 6, 1187, 122, 13369, 2334, 4656, 11, 5, 3061, 1733, 7, 130, 1322, 8, 25093, 6, 141, 58, 28, 10, 833, 1525, 14, 122, 4928, 159, 682, 18, 81336, 19471, 65, 65, 3858, 4634, 9558, 4560, 28, 45939, 14, 18564, 34, 117, 480, 21, 11, 60, 977, 4, 2682, 32596, 60, 867, 21, 5, 3417, 9, 9741, 574, 8, 5, 5016, 1734, 58, 2845, 62, 11, 6, 58, 28, 39720, 62, 35, 26986, 4551, 257, 7, 42083, 92, 5, 9296, 1045, 1255, 70, 80, 745, 88, 515, 135, 481, 14, 19471, 4, 23, 65853, 28, 10, 108, 18, 14087, 1197, 35, 5, 627, 9, 1427, 7, 474, 5, 546, 115, 92343, 12, 1821, 2531, 6, 11, 5415, 7, 10163, 6950, 1149, 27, 10, 1009, 37396, 17751, 19, 1067, 3375, 7827, 11, 5572, 50707, 7, 242, 1040, 5, 524, 1415, 36, 4708, 41, 2133, 18, 23736, 30765, 799, 9, 87, 6468, 8499, 8239, 142, 3858, 4634, 9558, 4560, 1199, 10, 1483, 185, 159, 157, 1923, 184, 35, 4380, 25, 12, 9136, 15298, 11, 4832, 2110, 3436, 440, 51, 8, 175, 4, 3858, 4634, 9558, 4560, 20, 12, 1218, 2540, 159, 60, 5500, 4121, 2543, 8, 19471, 119, 140, 8, 32209, 28066, 331, 9943, 20, 12, 27283, 6, 639, 415, 119, 47, 60, 582, 8, 4497, 2242, 28, 10, 4777, 54, 4, 84, 2848, 58, 28, 14800, 62, 35, 2558, 9283, 14, 29596, 678, 99, 6, 1187, 122, 13369, 2334, 4656, 11, 5, 3061, 1733, 7, 130, 1322, 8, 25093, 101, 141, 58, 28, 10, 833, 1525, 14, 122, 4928, 159, 682, 18, 81336, 19471, 4, 251, 28, 5, 1246, 9, 5, 255, 43, 3858, 4634, 9558, 4560, 20, 12, 1218, 28, 3795, 11, 416, 19, 60, 1608, 20, 12, 6, 7, 49, 1452, 62, 10, 8801, 18, 10836, 1419, 11, 1919, 35, 2531, 555, 377, 28566, 4, 78, 582, 8, 2558, 1163, 13713, 28, 144, 2416, 119, 58, 4838, 2842, 7, 347, 5, 1762, 30227, 3245, 3323, 139, 10, 145, 4, 133, 20, 12, 136, 11, 10007, 19, 8169, 6789, 38, 88402, 175, 431, 139, 157, 3265, 19, 5714, 232, 5, 932, 4, 4571, 43, 7624, 32771, 36, 7375, 549, 53, 55, 5682, 4333, 937, 19, 1837, 45, 221, 74, 19011, 6, 379, 6, 2009, 32771, 3879, 5, 46, 18, 1215, 5877, 941, 11, 1558, 7, 1845, 60, 433, 20, 12, 112, 18, 45430, 42510, 11, 1130, 1343, 65, 65, 1895, 6, 14, 116, 281, 279, 1143, 47923, 4, 2682, 1452, 62, 122, 4656, 11, 28293, 20, 12, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
08/03/2023 21:00:23 - INFO - __main__ - Sample 102 of the training set: {'id': 3023, 'context': '9 people who discovered just how terrible their exes really are9 people who discovered just how terrible their exes really are The list - That moment of clarity where you realise you’re way better off out of your previous relationship is worth all of the evenings spent crying into pizza. So if you\'re going through a breakup right now, we have the perfect support for you. A group of people have taken to anonymous confession website Whisper to share their moment of closure and sweet, sweet relief. Here are some of the best:   [I] Saw my ex on an episode of Cops. I dodged a bullet for sure. Found out my ex voted for Trump... Never been more glad we broke up. Got a bank statement for my ex. So glad we broke up. She is too old to be that shitty with money. My ex got matching name tattoos with her fiancé of three months.  My ex-boyfriend feared his future kids would turn out gay/lesbian. Thank God we broke up. I re-read screenshots from the end of my last relationship. I didn’t realise how insane and borderline abusive my ex was at the time – so glad to say I’m at least ten times happier now. I ran into my ex, who casually mentioned that he was high from Windex. I’m so glad I ended that relationship. My ex-boyfriend thinks I "got cancer" to get his attention. Yes I gave myself cancer so you would pay attention to me. I’m so glad we broke up. On a friend’s Snapchat I saw my ex so drunk he was unconscious and had dicks drawn on his face... While I’m over here planning my wedding. Oh boy, I dodged a bullet. He hasn’t changed.', 'label': 0, 'sentence': '9 people who discovered just how terrible their exes really are9 people who discovered just how terrible their exes really are The list - That moment of clarity where you realise you’re way better off out of your previous relationship is worth all of the evenings spent crying into pizza. So if you\'re going through a breakup right now, we have the perfect support for you. A group of people have taken to anonymous confession website Whisper to share their moment of closure and sweet, sweet relief. Here are some of the best:   [I] Saw my ex on an episode of Cops. I dodged a bullet for sure. Found out my ex voted for Trump... Never been more glad we broke up. Got a bank statement for my ex. So glad we broke up. She is too old to be that shitty with money. My ex got matching name tattoos with her fiancé of three months.  My ex-boyfriend feared his future kids would turn out gay/lesbian. Thank God we broke up. I re-read screenshots from the end of my last relationship. I didn’t realise how insane and borderline abusive my ex was at the time – so glad to say I’m at least ten times happier now. I ran into my ex, who casually mentioned that he was high from Windex. I’m so glad I ended that relationship. My ex-boyfriend thinks I "got cancer" to get his attention. Yes I gave myself cancer so you would pay attention to me. I’m so glad we broke up. On a friend’s Snapchat I saw my ex so drunk he was unconscious and had dicks drawn on his face... While I’m over here planning my wedding. Oh boy, I dodged a bullet. He hasn’t changed.', 'input_ids': [1, 728, 98, 70, 2260, 87, 100, 4506, 51, 41881, 12, 172, 24, 2577, 98, 70, 2260, 87, 100, 4506, 51, 41881, 12, 172, 24, 23, 405, 91, 251, 887, 9, 7195, 141, 17, 8335, 17, 20, 115, 123, 233, 185, 61, 9, 29, 1161, 1059, 13, 976, 46, 9, 5, 11832, 1187, 7741, 92, 5303, 4, 211, 75, 17, 25, 115, 189, 131, 10, 26773, 164, 136, 6, 42, 30, 5, 512, 264, 14, 17, 4, 78, 343, 9, 98, 30, 615, 8, 9107, 20106, 344, 38665, 8, 490, 51, 887, 9, 7910, 7, 1803, 6, 1803, 3243, 4, 651, 24, 85, 9, 5, 151, 43, 476, 235, 552, 14144, 50, 3319, 21, 41, 2592, 9, 58429, 4, 16, 59206, 10, 9323, 14, 254, 4, 12378, 61, 50, 3319, 5181, 14, 1291, 263, 4414, 74, 52, 2502, 42, 3762, 62, 4, 6977, 10, 1636, 1513, 14, 50, 3319, 4, 211, 2502, 42, 3762, 62, 4, 246, 13, 196, 323, 8, 26, 15, 109194, 19, 319, 4, 295, 3319, 260, 4753, 329, 17687, 19, 86, 36487, 9, 225, 489, 4, 295, 3319, 18, 48802, 13584, 60, 457, 696, 80, 659, 61, 4469, 96, 108518, 4, 1052, 339, 42, 3762, 62, 4, 16, 682, 18, 7907, 19110, 34, 5, 255, 9, 50, 190, 1059, 4, 16, 335, 20, 38, 8335, 100, 10643, 7, 32750, 14424, 50, 3319, 28, 33, 5, 66, 119, 63, 2502, 8, 253, 16, 20, 99, 33, 404, 1705, 367, 9306, 136, 4, 16, 2311, 92, 50, 3319, 6, 70, 21789, 1681, 15, 58, 28, 201, 34, 1143, 13334, 4, 16, 20, 99, 63, 2502, 16, 1873, 15, 1059, 4, 295, 3319, 18, 48802, 5351, 16, 55, 19327, 1495, 109, 8, 90, 60, 960, 4, 1270, 16, 1032, 806, 1495, 63, 17, 80, 534, 960, 8, 89, 4, 16, 20, 99, 63, 2502, 42, 3762, 62, 4, 333, 10, 865, 20, 12, 21297, 16, 798, 50, 3319, 63, 8640, 58, 28, 13867, 7, 73, 83249, 3713, 21, 60, 548, 263, 547, 16, 20, 99, 105, 160, 1141, 50, 1454, 4, 1895, 2149, 6, 16, 59206, 10, 9323, 4, 133, 3546, 20, 38, 1307, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
08/03/2023 21:00:24 - INFO - __main__ - Using metric accuracy for evaluation.
[INFO|trainer.py:749] 2023-08-03 21:00:25,859 >> The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: context, id, sentence. If context, id, sentence are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1681] 2023-08-03 21:00:25,871 >> ***** Running training *****
[INFO|trainer.py:1682] 2023-08-03 21:00:25,871 >>   Num examples = 3,200
[INFO|trainer.py:1683] 2023-08-03 21:00:25,871 >>   Num Epochs = 8
[INFO|trainer.py:1684] 2023-08-03 21:00:25,871 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1687] 2023-08-03 21:00:25,871 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1688] 2023-08-03 21:00:25,871 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1689] 2023-08-03 21:00:25,871 >>   Total optimization steps = 400
[INFO|trainer.py:1690] 2023-08-03 21:00:25,872 >>   Number of trainable parameters = 886,958,595
[INFO|integrations.py:716] 2023-08-03 21:00:25,879 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: ym_k. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.7
wandb: Run data is saved locally in /mnt/hpc/work/y53kang/pan-code/semeval23/baselines/transformer-baseline-task-1/wandb/run-20230803_210027-x4cf78yu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run task1-deberta-v2-xlarge-mnli-4
wandb: ⭐️ View project at https://wandb.ai/ym_k/huggingface
wandb: 🚀 View run at https://wandb.ai/ym_k/huggingface/runs/x4cf78yu
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:12<1:21:28, 12.25s/it]  0%|          | 2/400 [00:23<1:17:23, 11.67s/it]  1%|          | 3/400 [00:34<1:16:04, 11.50s/it]  1%|          | 4/400 [00:46<1:15:28, 11.43s/it]  1%|▏         | 5/400 [00:57<1:15:08, 11.42s/it]  2%|▏         | 6/400 [01:08<1:14:54, 11.41s/it]  2%|▏         | 7/400 [01:20<1:14:42, 11.41s/it]  2%|▏         | 8/400 [01:31<1:14:31, 11.41s/it]  2%|▏         | 9/400 [01:43<1:14:25, 11.42s/it]  2%|▎         | 10/400 [01:54<1:14:15, 11.43s/it]  3%|▎         | 11/400 [02:06<1:14:08, 11.43s/it]  3%|▎         | 12/400 [02:17<1:14:00, 11.44s/it]  3%|▎         | 13/400 [02:28<1:13:47, 11.44s/it]  4%|▎         | 14/400 [02:40<1:13:39, 11.45s/it]  4%|▍         | 15/400 [02:51<1:13:29, 11.45s/it]  4%|▍         | 16/400 [03:03<1:13:18, 11.45s/it]  4%|▍         | 17/400 [03:14<1:13:07, 11.46s/it]  4%|▍         | 18/400 [03:26<1:12:57, 11.46s/it]  5%|▍         | 19/400 [03:37<1:12:49, 11.47s/it]  5%|▌         | 20/400 [03:49<1:12:40, 11.48s/it]  5%|▌         | 21/400 [04:00<1:12:29, 11.48s/it]  6%|▌         | 22/400 [04:12<1:12:19, 11.48s/it]  6%|▌         | 23/400 [04:23<1:12:07, 11.48s/it]  6%|▌         | 24/400 [04:35<1:11:54, 11.47s/it]  6%|▋         | 25/400 [04:46<1:11:43, 11.48s/it]  6%|▋         | 26/400 [04:58<1:11:31, 11.47s/it]  7%|▋         | 27/400 [05:09<1:11:20, 11.48s/it]  7%|▋         | 28/400 [05:21<1:11:06, 11.47s/it]  7%|▋         | 29/400 [05:32<1:10:57, 11.47s/it]  8%|▊         | 30/400 [05:44<1:10:45, 11.47s/it]  8%|▊         | 31/400 [05:55<1:10:35, 11.48s/it]  8%|▊         | 32/400 [06:06<1:10:21, 11.47s/it]  8%|▊         | 33/400 [06:18<1:10:09, 11.47s/it]  8%|▊         | 34/400 [06:29<1:09:59, 11.47s/it]  9%|▉         | 35/400 [06:41<1:09:49, 11.48s/it]  9%|▉         | 36/400 [06:52<1:09:38, 11.48s/it]  9%|▉         | 37/400 [07:04<1:09:24, 11.47s/it] 10%|▉         | 38/400 [07:15<1:09:13, 11.47s/it] 10%|▉         | 39/400 [07:27<1:09:03, 11.48s/it] 10%|█         | 40/400 [07:38<1:08:51, 11.48s/it] 10%|█         | 41/400 [07:50<1:08:40, 11.48s/it] 10%|█         | 42/400 [08:01<1:08:28, 11.48s/it] 11%|█         | 43/400 [08:13<1:08:16, 11.47s/it] 11%|█         | 44/400 [08:24<1:08:07, 11.48s/it] 11%|█▏        | 45/400 [08:36<1:07:57, 11.48s/it] 12%|█▏        | 46/400 [08:47<1:07:44, 11.48s/it] 12%|█▏        | 47/400 [08:59<1:07:33, 11.48s/it] 12%|█▏        | 48/400 [09:10<1:07:23, 11.49s/it] 12%|█▏        | 49/400 [09:22<1:07:12, 11.49s/it] 12%|█▎        | 50/400 [09:33<1:07:00, 11.49s/it][INFO|trainer.py:749] 2023-08-03 21:10:04,965 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: context, id, sentence. If context, id, sentence are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3083] 2023-08-03 21:10:04,967 >> ***** Running Evaluation *****
[INFO|trainer.py:3085] 2023-08-03 21:10:04,967 >>   Num examples = 400
[INFO|trainer.py:3088] 2023-08-03 21:10:04,968 >>   Batch size = 8

  0%|          | 0/50 [00:00<?, ?it/s][A
  4%|▍         | 2/50 [00:00<00:12,  3.93it/s][A
  6%|▌         | 3/50 [00:01<00:16,  2.78it/s][A
  8%|▊         | 4/50 [00:01<00:19,  2.41it/s][A
 10%|█         | 5/50 [00:02<00:20,  2.24it/s][A
 12%|█▏        | 6/50 [00:02<00:20,  2.14it/s][A
 14%|█▍        | 7/50 [00:03<00:20,  2.08it/s][A
 16%|█▌        | 8/50 [00:03<00:20,  2.05it/s][A
 18%|█▊        | 9/50 [00:04<00:20,  2.02it/s][A
 20%|██        | 10/50 [00:04<00:19,  2.01it/s][A
 22%|██▏       | 11/50 [00:05<00:19,  2.00it/s][A
 24%|██▍       | 12/50 [00:05<00:19,  1.99it/s][A
 26%|██▌       | 13/50 [00:06<00:18,  1.98it/s][A
 28%|██▊       | 14/50 [00:06<00:18,  1.97it/s][A
 30%|███       | 15/50 [00:07<00:17,  1.98it/s][A
 32%|███▏      | 16/50 [00:07<00:17,  1.97it/s][A
 34%|███▍      | 17/50 [00:08<00:16,  1.98it/s][A
 36%|███▌      | 18/50 [00:08<00:16,  1.98it/s][A
 38%|███▊      | 19/50 [00:09<00:15,  1.98it/s][A
 40%|████      | 20/50 [00:09<00:15,  1.97it/s][A
 42%|████▏     | 21/50 [00:10<00:14,  1.97it/s][A
 44%|████▍     | 22/50 [00:10<00:14,  1.97it/s][A
 46%|████▌     | 23/50 [00:11<00:13,  1.97it/s][A
 48%|████▊     | 24/50 [00:11<00:13,  1.97it/s][A
 50%|█████     | 25/50 [00:12<00:12,  1.97it/s][A
 52%|█████▏    | 26/50 [00:12<00:12,  1.97it/s][A
 54%|█████▍    | 27/50 [00:13<00:11,  1.97it/s][A
 56%|█████▌    | 28/50 [00:13<00:11,  1.97it/s][A
 58%|█████▊    | 29/50 [00:14<00:10,  1.97it/s][A
 60%|██████    | 30/50 [00:14<00:10,  1.97it/s][A
 62%|██████▏   | 31/50 [00:15<00:09,  1.97it/s][A
 64%|██████▍   | 32/50 [00:15<00:09,  1.97it/s][A
 66%|██████▌   | 33/50 [00:16<00:08,  1.97it/s][A
 68%|██████▊   | 34/50 [00:16<00:08,  1.97it/s][A
 70%|███████   | 35/50 [00:17<00:07,  1.97it/s][A
 72%|███████▏  | 36/50 [00:17<00:07,  1.97it/s][A
 74%|███████▍  | 37/50 [00:18<00:06,  1.97it/s][A
 76%|███████▌  | 38/50 [00:18<00:06,  1.97it/s][A
 78%|███████▊  | 39/50 [00:19<00:05,  1.97it/s][A
 80%|████████  | 40/50 [00:19<00:05,  1.97it/s][A
 82%|████████▏ | 41/50 [00:20<00:04,  1.97it/s][A
 84%|████████▍ | 42/50 [00:20<00:04,  1.97it/s][A
 86%|████████▌ | 43/50 [00:21<00:03,  1.97it/s][A
 88%|████████▊ | 44/50 [00:21<00:03,  1.97it/s][A
 90%|█████████ | 45/50 [00:22<00:02,  1.97it/s][A
 92%|█████████▏| 46/50 [00:22<00:02,  1.97it/s][A
 94%|█████████▍| 47/50 [00:23<00:01,  1.96it/s][A
 96%|█████████▌| 48/50 [00:23<00:01,  1.96it/s][A
 98%|█████████▊| 49/50 [00:24<00:00,  1.96it/s][A
100%|██████████| 50/50 [00:24<00:00,  1.97it/s][A                                                  
                                               [A 12%|█▎        | 50/400 [09:59<1:07:00, 11.49s/it]
100%|██████████| 50/50 [00:24<00:00,  1.97it/s][A
                                               [A[INFO|trainer.py:2809] 2023-08-03 21:10:30,422 >> Saving model checkpoint to ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-50
[INFO|configuration_utils.py:460] 2023-08-03 21:10:30,425 >> Configuration saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-50/config.json
[INFO|modeling_utils.py:1874] 2023-08-03 21:11:06,610 >> Model weights saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-50/pytorch_model.bin
[INFO|tokenization_utils_base.py:2227] 2023-08-03 21:11:06,616 >> tokenizer config file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-50/tokenizer_config.json
[INFO|tokenization_utils_base.py:2234] 2023-08-03 21:11:06,618 >> Special tokens file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-50/special_tokens_map.json
 13%|█▎        | 51/400 [12:00<5:03:52, 52.24s/it] 13%|█▎        | 52/400 [12:12<3:51:54, 39.99s/it] 13%|█▎        | 53/400 [12:23<3:01:36, 31.40s/it] 14%|█▎        | 54/400 [12:35<2:26:31, 25.41s/it] 14%|█▍        | 55/400 [12:46<2:01:58, 21.21s/it] 14%|█▍        | 56/400 [12:58<1:44:48, 18.28s/it] 14%|█▍        | 57/400 [13:09<1:32:48, 16.24s/it] 14%|█▍        | 58/400 [13:20<1:24:21, 14.80s/it] 15%|█▍        | 59/400 [13:32<1:18:24, 13.80s/it] 15%|█▌        | 60/400 [13:43<1:14:12, 13.10s/it] 15%|█▌        | 61/400 [13:55<1:11:12, 12.60s/it] 16%|█▌        | 62/400 [14:06<1:09:05, 12.26s/it] 16%|█▌        | 63/400 [14:18<1:07:34, 12.03s/it] 16%|█▌        | 64/400 [14:29<1:06:28, 11.87s/it] 16%|█▋        | 65/400 [14:41<1:05:39, 11.76s/it] 16%|█▋        | 66/400 [14:52<1:05:01, 11.68s/it] 17%|█▋        | 67/400 [15:04<1:04:34, 11.63s/it] 17%|█▋        | 68/400 [15:15<1:04:10, 11.60s/it] 17%|█▋        | 69/400 [15:27<1:03:49, 11.57s/it] 18%|█▊        | 70/400 [15:38<1:03:31, 11.55s/it] 18%|█▊        | 71/400 [15:50<1:03:15, 11.54s/it] 18%|█▊        | 72/400 [16:01<1:03:02, 11.53s/it] 18%|█▊        | 73/400 [16:13<1:02:47, 11.52s/it] 18%|█▊        | 74/400 [16:24<1:02:34, 11.52s/it] 19%|█▉        | 75/400 [16:36<1:02:22, 11.51s/it] 19%|█▉        | 76/400 [16:47<1:02:06, 11.50s/it] 19%|█▉        | 77/400 [16:59<1:01:54, 11.50s/it] 20%|█▉        | 78/400 [17:10<1:01:43, 11.50s/it] 20%|█▉        | 79/400 [17:22<1:01:32, 11.50s/it] 20%|██        | 80/400 [17:33<1:01:21, 11.51s/it] 20%|██        | 81/400 [17:45<1:01:09, 11.50s/it] 20%|██        | 82/400 [17:56<1:00:58, 11.50s/it] 21%|██        | 83/400 [18:08<1:00:46, 11.50s/it] 21%|██        | 84/400 [18:19<1:00:32, 11.50s/it] 21%|██▏       | 85/400 [18:31<1:00:20, 11.49s/it] 22%|██▏       | 86/400 [18:42<1:00:10, 11.50s/it] 22%|██▏       | 87/400 [18:54<59:58, 11.50s/it]   22%|██▏       | 88/400 [19:05<59:46, 11.49s/it] 22%|██▏       | 89/400 [19:17<59:34, 11.49s/it] 22%|██▎       | 90/400 [19:28<59:23, 11.50s/it] 23%|██▎       | 91/400 [19:40<59:12, 11.50s/it] 23%|██▎       | 92/400 [19:51<59:01, 11.50s/it] 23%|██▎       | 93/400 [20:03<58:48, 11.49s/it] 24%|██▎       | 94/400 [20:14<58:38, 11.50s/it] 24%|██▍       | 95/400 [20:26<58:26, 11.50s/it] 24%|██▍       | 96/400 [20:37<58:16, 11.50s/it] 24%|██▍       | 97/400 [20:49<58:04, 11.50s/it] 24%|██▍       | 98/400 [21:00<57:53, 11.50s/it] 25%|██▍       | 99/400 [21:12<57:40, 11.50s/it] 25%|██▌       | 100/400 [21:23<57:28, 11.49s/it][INFO|trainer.py:749] 2023-08-03 21:21:55,104 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: context, id, sentence. If context, id, sentence are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3083] 2023-08-03 21:21:55,106 >> ***** Running Evaluation *****
[INFO|trainer.py:3085] 2023-08-03 21:21:55,106 >>   Num examples = 400
[INFO|trainer.py:3088] 2023-08-03 21:21:55,106 >>   Batch size = 8
{'eval_loss': 1.06712007522583, 'eval_accuracy': 0.405, 'eval_runtime': 25.4436, 'eval_samples_per_second': 15.721, 'eval_steps_per_second': 1.965, 'epoch': 1.0}

  0%|          | 0/50 [00:00<?, ?it/s][A
  4%|▍         | 2/50 [00:00<00:12,  3.94it/s][A
  6%|▌         | 3/50 [00:01<00:16,  2.78it/s][A
  8%|▊         | 4/50 [00:01<00:19,  2.41it/s][A
 10%|█         | 5/50 [00:02<00:20,  2.24it/s][A
 12%|█▏        | 6/50 [00:02<00:20,  2.14it/s][A
 14%|█▍        | 7/50 [00:03<00:20,  2.08it/s][A
 16%|█▌        | 8/50 [00:03<00:20,  2.05it/s][A
 18%|█▊        | 9/50 [00:04<00:20,  2.03it/s][A
 20%|██        | 10/50 [00:04<00:19,  2.01it/s][A
 22%|██▏       | 11/50 [00:05<00:19,  2.00it/s][A
 24%|██▍       | 12/50 [00:05<00:19,  1.99it/s][A
 26%|██▌       | 13/50 [00:06<00:18,  1.98it/s][A
 28%|██▊       | 14/50 [00:06<00:18,  1.98it/s][A
 30%|███       | 15/50 [00:07<00:17,  1.98it/s][A
 32%|███▏      | 16/50 [00:07<00:17,  1.97it/s][A
 34%|███▍      | 17/50 [00:08<00:16,  1.97it/s][A
 36%|███▌      | 18/50 [00:08<00:16,  1.97it/s][A
 38%|███▊      | 19/50 [00:09<00:15,  1.97it/s][A
 40%|████      | 20/50 [00:09<00:15,  1.98it/s][A
 42%|████▏     | 21/50 [00:10<00:14,  1.98it/s][A
 44%|████▍     | 22/50 [00:10<00:14,  1.98it/s][A
 46%|████▌     | 23/50 [00:11<00:13,  1.97it/s][A
 48%|████▊     | 24/50 [00:11<00:13,  1.97it/s][A
 50%|█████     | 25/50 [00:12<00:12,  1.97it/s][A
 52%|█████▏    | 26/50 [00:12<00:12,  1.97it/s][A
 54%|█████▍    | 27/50 [00:13<00:11,  1.97it/s][A
 56%|█████▌    | 28/50 [00:13<00:11,  1.97it/s][A
 58%|█████▊    | 29/50 [00:14<00:10,  1.97it/s][A
 60%|██████    | 30/50 [00:14<00:10,  1.97it/s][A
 62%|██████▏   | 31/50 [00:15<00:09,  1.97it/s][A
 64%|██████▍   | 32/50 [00:15<00:09,  1.97it/s][A
 66%|██████▌   | 33/50 [00:16<00:08,  1.97it/s][A
 68%|██████▊   | 34/50 [00:16<00:08,  1.97it/s][A
 70%|███████   | 35/50 [00:17<00:07,  1.97it/s][A
 72%|███████▏  | 36/50 [00:17<00:07,  1.97it/s][A
 74%|███████▍  | 37/50 [00:18<00:06,  1.97it/s][A
 76%|███████▌  | 38/50 [00:18<00:06,  1.97it/s][A
 78%|███████▊  | 39/50 [00:19<00:05,  1.97it/s][A
 80%|████████  | 40/50 [00:19<00:05,  1.97it/s][A
 82%|████████▏ | 41/50 [00:20<00:04,  1.97it/s][A
 84%|████████▍ | 42/50 [00:20<00:04,  1.98it/s][A
 86%|████████▌ | 43/50 [00:21<00:03,  1.97it/s][A
 88%|████████▊ | 44/50 [00:21<00:03,  1.97it/s][A
 90%|█████████ | 45/50 [00:22<00:02,  1.98it/s][A
 92%|█████████▏| 46/50 [00:22<00:02,  1.97it/s][A
 94%|█████████▍| 47/50 [00:23<00:01,  1.97it/s][A
 96%|█████████▌| 48/50 [00:23<00:01,  1.97it/s][A
 98%|█████████▊| 49/50 [00:24<00:00,  1.97it/s][A
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A                                                 
                                               [A 25%|██▌       | 100/400 [21:49<57:28, 11.49s/it]
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A
                                               [A[INFO|trainer.py:2809] 2023-08-03 21:22:20,523 >> Saving model checkpoint to ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-100
[INFO|configuration_utils.py:460] 2023-08-03 21:22:20,525 >> Configuration saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-100/config.json
[INFO|modeling_utils.py:1874] 2023-08-03 21:22:56,268 >> Model weights saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-100/pytorch_model.bin
[INFO|tokenization_utils_base.py:2227] 2023-08-03 21:22:56,274 >> tokenizer config file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2234] 2023-08-03 21:22:56,276 >> Special tokens file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-100/special_tokens_map.json
 25%|██▌       | 101/400 [24:12<4:52:49, 58.76s/it] 26%|██▌       | 102/400 [24:24<3:41:13, 44.54s/it] 26%|██▌       | 103/400 [24:35<2:51:15, 34.60s/it] 26%|██▌       | 104/400 [24:46<2:16:22, 27.64s/it] 26%|██▋       | 105/400 [24:58<1:52:01, 22.78s/it] 26%|██▋       | 106/400 [25:09<1:35:01, 19.39s/it] 27%|██▋       | 107/400 [25:21<1:23:09, 17.03s/it] 27%|██▋       | 108/400 [25:32<1:14:48, 15.37s/it] 27%|██▋       | 109/400 [25:44<1:08:56, 14.21s/it] 28%|██▊       | 110/400 [25:55<1:04:46, 13.40s/it] 28%|██▊       | 111/400 [26:07<1:01:46, 12.83s/it] 28%|██▊       | 112/400 [26:18<59:40, 12.43s/it]   28%|██▊       | 113/400 [26:30<58:07, 12.15s/it] 28%|██▊       | 114/400 [26:41<57:00, 11.96s/it] 29%|██▉       | 115/400 [26:53<56:09, 11.82s/it] 29%|██▉       | 116/400 [27:04<55:30, 11.73s/it] 29%|██▉       | 117/400 [27:16<55:00, 11.66s/it] 30%|██▉       | 118/400 [27:27<54:35, 11.62s/it] 30%|██▉       | 119/400 [27:39<54:12, 11.58s/it] 30%|███       | 120/400 [27:50<53:55, 11.55s/it] 30%|███       | 121/400 [28:02<53:38, 11.54s/it] 30%|███       | 122/400 [28:13<53:23, 11.52s/it] 31%|███       | 123/400 [28:25<53:09, 11.52s/it] 31%|███       | 124/400 [28:36<52:57, 11.51s/it] 31%|███▏      | 125/400 [28:48<52:45, 11.51s/it] 32%|███▏      | 126/400 [28:59<52:32, 11.51s/it] 32%|███▏      | 127/400 [29:11<52:22, 11.51s/it] 32%|███▏      | 128/400 [29:22<52:09, 11.50s/it] 32%|███▏      | 129/400 [29:34<51:56, 11.50s/it] 32%|███▎      | 130/400 [29:45<51:43, 11.49s/it] 33%|███▎      | 131/400 [29:57<51:31, 11.49s/it] 33%|███▎      | 132/400 [30:08<51:20, 11.50s/it] 33%|███▎      | 133/400 [30:20<51:09, 11.50s/it] 34%|███▎      | 134/400 [30:31<50:57, 11.49s/it] 34%|███▍      | 135/400 [30:43<50:47, 11.50s/it] 34%|███▍      | 136/400 [30:54<50:32, 11.49s/it] 34%|███▍      | 137/400 [31:06<50:23, 11.49s/it] 34%|███▍      | 138/400 [31:17<50:10, 11.49s/it] 35%|███▍      | 139/400 [31:29<49:59, 11.49s/it] 35%|███▌      | 140/400 [31:40<49:48, 11.49s/it] 35%|███▌      | 141/400 [31:52<49:36, 11.49s/it] 36%|███▌      | 142/400 [32:03<49:23, 11.49s/it] 36%|███▌      | 143/400 [32:15<49:12, 11.49s/it] 36%|███▌      | 144/400 [32:26<48:59, 11.48s/it] 36%|███▋      | 145/400 [32:38<48:48, 11.49s/it] 36%|███▋      | 146/400 [32:49<48:38, 11.49s/it] 37%|███▋      | 147/400 [33:01<48:26, 11.49s/it] 37%|███▋      | 148/400 [33:12<48:15, 11.49s/it] 37%|███▋      | 149/400 [33:24<48:03, 11.49s/it] 38%|███▊      | 150/400 [33:35<47:49, 11.48s/it][INFO|trainer.py:749] 2023-08-03 21:34:07,054 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: context, id, sentence. If context, id, sentence are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3083] 2023-08-03 21:34:07,056 >> ***** Running Evaluation *****
[INFO|trainer.py:3085] 2023-08-03 21:34:07,056 >>   Num examples = 400
[INFO|trainer.py:3088] 2023-08-03 21:34:07,056 >>   Batch size = 8
{'eval_loss': 1.0573816299438477, 'eval_accuracy': 0.4475, 'eval_runtime': 25.4057, 'eval_samples_per_second': 15.744, 'eval_steps_per_second': 1.968, 'epoch': 2.0}

  0%|          | 0/50 [00:00<?, ?it/s][A
  4%|▍         | 2/50 [00:00<00:12,  3.97it/s][A
  6%|▌         | 3/50 [00:01<00:16,  2.80it/s][A
  8%|▊         | 4/50 [00:01<00:18,  2.42it/s][A
 10%|█         | 5/50 [00:02<00:20,  2.25it/s][A
 12%|█▏        | 6/50 [00:02<00:20,  2.15it/s][A
 14%|█▍        | 7/50 [00:03<00:20,  2.09it/s][A
 16%|█▌        | 8/50 [00:03<00:20,  2.06it/s][A
 18%|█▊        | 9/50 [00:04<00:20,  2.03it/s][A
 20%|██        | 10/50 [00:04<00:19,  2.02it/s][A
 22%|██▏       | 11/50 [00:05<00:19,  2.00it/s][A
 24%|██▍       | 12/50 [00:05<00:19,  1.99it/s][A
 26%|██▌       | 13/50 [00:06<00:18,  1.98it/s][A
 28%|██▊       | 14/50 [00:06<00:18,  1.98it/s][A
 30%|███       | 15/50 [00:07<00:17,  1.98it/s][A
 32%|███▏      | 16/50 [00:07<00:17,  1.98it/s][A
 34%|███▍      | 17/50 [00:08<00:16,  1.98it/s][A
 36%|███▌      | 18/50 [00:08<00:16,  1.98it/s][A
 38%|███▊      | 19/50 [00:09<00:15,  1.98it/s][A
 40%|████      | 20/50 [00:09<00:15,  1.98it/s][A
 42%|████▏     | 21/50 [00:10<00:14,  1.98it/s][A
 44%|████▍     | 22/50 [00:10<00:14,  1.98it/s][A
 46%|████▌     | 23/50 [00:11<00:13,  1.98it/s][A
 48%|████▊     | 24/50 [00:11<00:13,  1.97it/s][A
 50%|█████     | 25/50 [00:12<00:12,  1.97it/s][A
 52%|█████▏    | 26/50 [00:12<00:12,  1.97it/s][A
 54%|█████▍    | 27/50 [00:13<00:11,  1.97it/s][A
 56%|█████▌    | 28/50 [00:13<00:11,  1.97it/s][A
 58%|█████▊    | 29/50 [00:14<00:10,  1.97it/s][A
 60%|██████    | 30/50 [00:14<00:10,  1.98it/s][A
 62%|██████▏   | 31/50 [00:15<00:09,  1.98it/s][A
 64%|██████▍   | 32/50 [00:15<00:09,  1.98it/s][A
 66%|██████▌   | 33/50 [00:16<00:08,  1.97it/s][A
 68%|██████▊   | 34/50 [00:16<00:08,  1.97it/s][A
 70%|███████   | 35/50 [00:17<00:07,  1.97it/s][A
 72%|███████▏  | 36/50 [00:17<00:07,  1.97it/s][A
 74%|███████▍  | 37/50 [00:18<00:06,  1.98it/s][A
 76%|███████▌  | 38/50 [00:18<00:06,  1.98it/s][A
 78%|███████▊  | 39/50 [00:19<00:05,  1.98it/s][A
 80%|████████  | 40/50 [00:19<00:05,  1.98it/s][A
 82%|████████▏ | 41/50 [00:20<00:04,  1.97it/s][A
 84%|████████▍ | 42/50 [00:20<00:04,  1.98it/s][A
 86%|████████▌ | 43/50 [00:21<00:03,  1.97it/s][A
 88%|████████▊ | 44/50 [00:21<00:03,  1.97it/s][A
 90%|█████████ | 45/50 [00:22<00:02,  1.98it/s][A
 92%|█████████▏| 46/50 [00:22<00:02,  1.97it/s][A
 94%|█████████▍| 47/50 [00:23<00:01,  1.98it/s][A
 96%|█████████▌| 48/50 [00:23<00:01,  1.98it/s][A
 98%|█████████▊| 49/50 [00:24<00:00,  1.98it/s][A
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A                                                 
                                               [A 38%|███▊      | 150/400 [34:01<47:49, 11.48s/it]
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A
                                               [A[INFO|trainer.py:2809] 2023-08-03 21:34:32,422 >> Saving model checkpoint to ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-150
[INFO|configuration_utils.py:460] 2023-08-03 21:34:32,424 >> Configuration saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-150/config.json
[INFO|modeling_utils.py:1874] 2023-08-03 21:35:06,577 >> Model weights saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-150/pytorch_model.bin
[INFO|tokenization_utils_base.py:2227] 2023-08-03 21:35:06,581 >> tokenizer config file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-150/tokenizer_config.json
[INFO|tokenization_utils_base.py:2234] 2023-08-03 21:35:06,583 >> Special tokens file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-150/special_tokens_map.json
 38%|███▊      | 151/400 [36:18<3:55:30, 56.75s/it] 38%|███▊      | 152/400 [36:29<2:58:16, 43.13s/it] 38%|███▊      | 153/400 [36:40<2:18:21, 33.61s/it] 38%|███▊      | 154/400 [36:52<1:50:30, 26.95s/it] 39%|███▉      | 155/400 [37:03<1:31:03, 22.30s/it] 39%|███▉      | 156/400 [37:15<1:17:29, 19.06s/it] 39%|███▉      | 157/400 [37:26<1:08:00, 16.79s/it] 40%|███▉      | 158/400 [37:38<1:01:20, 15.21s/it] 40%|███▉      | 159/400 [37:49<56:41, 14.11s/it]   40%|████      | 160/400 [38:01<53:21, 13.34s/it] 40%|████      | 161/400 [38:12<50:57, 12.79s/it] 40%|████      | 162/400 [38:24<49:14, 12.41s/it] 41%|████      | 163/400 [38:35<47:57, 12.14s/it] 41%|████      | 164/400 [38:47<46:59, 11.95s/it] 41%|████▏     | 165/400 [38:58<46:16, 11.82s/it] 42%|████▏     | 166/400 [39:10<45:44, 11.73s/it] 42%|████▏     | 167/400 [39:21<45:15, 11.66s/it] 42%|████▏     | 168/400 [39:33<44:52, 11.61s/it] 42%|████▏     | 169/400 [39:44<44:34, 11.58s/it] 42%|████▎     | 170/400 [39:56<44:16, 11.55s/it] 43%|████▎     | 171/400 [40:07<43:59, 11.53s/it] 43%|████▎     | 172/400 [40:19<43:46, 11.52s/it] 43%|████▎     | 173/400 [40:30<43:34, 11.52s/it] 44%|████▎     | 174/400 [40:42<43:22, 11.52s/it] 44%|████▍     | 175/400 [40:53<43:10, 11.51s/it] 44%|████▍     | 176/400 [41:05<42:57, 11.50s/it] 44%|████▍     | 177/400 [41:16<42:44, 11.50s/it] 44%|████▍     | 178/400 [41:28<42:31, 11.49s/it] 45%|████▍     | 179/400 [41:39<42:21, 11.50s/it] 45%|████▌     | 180/400 [41:51<42:09, 11.50s/it] 45%|████▌     | 181/400 [42:02<41:57, 11.50s/it] 46%|████▌     | 182/400 [42:14<41:43, 11.49s/it] 46%|████▌     | 183/400 [42:25<41:31, 11.48s/it] 46%|████▌     | 184/400 [42:37<41:20, 11.49s/it] 46%|████▋     | 185/400 [42:48<41:09, 11.48s/it] 46%|████▋     | 186/400 [43:00<40:58, 11.49s/it] 47%|████▋     | 187/400 [43:11<40:47, 11.49s/it] 47%|████▋     | 188/400 [43:23<40:36, 11.49s/it] 47%|████▋     | 189/400 [43:34<40:26, 11.50s/it] 48%|████▊     | 190/400 [43:46<40:14, 11.50s/it] 48%|████▊     | 191/400 [43:57<40:00, 11.49s/it] 48%|████▊     | 192/400 [44:09<39:48, 11.48s/it] 48%|████▊     | 193/400 [44:20<39:36, 11.48s/it] 48%|████▊     | 194/400 [44:32<39:24, 11.48s/it] 49%|████▉     | 195/400 [44:43<39:13, 11.48s/it] 49%|████▉     | 196/400 [44:55<39:00, 11.48s/it] 49%|████▉     | 197/400 [45:06<38:48, 11.47s/it] 50%|████▉     | 198/400 [45:17<38:37, 11.47s/it] 50%|████▉     | 199/400 [45:29<38:25, 11.47s/it] 50%|█████     | 200/400 [45:40<38:15, 11.48s/it][INFO|trainer.py:749] 2023-08-03 21:46:12,280 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: context, id, sentence. If context, id, sentence are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3083] 2023-08-03 21:46:12,282 >> ***** Running Evaluation *****
[INFO|trainer.py:3085] 2023-08-03 21:46:12,282 >>   Num examples = 400
[INFO|trainer.py:3088] 2023-08-03 21:46:12,282 >>   Batch size = 8
{'eval_loss': 1.0110676288604736, 'eval_accuracy': 0.515, 'eval_runtime': 25.3594, 'eval_samples_per_second': 15.773, 'eval_steps_per_second': 1.972, 'epoch': 3.0}

  0%|          | 0/50 [00:00<?, ?it/s][A
  4%|▍         | 2/50 [00:00<00:12,  3.97it/s][A
  6%|▌         | 3/50 [00:01<00:16,  2.80it/s][A
  8%|▊         | 4/50 [00:01<00:18,  2.42it/s][A
 10%|█         | 5/50 [00:02<00:19,  2.25it/s][A
 12%|█▏        | 6/50 [00:02<00:20,  2.15it/s][A
 14%|█▍        | 7/50 [00:03<00:20,  2.09it/s][A
 16%|█▌        | 8/50 [00:03<00:20,  2.05it/s][A
 18%|█▊        | 9/50 [00:04<00:20,  2.02it/s][A
 20%|██        | 10/50 [00:04<00:19,  2.01it/s][A
 22%|██▏       | 11/50 [00:05<00:19,  2.00it/s][A
 24%|██▍       | 12/50 [00:05<00:19,  1.99it/s][A
 26%|██▌       | 13/50 [00:06<00:18,  1.98it/s][A
 28%|██▊       | 14/50 [00:06<00:18,  1.98it/s][A
 30%|███       | 15/50 [00:07<00:17,  1.98it/s][A
 32%|███▏      | 16/50 [00:07<00:17,  1.98it/s][A
 34%|███▍      | 17/50 [00:08<00:16,  1.98it/s][A
 36%|███▌      | 18/50 [00:08<00:16,  1.98it/s][A
 38%|███▊      | 19/50 [00:09<00:15,  1.98it/s][A
 40%|████      | 20/50 [00:09<00:15,  1.98it/s][A
 42%|████▏     | 21/50 [00:10<00:14,  1.98it/s][A
 44%|████▍     | 22/50 [00:10<00:14,  1.98it/s][A
 46%|████▌     | 23/50 [00:11<00:13,  1.98it/s][A
 48%|████▊     | 24/50 [00:11<00:13,  1.97it/s][A
 50%|█████     | 25/50 [00:12<00:12,  1.97it/s][A
 52%|█████▏    | 26/50 [00:12<00:12,  1.97it/s][A
 54%|█████▍    | 27/50 [00:13<00:11,  1.97it/s][A
 56%|█████▌    | 28/50 [00:13<00:11,  1.97it/s][A
 58%|█████▊    | 29/50 [00:14<00:10,  1.97it/s][A
 60%|██████    | 30/50 [00:14<00:10,  1.97it/s][A
 62%|██████▏   | 31/50 [00:15<00:09,  1.97it/s][A
 64%|██████▍   | 32/50 [00:15<00:09,  1.97it/s][A
 66%|██████▌   | 33/50 [00:16<00:08,  1.97it/s][A
 68%|██████▊   | 34/50 [00:16<00:08,  1.97it/s][A
 70%|███████   | 35/50 [00:17<00:07,  1.97it/s][A
 72%|███████▏  | 36/50 [00:17<00:07,  1.97it/s][A
 74%|███████▍  | 37/50 [00:18<00:06,  1.98it/s][A
 76%|███████▌  | 38/50 [00:18<00:06,  1.98it/s][A
 78%|███████▊  | 39/50 [00:19<00:05,  1.98it/s][A
 80%|████████  | 40/50 [00:19<00:05,  1.98it/s][A
 82%|████████▏ | 41/50 [00:20<00:04,  1.97it/s][A
 84%|████████▍ | 42/50 [00:20<00:04,  1.98it/s][A
 86%|████████▌ | 43/50 [00:21<00:03,  1.98it/s][A
 88%|████████▊ | 44/50 [00:21<00:03,  1.98it/s][A
 90%|█████████ | 45/50 [00:22<00:02,  1.98it/s][A
 92%|█████████▏| 46/50 [00:22<00:02,  1.98it/s][A
 94%|█████████▍| 47/50 [00:23<00:01,  1.98it/s][A
 96%|█████████▌| 48/50 [00:23<00:01,  1.98it/s][A
 98%|█████████▊| 49/50 [00:24<00:00,  1.97it/s][A
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A                                                 
                                               [A 50%|█████     | 200/400 [46:06<38:15, 11.48s/it]
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A
                                               [A[INFO|trainer.py:2809] 2023-08-03 21:46:37,653 >> Saving model checkpoint to ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-200
[INFO|configuration_utils.py:460] 2023-08-03 21:46:37,655 >> Configuration saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-200/config.json
[INFO|modeling_utils.py:1874] 2023-08-03 21:47:14,567 >> Model weights saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-200/pytorch_model.bin
[INFO|tokenization_utils_base.py:2227] 2023-08-03 21:47:14,570 >> tokenizer config file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2234] 2023-08-03 21:47:14,571 >> Special tokens file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-200/special_tokens_map.json
 50%|█████     | 201/400 [48:33<3:18:38, 59.89s/it] 50%|█████     | 202/400 [48:45<2:29:36, 45.34s/it] 51%|█████     | 203/400 [48:56<1:55:26, 35.16s/it] 51%|█████     | 204/400 [49:08<1:31:38, 28.05s/it] 51%|█████▏    | 205/400 [49:19<1:15:00, 23.08s/it] 52%|█████▏    | 206/400 [49:31<1:03:24, 19.61s/it] 52%|█████▏    | 207/400 [49:42<55:17, 17.19s/it]   52%|█████▏    | 208/400 [49:54<49:35, 15.50s/it] 52%|█████▏    | 209/400 [50:05<45:33, 14.31s/it] 52%|█████▎    | 210/400 [50:17<42:40, 13.48s/it] 53%|█████▎    | 211/400 [50:28<40:36, 12.89s/it] 53%|█████▎    | 212/400 [50:40<39:07, 12.49s/it] 53%|█████▎    | 213/400 [50:51<37:59, 12.19s/it] 54%|█████▎    | 214/400 [51:03<37:10, 11.99s/it] 54%|█████▍    | 215/400 [51:14<36:31, 11.85s/it] 54%|█████▍    | 216/400 [51:26<36:01, 11.74s/it] 54%|█████▍    | 217/400 [51:37<35:35, 11.67s/it] 55%|█████▍    | 218/400 [51:49<35:13, 11.62s/it] 55%|█████▍    | 219/400 [52:00<34:57, 11.59s/it] 55%|█████▌    | 220/400 [52:12<34:40, 11.56s/it] 55%|█████▌    | 221/400 [52:23<34:23, 11.53s/it] 56%|█████▌    | 222/400 [52:35<34:10, 11.52s/it] 56%|█████▌    | 223/400 [52:46<33:56, 11.51s/it] 56%|█████▌    | 224/400 [52:58<33:43, 11.50s/it] 56%|█████▋    | 225/400 [53:09<33:31, 11.49s/it] 56%|█████▋    | 226/400 [53:21<33:19, 11.49s/it] 57%|█████▋    | 227/400 [53:32<33:08, 11.49s/it] 57%|█████▋    | 228/400 [53:44<32:56, 11.49s/it] 57%|█████▋    | 229/400 [53:55<32:44, 11.49s/it] 57%|█████▊    | 230/400 [54:07<32:32, 11.49s/it] 58%|█████▊    | 231/400 [54:18<32:22, 11.49s/it] 58%|█████▊    | 232/400 [54:30<32:11, 11.50s/it] 58%|█████▊    | 233/400 [54:41<31:58, 11.49s/it] 58%|█████▊    | 234/400 [54:53<31:46, 11.49s/it] 59%|█████▉    | 235/400 [55:04<31:35, 11.49s/it] 59%|█████▉    | 236/400 [55:16<31:24, 11.49s/it] 59%|█████▉    | 237/400 [55:27<31:13, 11.49s/it] 60%|█████▉    | 238/400 [55:39<31:02, 11.49s/it] 60%|█████▉    | 239/400 [55:50<30:50, 11.49s/it] 60%|██████    | 240/400 [56:02<30:38, 11.49s/it] 60%|██████    | 241/400 [56:13<30:26, 11.49s/it] 60%|██████    | 242/400 [56:25<30:15, 11.49s/it] 61%|██████    | 243/400 [56:36<30:04, 11.49s/it] 61%|██████    | 244/400 [56:48<29:51, 11.48s/it] 61%|██████▏   | 245/400 [56:59<29:40, 11.49s/it] 62%|██████▏   | 246/400 [57:11<29:28, 11.49s/it] 62%|██████▏   | 247/400 [57:22<29:17, 11.49s/it] 62%|██████▏   | 248/400 [57:33<29:05, 11.48s/it] 62%|██████▏   | 249/400 [57:45<28:53, 11.48s/it] 62%|██████▎   | 250/400 [57:56<28:41, 11.48s/it][INFO|trainer.py:749] 2023-08-03 21:58:28,253 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: context, id, sentence. If context, id, sentence are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3083] 2023-08-03 21:58:28,255 >> ***** Running Evaluation *****
[INFO|trainer.py:3085] 2023-08-03 21:58:28,255 >>   Num examples = 400
[INFO|trainer.py:3088] 2023-08-03 21:58:28,255 >>   Batch size = 8
{'eval_loss': 0.9235303997993469, 'eval_accuracy': 0.5975, 'eval_runtime': 25.3642, 'eval_samples_per_second': 15.77, 'eval_steps_per_second': 1.971, 'epoch': 4.0}

  0%|          | 0/50 [00:00<?, ?it/s][A
  4%|▍         | 2/50 [00:00<00:12,  3.94it/s][A
  6%|▌         | 3/50 [00:01<00:16,  2.79it/s][A
  8%|▊         | 4/50 [00:01<00:19,  2.41it/s][A
 10%|█         | 5/50 [00:02<00:20,  2.24it/s][A
 12%|█▏        | 6/50 [00:02<00:20,  2.14it/s][A
 14%|█▍        | 7/50 [00:03<00:20,  2.09it/s][A
 16%|█▌        | 8/50 [00:03<00:20,  2.06it/s][A
 18%|█▊        | 9/50 [00:04<00:20,  2.03it/s][A
 20%|██        | 10/50 [00:04<00:19,  2.02it/s][A
 22%|██▏       | 11/50 [00:05<00:19,  2.00it/s][A
 24%|██▍       | 12/50 [00:05<00:19,  1.98it/s][A
 26%|██▌       | 13/50 [00:06<00:18,  1.98it/s][A
 28%|██▊       | 14/50 [00:06<00:18,  1.98it/s][A
 30%|███       | 15/50 [00:07<00:17,  1.98it/s][A
 32%|███▏      | 16/50 [00:07<00:17,  1.98it/s][A
 34%|███▍      | 17/50 [00:08<00:16,  1.98it/s][A
 36%|███▌      | 18/50 [00:08<00:16,  1.98it/s][A
 38%|███▊      | 19/50 [00:09<00:15,  1.98it/s][A
 40%|████      | 20/50 [00:09<00:15,  1.98it/s][A
 42%|████▏     | 21/50 [00:10<00:14,  1.99it/s][A
 44%|████▍     | 22/50 [00:10<00:14,  1.98it/s][A
 46%|████▌     | 23/50 [00:11<00:13,  1.98it/s][A
 48%|████▊     | 24/50 [00:11<00:13,  1.97it/s][A
 50%|█████     | 25/50 [00:12<00:12,  1.97it/s][A
 52%|█████▏    | 26/50 [00:12<00:12,  1.97it/s][A
 54%|█████▍    | 27/50 [00:13<00:11,  1.97it/s][A
 56%|█████▌    | 28/50 [00:13<00:11,  1.97it/s][A
 58%|█████▊    | 29/50 [00:14<00:10,  1.98it/s][A
 60%|██████    | 30/50 [00:14<00:10,  1.98it/s][A
 62%|██████▏   | 31/50 [00:15<00:09,  1.98it/s][A
 64%|██████▍   | 32/50 [00:15<00:09,  1.97it/s][A
 66%|██████▌   | 33/50 [00:16<00:08,  1.97it/s][A
 68%|██████▊   | 34/50 [00:16<00:08,  1.97it/s][A
 70%|███████   | 35/50 [00:17<00:07,  1.97it/s][A
 72%|███████▏  | 36/50 [00:17<00:07,  1.98it/s][A
 74%|███████▍  | 37/50 [00:18<00:06,  1.98it/s][A
 76%|███████▌  | 38/50 [00:18<00:06,  1.98it/s][A
 78%|███████▊  | 39/50 [00:19<00:05,  1.98it/s][A
 80%|████████  | 40/50 [00:19<00:05,  1.98it/s][A
 82%|████████▏ | 41/50 [00:20<00:04,  1.97it/s][A
 84%|████████▍ | 42/50 [00:20<00:04,  1.98it/s][A
 86%|████████▌ | 43/50 [00:21<00:03,  1.97it/s][A
 88%|████████▊ | 44/50 [00:21<00:03,  1.97it/s][A
 90%|█████████ | 45/50 [00:22<00:02,  1.98it/s][A
 92%|█████████▏| 46/50 [00:22<00:02,  1.97it/s][A
 94%|█████████▍| 47/50 [00:23<00:01,  1.98it/s][A
 96%|█████████▌| 48/50 [00:23<00:01,  1.98it/s][A
 98%|█████████▊| 49/50 [00:24<00:00,  1.98it/s][A
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A                                                 
                                               [A 62%|██████▎   | 250/400 [58:22<28:41, 11.48s/it]
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A
                                               [A[INFO|trainer.py:2809] 2023-08-03 21:58:53,617 >> Saving model checkpoint to ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-250
[INFO|configuration_utils.py:460] 2023-08-03 21:58:53,620 >> Configuration saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-250/config.json
[INFO|modeling_utils.py:1874] 2023-08-03 21:59:28,028 >> Model weights saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-250/pytorch_model.bin
[INFO|tokenization_utils_base.py:2227] 2023-08-03 21:59:28,032 >> tokenizer config file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-250/tokenizer_config.json
[INFO|tokenization_utils_base.py:2234] 2023-08-03 21:59:28,033 >> Special tokens file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-250/special_tokens_map.json
 63%|██████▎   | 251/400 [1:00:39<2:20:53, 56.74s/it] 63%|██████▎   | 252/400 [1:00:50<1:46:22, 43.13s/it] 63%|██████▎   | 253/400 [1:01:02<1:22:20, 33.61s/it] 64%|██████▎   | 254/400 [1:01:13<1:05:35, 26.96s/it] 64%|██████▍   | 255/400 [1:01:24<53:54, 22.31s/it]   64%|██████▍   | 256/400 [1:01:36<45:46, 19.07s/it] 64%|██████▍   | 257/400 [1:01:47<40:04, 16.81s/it] 64%|██████▍   | 258/400 [1:01:59<36:02, 15.23s/it] 65%|██████▍   | 259/400 [1:02:11<33:11, 14.13s/it] 65%|██████▌   | 260/400 [1:02:22<31:08, 13.34s/it] 65%|██████▌   | 261/400 [1:02:34<29:39, 12.81s/it] 66%|██████▌   | 262/400 [1:02:45<28:33, 12.41s/it] 66%|██████▌   | 263/400 [1:02:57<27:44, 12.15s/it] 66%|██████▌   | 264/400 [1:03:08<27:07, 11.96s/it] 66%|██████▋   | 265/400 [1:03:20<26:36, 11.83s/it] 66%|██████▋   | 266/400 [1:03:31<26:11, 11.73s/it] 67%|██████▋   | 267/400 [1:03:43<25:51, 11.66s/it] 67%|██████▋   | 268/400 [1:03:54<25:32, 11.61s/it] 67%|██████▋   | 269/400 [1:04:06<25:16, 11.58s/it] 68%|██████▊   | 270/400 [1:04:17<25:02, 11.56s/it] 68%|██████▊   | 271/400 [1:04:29<24:48, 11.54s/it] 68%|██████▊   | 272/400 [1:04:40<24:35, 11.53s/it] 68%|██████▊   | 273/400 [1:04:52<24:23, 11.52s/it] 68%|██████▊   | 274/400 [1:05:03<24:09, 11.51s/it] 69%|██████▉   | 275/400 [1:05:15<23:57, 11.50s/it] 69%|██████▉   | 276/400 [1:05:26<23:45, 11.50s/it] 69%|██████▉   | 277/400 [1:05:38<23:33, 11.49s/it] 70%|██████▉   | 278/400 [1:05:49<23:21, 11.49s/it] 70%|██████▉   | 279/400 [1:06:01<23:10, 11.49s/it] 70%|███████   | 280/400 [1:06:12<22:58, 11.48s/it] 70%|███████   | 281/400 [1:06:24<22:46, 11.49s/it] 70%|███████   | 282/400 [1:06:35<22:35, 11.48s/it] 71%|███████   | 283/400 [1:06:47<22:23, 11.49s/it] 71%|███████   | 284/400 [1:06:58<22:12, 11.49s/it] 71%|███████▏  | 285/400 [1:07:10<22:01, 11.49s/it] 72%|███████▏  | 286/400 [1:07:21<21:48, 11.48s/it] 72%|███████▏  | 287/400 [1:07:32<21:37, 11.48s/it] 72%|███████▏  | 288/400 [1:07:44<21:26, 11.48s/it] 72%|███████▏  | 289/400 [1:07:55<21:14, 11.49s/it] 72%|███████▎  | 290/400 [1:08:07<21:03, 11.48s/it] 73%|███████▎  | 291/400 [1:08:18<20:52, 11.49s/it] 73%|███████▎  | 292/400 [1:08:30<20:40, 11.48s/it] 73%|███████▎  | 293/400 [1:08:41<20:27, 11.47s/it] 74%|███████▎  | 294/400 [1:08:53<20:16, 11.47s/it] 74%|███████▍  | 295/400 [1:09:04<20:04, 11.47s/it] 74%|███████▍  | 296/400 [1:09:16<19:53, 11.48s/it] 74%|███████▍  | 297/400 [1:09:27<19:42, 11.48s/it] 74%|███████▍  | 298/400 [1:09:39<19:31, 11.48s/it] 75%|███████▍  | 299/400 [1:09:50<19:19, 11.48s/it] 75%|███████▌  | 300/400 [1:10:02<19:08, 11.48s/it][INFO|trainer.py:749] 2023-08-03 22:10:33,583 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: context, id, sentence. If context, id, sentence are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3083] 2023-08-03 22:10:33,585 >> ***** Running Evaluation *****
[INFO|trainer.py:3085] 2023-08-03 22:10:33,585 >>   Num examples = 400
[INFO|trainer.py:3088] 2023-08-03 22:10:33,585 >>   Batch size = 8
{'eval_loss': 0.9131903648376465, 'eval_accuracy': 0.6125, 'eval_runtime': 25.3542, 'eval_samples_per_second': 15.776, 'eval_steps_per_second': 1.972, 'epoch': 5.0}

  0%|          | 0/50 [00:00<?, ?it/s][A
  4%|▍         | 2/50 [00:00<00:12,  3.97it/s][A
  6%|▌         | 3/50 [00:01<00:16,  2.80it/s][A
  8%|▊         | 4/50 [00:01<00:18,  2.42it/s][A
 10%|█         | 5/50 [00:02<00:19,  2.25it/s][A
 12%|█▏        | 6/50 [00:02<00:20,  2.15it/s][A
 14%|█▍        | 7/50 [00:03<00:20,  2.09it/s][A
 16%|█▌        | 8/50 [00:03<00:20,  2.06it/s][A
 18%|█▊        | 9/50 [00:04<00:20,  2.03it/s][A
 20%|██        | 10/50 [00:04<00:19,  2.01it/s][A
 22%|██▏       | 11/50 [00:05<00:19,  2.00it/s][A
 24%|██▍       | 12/50 [00:05<00:19,  1.99it/s][A
 26%|██▌       | 13/50 [00:06<00:18,  1.99it/s][A
 28%|██▊       | 14/50 [00:06<00:18,  1.98it/s][A
 30%|███       | 15/50 [00:07<00:17,  1.98it/s][A
 32%|███▏      | 16/50 [00:07<00:17,  1.98it/s][A
 34%|███▍      | 17/50 [00:08<00:16,  1.98it/s][A
 36%|███▌      | 18/50 [00:08<00:16,  1.98it/s][A
 38%|███▊      | 19/50 [00:09<00:15,  1.98it/s][A
 40%|████      | 20/50 [00:09<00:15,  1.98it/s][A
 42%|████▏     | 21/50 [00:10<00:14,  1.98it/s][A
 44%|████▍     | 22/50 [00:10<00:14,  1.98it/s][A
 46%|████▌     | 23/50 [00:11<00:13,  1.98it/s][A
 48%|████▊     | 24/50 [00:11<00:13,  1.98it/s][A
 50%|█████     | 25/50 [00:12<00:12,  1.97it/s][A
 52%|█████▏    | 26/50 [00:12<00:12,  1.97it/s][A
 54%|█████▍    | 27/50 [00:13<00:11,  1.97it/s][A
 56%|█████▌    | 28/50 [00:13<00:11,  1.97it/s][A
 58%|█████▊    | 29/50 [00:14<00:10,  1.98it/s][A
 60%|██████    | 30/50 [00:14<00:10,  1.98it/s][A
 62%|██████▏   | 31/50 [00:15<00:09,  1.98it/s][A
 64%|██████▍   | 32/50 [00:15<00:09,  1.98it/s][A
 66%|██████▌   | 33/50 [00:16<00:08,  1.97it/s][A
 68%|██████▊   | 34/50 [00:16<00:08,  1.97it/s][A
 70%|███████   | 35/50 [00:17<00:07,  1.97it/s][A
 72%|███████▏  | 36/50 [00:17<00:07,  1.97it/s][A
 74%|███████▍  | 37/50 [00:18<00:06,  1.98it/s][A
 76%|███████▌  | 38/50 [00:18<00:06,  1.98it/s][A
 78%|███████▊  | 39/50 [00:19<00:05,  1.98it/s][A
 80%|████████  | 40/50 [00:19<00:05,  1.98it/s][A
 82%|████████▏ | 41/50 [00:20<00:04,  1.97it/s][A
 84%|████████▍ | 42/50 [00:20<00:04,  1.98it/s][A
 86%|████████▌ | 43/50 [00:21<00:03,  1.97it/s][A
 88%|████████▊ | 44/50 [00:21<00:03,  1.97it/s][A
 90%|█████████ | 45/50 [00:22<00:02,  1.98it/s][A
 92%|█████████▏| 46/50 [00:22<00:02,  1.97it/s][A
 94%|█████████▍| 47/50 [00:23<00:01,  1.98it/s][A
 96%|█████████▌| 48/50 [00:23<00:01,  1.98it/s][A
 98%|█████████▊| 49/50 [00:24<00:00,  1.98it/s][A
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A                                                   
                                               [A 75%|███████▌  | 300/400 [1:10:27<19:08, 11.48s/it]
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A
                                               [A[INFO|trainer.py:2809] 2023-08-03 22:10:58,956 >> Saving model checkpoint to ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-300
[INFO|configuration_utils.py:460] 2023-08-03 22:10:58,960 >> Configuration saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-300/config.json
[INFO|modeling_utils.py:1874] 2023-08-03 22:11:34,799 >> Model weights saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-300/pytorch_model.bin
[INFO|tokenization_utils_base.py:2227] 2023-08-03 22:11:34,802 >> tokenizer config file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2234] 2023-08-03 22:11:34,803 >> Special tokens file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-300/special_tokens_map.json
 75%|███████▌  | 301/400 [1:12:36<1:29:26, 54.21s/it] 76%|███████▌  | 302/400 [1:12:47<1:07:32, 41.35s/it] 76%|███████▌  | 303/400 [1:12:58<52:18, 32.36s/it]   76%|███████▌  | 304/400 [1:13:10<41:42, 26.07s/it] 76%|███████▋  | 305/400 [1:13:21<34:20, 21.69s/it] 76%|███████▋  | 306/400 [1:13:33<29:12, 18.64s/it] 77%|███████▋  | 307/400 [1:13:44<25:34, 16.50s/it] 77%|███████▋  | 308/400 [1:13:56<23:01, 15.01s/it] 77%|███████▋  | 309/400 [1:14:07<21:11, 13.97s/it] 78%|███████▊  | 310/400 [1:14:19<19:51, 13.24s/it] 78%|███████▊  | 311/400 [1:14:30<18:53, 12.74s/it] 78%|███████▊  | 312/400 [1:14:42<18:09, 12.38s/it] 78%|███████▊  | 313/400 [1:14:54<17:34, 12.12s/it] 78%|███████▊  | 314/400 [1:15:05<17:07, 11.95s/it] 79%|███████▉  | 315/400 [1:15:17<16:44, 11.82s/it] 79%|███████▉  | 316/400 [1:15:28<16:25, 11.73s/it] 79%|███████▉  | 317/400 [1:15:40<16:08, 11.66s/it] 80%|███████▉  | 318/400 [1:15:51<15:52, 11.62s/it] 80%|███████▉  | 319/400 [1:16:03<15:38, 11.58s/it] 80%|████████  | 320/400 [1:16:14<15:24, 11.56s/it] 80%|████████  | 321/400 [1:16:26<15:11, 11.54s/it] 80%|████████  | 322/400 [1:16:37<14:58, 11.52s/it] 81%|████████  | 323/400 [1:16:49<14:46, 11.51s/it] 81%|████████  | 324/400 [1:17:00<14:34, 11.50s/it] 81%|████████▏ | 325/400 [1:17:12<14:21, 11.49s/it] 82%|████████▏ | 326/400 [1:17:23<14:09, 11.48s/it] 82%|████████▏ | 327/400 [1:17:34<13:58, 11.48s/it] 82%|████████▏ | 328/400 [1:17:46<13:46, 11.48s/it] 82%|████████▏ | 329/400 [1:17:57<13:35, 11.49s/it] 82%|████████▎ | 330/400 [1:18:09<13:23, 11.48s/it] 83%|████████▎ | 331/400 [1:18:20<13:12, 11.49s/it] 83%|████████▎ | 332/400 [1:18:32<13:01, 11.49s/it] 83%|████████▎ | 333/400 [1:18:43<12:50, 11.49s/it] 84%|████████▎ | 334/400 [1:18:55<12:37, 11.48s/it] 84%|████████▍ | 335/400 [1:19:06<12:26, 11.48s/it] 84%|████████▍ | 336/400 [1:19:18<12:15, 11.49s/it] 84%|████████▍ | 337/400 [1:19:29<12:03, 11.48s/it] 84%|████████▍ | 338/400 [1:19:41<11:51, 11.48s/it] 85%|████████▍ | 339/400 [1:19:52<11:40, 11.48s/it] 85%|████████▌ | 340/400 [1:20:04<11:28, 11.48s/it] 85%|████████▌ | 341/400 [1:20:15<11:17, 11.48s/it] 86%|████████▌ | 342/400 [1:20:27<11:05, 11.47s/it] 86%|████████▌ | 343/400 [1:20:38<10:54, 11.47s/it] 86%|████████▌ | 344/400 [1:20:50<10:42, 11.47s/it] 86%|████████▋ | 345/400 [1:21:01<10:31, 11.48s/it] 86%|████████▋ | 346/400 [1:21:13<10:19, 11.48s/it] 87%|████████▋ | 347/400 [1:21:24<10:08, 11.48s/it] 87%|████████▋ | 348/400 [1:21:36<09:56, 11.48s/it] 87%|████████▋ | 349/400 [1:21:47<09:45, 11.49s/it] 88%|████████▊ | 350/400 [1:21:59<09:34, 11.48s/it][INFO|trainer.py:749] 2023-08-03 22:22:30,387 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: context, id, sentence. If context, id, sentence are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3083] 2023-08-03 22:22:30,389 >> ***** Running Evaluation *****
[INFO|trainer.py:3085] 2023-08-03 22:22:30,389 >>   Num examples = 400
[INFO|trainer.py:3088] 2023-08-03 22:22:30,389 >>   Batch size = 8
{'eval_loss': 0.882086992263794, 'eval_accuracy': 0.6275, 'eval_runtime': 25.3605, 'eval_samples_per_second': 15.773, 'eval_steps_per_second': 1.972, 'epoch': 6.0}

  0%|          | 0/50 [00:00<?, ?it/s][A
  4%|▍         | 2/50 [00:00<00:12,  3.94it/s][A
  6%|▌         | 3/50 [00:01<00:16,  2.79it/s][A
  8%|▊         | 4/50 [00:01<00:19,  2.41it/s][A
 10%|█         | 5/50 [00:02<00:20,  2.24it/s][A
 12%|█▏        | 6/50 [00:02<00:20,  2.15it/s][A
 14%|█▍        | 7/50 [00:03<00:20,  2.09it/s][A
 16%|█▌        | 8/50 [00:03<00:20,  2.05it/s][A
 18%|█▊        | 9/50 [00:04<00:20,  2.03it/s][A
 20%|██        | 10/50 [00:04<00:19,  2.02it/s][A
 22%|██▏       | 11/50 [00:05<00:19,  2.00it/s][A
 24%|██▍       | 12/50 [00:05<00:19,  1.99it/s][A
 26%|██▌       | 13/50 [00:06<00:18,  1.98it/s][A
 28%|██▊       | 14/50 [00:06<00:18,  1.98it/s][A
 30%|███       | 15/50 [00:07<00:17,  1.98it/s][A
 32%|███▏      | 16/50 [00:07<00:17,  1.98it/s][A
 34%|███▍      | 17/50 [00:08<00:16,  1.98it/s][A
 36%|███▌      | 18/50 [00:08<00:16,  1.98it/s][A
 38%|███▊      | 19/50 [00:09<00:15,  1.98it/s][A
 40%|████      | 20/50 [00:09<00:15,  1.98it/s][A
 42%|████▏     | 21/50 [00:10<00:14,  1.98it/s][A
 44%|████▍     | 22/50 [00:10<00:14,  1.98it/s][A
 46%|████▌     | 23/50 [00:11<00:13,  1.98it/s][A
 48%|████▊     | 24/50 [00:11<00:13,  1.98it/s][A
 50%|█████     | 25/50 [00:12<00:12,  1.98it/s][A
 52%|█████▏    | 26/50 [00:12<00:12,  1.98it/s][A
 54%|█████▍    | 27/50 [00:13<00:11,  1.97it/s][A
 56%|█████▌    | 28/50 [00:13<00:11,  1.97it/s][A
 58%|█████▊    | 29/50 [00:14<00:10,  1.97it/s][A
 60%|██████    | 30/50 [00:14<00:10,  1.98it/s][A
 62%|██████▏   | 31/50 [00:15<00:09,  1.97it/s][A
 64%|██████▍   | 32/50 [00:15<00:09,  1.98it/s][A
 66%|██████▌   | 33/50 [00:16<00:08,  1.97it/s][A
 68%|██████▊   | 34/50 [00:16<00:08,  1.97it/s][A
 70%|███████   | 35/50 [00:17<00:07,  1.97it/s][A
 72%|███████▏  | 36/50 [00:17<00:07,  1.97it/s][A
 74%|███████▍  | 37/50 [00:18<00:06,  1.98it/s][A
 76%|███████▌  | 38/50 [00:18<00:06,  1.98it/s][A
 78%|███████▊  | 39/50 [00:19<00:05,  1.98it/s][A
 80%|████████  | 40/50 [00:19<00:05,  1.98it/s][A
 82%|████████▏ | 41/50 [00:20<00:04,  1.97it/s][A
 84%|████████▍ | 42/50 [00:20<00:04,  1.98it/s][A
 86%|████████▌ | 43/50 [00:21<00:03,  1.97it/s][A
 88%|████████▊ | 44/50 [00:21<00:03,  1.97it/s][A
 90%|█████████ | 45/50 [00:22<00:02,  1.98it/s][A
 92%|█████████▏| 46/50 [00:22<00:02,  1.97it/s][A
 94%|█████████▍| 47/50 [00:23<00:01,  1.98it/s][A
 96%|█████████▌| 48/50 [00:23<00:01,  1.98it/s][A
 98%|█████████▊| 49/50 [00:24<00:00,  1.98it/s][A
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A                                                   
                                               [A 88%|████████▊ | 350/400 [1:22:24<09:34, 11.48s/it]
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A
                                               [A[INFO|trainer.py:2809] 2023-08-03 22:22:55,757 >> Saving model checkpoint to ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-350
[INFO|configuration_utils.py:460] 2023-08-03 22:22:55,760 >> Configuration saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-350/config.json
[INFO|modeling_utils.py:1874] 2023-08-03 22:23:31,937 >> Model weights saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-350/pytorch_model.bin
[INFO|tokenization_utils_base.py:2227] 2023-08-03 22:23:31,941 >> tokenizer config file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-350/tokenizer_config.json
[INFO|tokenization_utils_base.py:2234] 2023-08-03 22:23:31,943 >> Special tokens file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-350/special_tokens_map.json
 88%|████████▊ | 351/400 [1:24:22<41:45, 51.12s/it] 88%|████████▊ | 352/400 [1:24:34<31:21, 39.19s/it] 88%|████████▊ | 353/400 [1:24:45<24:09, 30.84s/it] 88%|████████▊ | 354/400 [1:24:56<19:10, 25.02s/it] 89%|████████▉ | 355/400 [1:25:08<15:42, 20.94s/it] 89%|████████▉ | 356/400 [1:25:19<13:16, 18.09s/it] 89%|████████▉ | 357/400 [1:25:31<11:33, 16.12s/it] 90%|████████▉ | 358/400 [1:25:42<10:18, 14.73s/it] 90%|████████▉ | 359/400 [1:25:54<09:24, 13.76s/it] 90%|█████████ | 360/400 [1:26:05<08:43, 13.09s/it] 90%|█████████ | 361/400 [1:26:17<08:12, 12.62s/it] 90%|█████████ | 362/400 [1:26:28<07:47, 12.30s/it] 91%|█████████ | 363/400 [1:26:40<07:26, 12.07s/it] 91%|█████████ | 364/400 [1:26:51<07:08, 11.90s/it] 91%|█████████▏| 365/400 [1:27:03<06:52, 11.78s/it] 92%|█████████▏| 366/400 [1:27:14<06:37, 11.69s/it] 92%|█████████▏| 367/400 [1:27:26<06:23, 11.63s/it] 92%|█████████▏| 368/400 [1:27:37<06:10, 11.59s/it] 92%|█████████▏| 369/400 [1:27:49<05:58, 11.55s/it] 92%|█████████▎| 370/400 [1:28:00<05:46, 11.53s/it] 93%|█████████▎| 371/400 [1:28:12<05:34, 11.52s/it] 93%|█████████▎| 372/400 [1:28:23<05:22, 11.51s/it] 93%|█████████▎| 373/400 [1:28:35<05:10, 11.49s/it] 94%|█████████▎| 374/400 [1:28:46<04:58, 11.49s/it] 94%|█████████▍| 375/400 [1:28:58<04:47, 11.49s/it] 94%|█████████▍| 376/400 [1:29:09<04:35, 11.48s/it] 94%|█████████▍| 377/400 [1:29:21<04:24, 11.48s/it] 94%|█████████▍| 378/400 [1:29:32<04:12, 11.48s/it] 95%|█████████▍| 379/400 [1:29:44<04:01, 11.48s/it] 95%|█████████▌| 380/400 [1:29:55<03:49, 11.49s/it] 95%|█████████▌| 381/400 [1:30:07<03:38, 11.49s/it] 96%|█████████▌| 382/400 [1:30:18<03:26, 11.49s/it] 96%|█████████▌| 383/400 [1:30:29<03:15, 11.48s/it] 96%|█████████▌| 384/400 [1:30:41<03:03, 11.49s/it] 96%|█████████▋| 385/400 [1:30:52<02:52, 11.48s/it] 96%|█████████▋| 386/400 [1:31:04<02:40, 11.48s/it] 97%|█████████▋| 387/400 [1:31:15<02:29, 11.49s/it] 97%|█████████▋| 388/400 [1:31:27<02:17, 11.49s/it] 97%|█████████▋| 389/400 [1:31:38<02:06, 11.49s/it] 98%|█████████▊| 390/400 [1:31:50<01:54, 11.49s/it] 98%|█████████▊| 391/400 [1:32:01<01:43, 11.48s/it] 98%|█████████▊| 392/400 [1:32:13<01:31, 11.48s/it] 98%|█████████▊| 393/400 [1:32:24<01:20, 11.49s/it] 98%|█████████▊| 394/400 [1:32:36<01:08, 11.49s/it] 99%|█████████▉| 395/400 [1:32:47<00:57, 11.48s/it] 99%|█████████▉| 396/400 [1:32:59<00:45, 11.48s/it] 99%|█████████▉| 397/400 [1:33:10<00:34, 11.49s/it]100%|█████████▉| 398/400 [1:33:22<00:22, 11.49s/it]100%|█████████▉| 399/400 [1:33:33<00:11, 11.48s/it]100%|██████████| 400/400 [1:33:45<00:00, 11.47s/it][INFO|trainer.py:749] 2023-08-03 22:34:16,513 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: context, id, sentence. If context, id, sentence are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3083] 2023-08-03 22:34:16,515 >> ***** Running Evaluation *****
[INFO|trainer.py:3085] 2023-08-03 22:34:16,516 >>   Num examples = 400
[INFO|trainer.py:3088] 2023-08-03 22:34:16,516 >>   Batch size = 8
{'eval_loss': 0.8725324869155884, 'eval_accuracy': 0.6325, 'eval_runtime': 25.3581, 'eval_samples_per_second': 15.774, 'eval_steps_per_second': 1.972, 'epoch': 7.0}

  0%|          | 0/50 [00:00<?, ?it/s][A
  4%|▍         | 2/50 [00:00<00:12,  3.95it/s][A
  6%|▌         | 3/50 [00:01<00:16,  2.79it/s][A
  8%|▊         | 4/50 [00:01<00:19,  2.42it/s][A
 10%|█         | 5/50 [00:02<00:20,  2.25it/s][A
 12%|█▏        | 6/50 [00:02<00:20,  2.15it/s][A
 14%|█▍        | 7/50 [00:03<00:20,  2.09it/s][A
 16%|█▌        | 8/50 [00:03<00:20,  2.06it/s][A
 18%|█▊        | 9/50 [00:04<00:20,  2.03it/s][A
 20%|██        | 10/50 [00:04<00:19,  2.02it/s][A
 22%|██▏       | 11/50 [00:05<00:19,  2.00it/s][A
 24%|██▍       | 12/50 [00:05<00:19,  1.99it/s][A
 26%|██▌       | 13/50 [00:06<00:18,  1.98it/s][A
 28%|██▊       | 14/50 [00:06<00:18,  1.98it/s][A
 30%|███       | 15/50 [00:07<00:17,  1.98it/s][A
 32%|███▏      | 16/50 [00:07<00:17,  1.98it/s][A
 34%|███▍      | 17/50 [00:08<00:16,  1.98it/s][A
 36%|███▌      | 18/50 [00:08<00:16,  1.98it/s][A
 38%|███▊      | 19/50 [00:09<00:15,  1.98it/s][A
 40%|████      | 20/50 [00:09<00:15,  1.98it/s][A
 42%|████▏     | 21/50 [00:10<00:14,  1.99it/s][A
 44%|████▍     | 22/50 [00:10<00:14,  1.98it/s][A
 46%|████▌     | 23/50 [00:11<00:13,  1.98it/s][A
 48%|████▊     | 24/50 [00:11<00:13,  1.97it/s][A
 50%|█████     | 25/50 [00:12<00:12,  1.97it/s][A
 52%|█████▏    | 26/50 [00:12<00:12,  1.97it/s][A
 54%|█████▍    | 27/50 [00:13<00:11,  1.97it/s][A
 56%|█████▌    | 28/50 [00:13<00:11,  1.97it/s][A
 58%|█████▊    | 29/50 [00:14<00:10,  1.98it/s][A
 60%|██████    | 30/50 [00:14<00:10,  1.98it/s][A
 62%|██████▏   | 31/50 [00:15<00:09,  1.98it/s][A
 64%|██████▍   | 32/50 [00:15<00:09,  1.98it/s][A
 66%|██████▌   | 33/50 [00:16<00:08,  1.97it/s][A
 68%|██████▊   | 34/50 [00:16<00:08,  1.97it/s][A
 70%|███████   | 35/50 [00:17<00:07,  1.97it/s][A
 72%|███████▏  | 36/50 [00:17<00:07,  1.97it/s][A
 74%|███████▍  | 37/50 [00:18<00:06,  1.98it/s][A
 76%|███████▌  | 38/50 [00:18<00:06,  1.98it/s][A
 78%|███████▊  | 39/50 [00:19<00:05,  1.98it/s][A
 80%|████████  | 40/50 [00:19<00:05,  1.98it/s][A
 82%|████████▏ | 41/50 [00:20<00:04,  1.97it/s][A
 84%|████████▍ | 42/50 [00:20<00:04,  1.98it/s][A
 86%|████████▌ | 43/50 [00:21<00:03,  1.97it/s][A
 88%|████████▊ | 44/50 [00:21<00:03,  1.97it/s][A
 90%|█████████ | 45/50 [00:22<00:02,  1.98it/s][A
 92%|█████████▏| 46/50 [00:22<00:02,  1.97it/s][A
 94%|█████████▍| 47/50 [00:23<00:01,  1.98it/s][A
 96%|█████████▌| 48/50 [00:23<00:01,  1.98it/s][A
 98%|█████████▊| 49/50 [00:24<00:00,  1.98it/s][A
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A                                                   
                                               [A100%|██████████| 400/400 [1:34:10<00:00, 11.47s/it]
100%|██████████| 50/50 [00:24<00:00,  1.98it/s][A
                                               [A[INFO|trainer.py:2809] 2023-08-03 22:34:41,879 >> Saving model checkpoint to ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-400
[INFO|configuration_utils.py:460] 2023-08-03 22:34:41,882 >> Configuration saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-400/config.json
[INFO|modeling_utils.py:1874] 2023-08-03 22:35:16,717 >> Model weights saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:2227] 2023-08-03 22:35:16,722 >> tokenizer config file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2234] 2023-08-03 22:35:16,724 >> Special tokens file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:1929] 2023-08-03 22:36:31,057 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 400/400 [1:35:59<00:00, 11.47s/it]100%|██████████| 400/400 [1:35:59<00:00, 14.40s/it]
[INFO|trainer.py:2809] 2023-08-03 22:36:31,069 >> Saving model checkpoint to ../../output/task1-deberta-v2-xlarge-mnli-4
[INFO|configuration_utils.py:460] 2023-08-03 22:36:31,072 >> Configuration saved in ../../output/task1-deberta-v2-xlarge-mnli-4/config.json
[INFO|modeling_utils.py:1874] 2023-08-03 22:37:30,200 >> Model weights saved in ../../output/task1-deberta-v2-xlarge-mnli-4/pytorch_model.bin
[INFO|tokenization_utils_base.py:2227] 2023-08-03 22:37:30,395 >> tokenizer config file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2234] 2023-08-03 22:37:30,538 >> Special tokens file saved in ../../output/task1-deberta-v2-xlarge-mnli-4/special_tokens_map.json
{'eval_loss': 0.8676473498344421, 'eval_accuracy': 0.63, 'eval_runtime': 25.3524, 'eval_samples_per_second': 15.778, 'eval_steps_per_second': 1.972, 'epoch': 8.0}
{'train_runtime': 5765.1855, 'train_samples_per_second': 4.44, 'train_steps_per_second': 0.069, 'train_loss': 0.8988578796386719, 'epoch': 8.0}
***** train metrics *****
  epoch                    =        8.0
  train_loss               =     0.8989
  train_runtime            = 1:36:05.18
  train_samples            =       3200
  train_samples_per_second =       4.44
  train_steps_per_second   =      0.069
08/03/2023 22:37:33 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:749] 2023-08-03 22:37:33,830 >> The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: context, id, sentence. If context, id, sentence are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3083] 2023-08-03 22:37:33,833 >> ***** Running Evaluation *****
[INFO|trainer.py:3085] 2023-08-03 22:37:33,833 >>   Num examples = 400
[INFO|trainer.py:3088] 2023-08-03 22:37:33,834 >>   Batch size = 8
  0%|          | 0/50 [00:00<?, ?it/s]  4%|▍         | 2/50 [00:00<00:11,  4.02it/s]  6%|▌         | 3/50 [00:00<00:16,  2.85it/s]  8%|▊         | 4/50 [00:01<00:18,  2.47it/s] 10%|█         | 5/50 [00:01<00:19,  2.29it/s] 12%|█▏        | 6/50 [00:02<00:20,  2.19it/s] 14%|█▍        | 7/50 [00:02<00:20,  2.13it/s] 16%|█▌        | 8/50 [00:03<00:20,  2.09it/s] 18%|█▊        | 9/50 [00:03<00:19,  2.07it/s] 20%|██        | 10/50 [00:04<00:19,  2.05it/s] 22%|██▏       | 11/50 [00:04<00:19,  2.03it/s] 24%|██▍       | 12/50 [00:05<00:18,  2.03it/s] 26%|██▌       | 13/50 [00:05<00:18,  2.02it/s] 28%|██▊       | 14/50 [00:06<00:17,  2.01it/s] 30%|███       | 15/50 [00:06<00:17,  2.01it/s] 32%|███▏      | 16/50 [00:07<00:16,  2.01it/s] 34%|███▍      | 17/50 [00:07<00:16,  2.01it/s] 36%|███▌      | 18/50 [00:08<00:15,  2.01it/s] 38%|███▊      | 19/50 [00:08<00:15,  2.01it/s] 40%|████      | 20/50 [00:09<00:14,  2.01it/s] 42%|████▏     | 21/50 [00:09<00:14,  2.01it/s] 44%|████▍     | 22/50 [00:10<00:13,  2.01it/s] 46%|████▌     | 23/50 [00:10<00:13,  2.01it/s] 48%|████▊     | 24/50 [00:11<00:12,  2.00it/s] 50%|█████     | 25/50 [00:11<00:12,  2.00it/s] 52%|█████▏    | 26/50 [00:12<00:11,  2.00it/s] 54%|█████▍    | 27/50 [00:12<00:11,  2.00it/s] 56%|█████▌    | 28/50 [00:13<00:10,  2.00it/s] 58%|█████▊    | 29/50 [00:13<00:10,  2.00it/s] 60%|██████    | 30/50 [00:14<00:10,  2.00it/s] 62%|██████▏   | 31/50 [00:14<00:09,  2.00it/s] 64%|██████▍   | 32/50 [00:15<00:08,  2.00it/s] 66%|██████▌   | 33/50 [00:15<00:08,  2.00it/s] 68%|██████▊   | 34/50 [00:16<00:07,  2.00it/s] 70%|███████   | 35/50 [00:16<00:07,  2.00it/s] 72%|███████▏  | 36/50 [00:17<00:07,  2.00it/s] 74%|███████▍  | 37/50 [00:17<00:06,  2.00it/s] 76%|███████▌  | 38/50 [00:18<00:06,  2.00it/s] 78%|███████▊  | 39/50 [00:18<00:05,  2.00it/s] 80%|████████  | 40/50 [00:19<00:05,  2.00it/s] 82%|████████▏ | 41/50 [00:19<00:04,  1.99it/s] 84%|████████▍ | 42/50 [00:20<00:04,  2.00it/s] 86%|████████▌ | 43/50 [00:20<00:03,  1.99it/s] 88%|████████▊ | 44/50 [00:21<00:03,  1.99it/s] 90%|█████████ | 45/50 [00:21<00:02,  2.00it/s] 92%|█████████▏| 46/50 [00:22<00:02,  2.00it/s] 94%|█████████▍| 47/50 [00:22<00:01,  2.00it/s] 96%|█████████▌| 48/50 [00:23<00:01,  2.00it/s] 98%|█████████▊| 49/50 [00:23<00:00,  1.99it/s]100%|██████████| 50/50 [00:24<00:00,  2.00it/s]100%|██████████| 50/50 [00:26<00:00,  1.91it/s]
[INFO|modelcard.py:452] 2023-08-03 22:38:06,497 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.63}]}
***** eval metrics *****
  epoch                   =        8.0
  eval_accuracy           =       0.63
  eval_loss               =     0.8676
  eval_runtime            = 0:00:25.65
  eval_samples            =        400
  eval_samples_per_second =     15.589
  eval_steps_per_second   =      1.949
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: \ 0.015 MB of 0.015 MB uploaded (0.000 MB deduped)wandb: | 0.015 MB of 0.048 MB uploaded (0.000 MB deduped)wandb: / 0.034 MB of 0.048 MB uploaded (0.000 MB deduped)wandb: - 0.048 MB of 0.048 MB uploaded (0.000 MB deduped)wandb: \ 0.048 MB of 0.048 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                  eval/accuracy ▁▂▄▇▇████
wandb:                      eval/loss ██▆▃▃▂▁▁▁
wandb:                   eval/runtime ▃▂▁▁▁▁▁▁█
wandb:        eval/samples_per_second ▆▇██████▁
wandb:          eval/steps_per_second ▆▇██████▁
wandb:                    train/epoch ▁▂▃▄▅▆▇███
wandb:              train/global_step ▁▂▃▄▅▆▇███
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                  eval/accuracy 0.63
wandb:                      eval/loss 0.86765
wandb:                   eval/runtime 25.6599
wandb:        eval/samples_per_second 15.589
wandb:          eval/steps_per_second 1.949
wandb:                    train/epoch 8.0
wandb:              train/global_step 400
wandb:               train/total_flos 5.42174527881216e+16
wandb:               train/train_loss 0.89886
wandb:            train/train_runtime 5765.1855
wandb: train/train_samples_per_second 4.44
wandb:   train/train_steps_per_second 0.069
wandb: 
wandb: 🚀 View run task1-deberta-v2-xlarge-mnli-4 at: https://wandb.ai/ym_k/huggingface/runs/x4cf78yu
wandb: ️⚡ View job at https://wandb.ai/ym_k/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg3NzUxMjEw/version_details/v2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230803_210027-x4cf78yu/logs
Exception in thread ChkStopThr:
Traceback (most recent call last):
  File "/work/y53kang/.conda/envs/spoiler/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/work/y53kang/.conda/envs/spoiler/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 274, in check_stop_status
    self._loop_check_status(
  File "/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 212, in _loop_check_status
    local_handle = request()
  File "/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/wandb/sdk/interface/interface.py", line 854, in deliver_stop_status
