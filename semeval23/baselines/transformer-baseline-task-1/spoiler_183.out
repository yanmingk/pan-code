Thu Aug  3 16:23:55 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100 80G...  On   | 00000000:C1:00.0 Off |                    0 |
| N/A   33C    P0    43W / 300W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
08/03/2023 16:24:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
08/03/2023 16:24:00 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=../../outputs/task1-deberta-v2-xlarge-mnli/runs/Aug03_16-24-00_gpu-pr1-02,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=4.0,
optim=adamw_torch,
optim_args=None,
output_dir=../../outputs/task1-deberta-v2-xlarge-mnli,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=../../outputs/task1-deberta-v2-xlarge-mnli,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
08/03/2023 16:24:00 - INFO - __main__ - load a local file for train: ../../data/train.json
08/03/2023 16:24:00 - INFO - __main__ - load a local file for validation: ../../data/val.json
08/03/2023 16:24:00 - INFO - datasets.builder - Using custom data configuration default-d7a57e858efbbe1f
08/03/2023 16:24:00 - INFO - datasets.info - Loading Dataset Infos from /work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/datasets/packaged_modules/json
08/03/2023 16:24:00 - INFO - datasets.builder - Generating dataset json (/work/y53kang/.cache/huggingface/datasets/json/default-d7a57e858efbbe1f/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
Downloading and preparing dataset json/default to /work/y53kang/.cache/huggingface/datasets/json/default-d7a57e858efbbe1f/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...
Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 10965.50it/s]
08/03/2023 16:24:00 - INFO - datasets.download.download_manager - Downloading took 0.0 min
08/03/2023 16:24:00 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 51.05it/s]
08/03/2023 16:24:01 - INFO - datasets.builder - Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1744 examples [00:00, 10625.39 examples/s]Generating train split: 3200 examples [00:00, 12063.23 examples/s]                                                                  08/03/2023 16:24:01 - INFO - datasets.builder - Generating validation split
Generating validation split: 0 examples [00:00, ? examples/s]                                                             ╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/datasets/buil │
│ der.py:1894 in _prepare_split_single                                         │
│                                                                              │
│   1891 │   │   │   │   │   │   │   storage_options=self._fs.storage_options, │
│   1892 │   │   │   │   │   │   │   embed_local_files=embed_local_files,      │
│   1893 │   │   │   │   │   │   )                                             │
│ ❱ 1894 │   │   │   │   │   writer.write_table(table)                         │
│   1895 │   │   │   │   │   num_examples_progress_update += len(table)        │
│   1896 │   │   │   │   │   if time.time() > _time + config.PBAR_REFRESH_TIME │
│   1897 │   │   │   │   │   │   _time = time.time()                           │
│                                                                              │
│ /work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/datasets/arro │
│ w_writer.py:570 in write_table                                               │
│                                                                              │
│   567 │   │   if self.pa_writer is None:                                     │
│   568 │   │   │   self._build_writer(inferred_schema=pa_table.schema)        │
│   569 │   │   pa_table = pa_table.combine_chunks()                           │
│ ❱ 570 │   │   pa_table = table_cast(pa_table, self._schema)                  │
│   571 │   │   if self.embed_local_files:                                     │
│   572 │   │   │   pa_table = embed_table_storage(pa_table)                   │
│   573 │   │   self._num_bytes += pa_table.nbytes                             │
│                                                                              │
│ /work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/datasets/tabl │
│ e.py:2324 in table_cast                                                      │
│                                                                              │
│   2321 │   │   table (`pyarrow.Table`): the casted table                     │
│   2322 │   """                                                               │
│   2323 │   if table.schema != schema:                                        │
│ ❱ 2324 │   │   return cast_table_to_schema(table, schema)                    │
│   2325 │   elif table.schema.metadata != schema.metadata:                    │
│   2326 │   │   return table.replace_schema_metadata(schema.metadata)         │
│   2327 │   else:                                                             │
│                                                                              │
│ /work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/datasets/tabl │
│ e.py:2282 in cast_table_to_schema                                            │
│                                                                              │
│   2279 │                                                                     │
│   2280 │   features = Features.from_arrow_schema(schema)                     │
│   2281 │   if sorted(table.column_names) != sorted(features):                │
│ ❱ 2282 │   │   raise ValueError(f"Couldn't cast\n{table.schema}\nto\n{featur │
│   2283 │   arrays = [cast_array_to_feature(table[name], feature) for name, f │
│   2284 │   return pa.Table.from_arrays(arrays, schema=schema)                │
│   2285                                                                       │
╰──────────────────────────────────────────────────────────────────────────────╯
ValueError: Couldn't cast
postId: string
postText: list<item: string>
  child 0, item: string
postPlatform: string
targetParagraphs: list<item: string>
  child 0, item: string
targetTitle: string
targetDescription: string
targetKeywords: string
targetMedia: list<item: string>
  child 0, item: string
targetUrl: string
provenance: struct<source: string, humanSpoiler: string, spoilerPublisher: 
string>
  child 0, source: string
  child 1, humanSpoiler: string
  child 2, spoilerPublisher: string
spoiler: list<item: string>
  child 0, item: string
spoilerPositions: list<item: list<item: list<item: int64>>>
  child 0, item: list<item: list<item: int64>>
      child 0, item: list<item: int64>
          child 0, item: int64
tags: list<item: string>
  child 0, item: string
id: int64
to
{'uuid': Value(dtype='string', id=None), 'postId': Value(dtype='string', 
id=None), 'postText': Sequence(feature=Value(dtype='string', id=None), 
length=-1, id=None), 'postPlatform': Value(dtype='string', id=None), 
'targetParagraphs': Sequence(feature=Value(dtype='string', id=None), length=-1, 
id=None), 'targetTitle': Value(dtype='string', id=None), 'targetDescription': 
Value(dtype='string', id=None), 'targetKeywords': Value(dtype='string', 
id=None), 'targetMedia': Sequence(feature=Value(dtype='string', id=None), 
length=-1, id=None), 'targetUrl': Value(dtype='string', id=None), 'provenance': 
{'source': Value(dtype='string', id=None), 'humanSpoiler': Value(dtype='string',
id=None), 'spoilerPublisher': Value(dtype='string', id=None)}, 'spoiler': 
Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 
'spoilerPositions': 
Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='int64', 
id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None), 'tags':
Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}
because column names don't match

The above exception was the direct cause of the following exception:

╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /mnt/hpc/work/y53kang/pan-code/semeval23/baselines/transformer-baseline-task │
│ -1/run_classification.py:744 in <module>                                     │
│                                                                              │
│   741                                                                        │
│   742                                                                        │
│   743 if __name__ == "__main__":                                             │
│ ❱ 744 │   main()                                                             │
│   745                                                                        │
│                                                                              │
│ /mnt/hpc/work/y53kang/pan-code/semeval23/baselines/transformer-baseline-task │
│ -1/run_classification.py:378 in main                                         │
│                                                                              │
│   375 │   │   │   )                                                          │
│   376 │   │   else:                                                          │
│   377 │   │   │   # Loading a dataset from local json files                  │
│ ❱ 378 │   │   │   raw_datasets = load_dataset(                               │
│   379 │   │   │   │   "json",                                                │
│   380 │   │   │   │   data_files=data_files,                                 │
│   381 │   │   │   │   cache_dir=model_args.cache_dir,                        │
│                                                                              │
│ /work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/datasets/load │
│ .py:1797 in load_dataset                                                     │
│                                                                              │
│   1794 │   try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES          │
│   1795 │                                                                     │
│   1796 │   # Download and prepare data                                       │
│ ❱ 1797 │   builder_instance.download_and_prepare(                            │
│   1798 │   │   download_config=download_config,                              │
│   1799 │   │   download_mode=download_mode,                                  │
│   1800 │   │   verification_mode=verification_mode,                          │
│                                                                              │
│ /work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/datasets/buil │
│ der.py:909 in download_and_prepare                                           │
│                                                                              │
│    906 │   │   │   │   │   │   │   prepare_split_kwargs["max_shard_size"] =  │
│    907 │   │   │   │   │   │   if num_proc is not None:                      │
│    908 │   │   │   │   │   │   │   prepare_split_kwargs["num_proc"] = num_pr │
│ ❱  909 │   │   │   │   │   │   self._download_and_prepare(                   │
│    910 │   │   │   │   │   │   │   dl_manager=dl_manager,                    │
│    911 │   │   │   │   │   │   │   verification_mode=verification_mode,      │
│    912 │   │   │   │   │   │   │   **prepare_split_kwargs,                   │
│                                                                              │
│ /work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/datasets/buil │
│ der.py:1004 in _download_and_prepare                                         │
│                                                                              │
│   1001 │   │   │                                                             │
│   1002 │   │   │   try:                                                      │
│   1003 │   │   │   │   # Prepare split will record examples associated to th │
│ ❱ 1004 │   │   │   │   self._prepare_split(split_generator, **prepare_split_ │
│   1005 │   │   │   except OSError as e:                                      │
│   1006 │   │   │   │   raise OSError(                                        │
│   1007 │   │   │   │   │   "Cannot find data file. "                         │
│                                                                              │
│ /work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/datasets/buil │
│ der.py:1767 in _prepare_split                                                │
│                                                                              │
│   1764 │   │   │   gen_kwargs = split_generator.gen_kwargs                   │
│   1765 │   │   │   job_id = 0                                                │
│   1766 │   │   │   with pbar:                                                │
│ ❱ 1767 │   │   │   │   for job_id, done, content in self._prepare_split_sing │
│   1768 │   │   │   │   │   gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_ │
│   1769 │   │   │   │   ):                                                    │
│   1770 │   │   │   │   │   if done:                                          │
│                                                                              │
│ /work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/datasets/buil │
│ der.py:1912 in _prepare_split_single                                         │
│                                                                              │
│   1909 │   │   │   # Ignore the writer's error for no examples written to th │
│   1910 │   │   │   if isinstance(e, SchemaInferenceError) and e.__context__  │
│   1911 │   │   │   │   e = e.__context__                                     │
│ ❱ 1912 │   │   │   raise DatasetGenerationError("An error occurred while gen │
│   1913 │   │                                                                 │
│   1914 │   │   yield job_id, True, (total_num_examples, total_num_bytes, wri │
│   1915                                                                       │
╰──────────────────────────────────────────────────────────────────────────────╯
DatasetGenerationError: An error occurred while generating the dataset
