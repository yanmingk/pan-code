Thu Aug  3 14:26:49 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100 80G...  On   | 00000000:41:00.0 Off |                    0 |
| N/A   37C    P0    44W / 300W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
08/03/2023 14:26:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
08/03/2023 14:26:58 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=../../output/bert_large/runs/Aug03_14-26-58_gpu-pr1-02,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=8.0,
optim=adamw_torch,
optim_args=None,
output_dir=../../output/bert_large,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=12,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=../../output/bert_large,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
08/03/2023 14:26:58 - INFO - __main__ - Checkpoint detected, resuming training at ../../output/bert_large/checkpoint-4000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.
[INFO|configuration_utils.py:715] 2023-08-03 14:26:58,693 >> loading configuration file config.json from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json
[INFO|configuration_utils.py:771] 2023-08-03 14:26:58,700 >> Model config BertConfig {
  "_name_or_path": "bert-large-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.32.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|configuration_utils.py:715] 2023-08-03 14:26:58,759 >> loading configuration file config.json from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json
[INFO|configuration_utils.py:771] 2023-08-03 14:26:58,760 >> Model config BertConfig {
  "_name_or_path": "bert-large-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.32.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1844] 2023-08-03 14:26:58,764 >> loading file vocab.txt from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/vocab.txt
[INFO|tokenization_utils_base.py:1844] 2023-08-03 14:26:58,764 >> loading file tokenizer.json from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/tokenizer.json
[INFO|tokenization_utils_base.py:1844] 2023-08-03 14:26:58,764 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1844] 2023-08-03 14:26:58,764 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1844] 2023-08-03 14:26:58,764 >> loading file tokenizer_config.json from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/tokenizer_config.json
[INFO|configuration_utils.py:715] 2023-08-03 14:26:58,764 >> loading configuration file config.json from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json
[INFO|configuration_utils.py:771] 2023-08-03 14:26:58,765 >> Model config BertConfig {
  "_name_or_path": "bert-large-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.32.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:2638] 2023-08-03 14:26:58,857 >> loading weights file model.safetensors from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/model.safetensors
[INFO|modeling_utils.py:3360] 2023-08-03 14:27:00,911 >> Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3372] 2023-08-03 14:27:00,912 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'id': 10, 'title': 'This may be the most brutal number in the CBO report', 'question': 'Analysis: This may be the most brutal number in the CBO report', 'context': 'This may be the most brutal number in the CBO report - Plenty has been made of the big Congressional Budget Office finding that 24 million people could lose their insurance under Republicans\' Obamacare replacement over the next decade. That\'s higher than expected and poses a clear and massive hurdle for Republicans as they attempt to convince dozens of skeptical members. But there\'s another number that paints a particularly dire picture for the GOP\'s alternative — especially in light of President Trump\'s populist rhetoric. According to the CBO, 64-year olds making $26,500 per year would see their premiums increase by an estimated 750 percent by 2026. While they are on track to pay $1,700 under the current law, the CBO projects the American Health Care Act would force them to pay $14,600. Even if you grant that inflation will allow them to make slightly more money by 2026, that\'s still about half of their income going to health care. Here\'s how that looks as a percentage of income (h/t Philip Bump): As skeptics of the law noted, that suggests the reason premiums as a whole will eventually decline is because older, poorer people simply won\'t be able to afford it. The legislation reduces premiums substantially for younger people but increases them substantially for older people — and especially poorer, older people — according to the CBO. The CBO\'s estimate of the premium increases for older, poorer Americans is actually worse than a previous one from AARP. Here\'s what AARP, which has announced its opposition to the bill, said last week: ... Our estimates find that, taken together, premiums for older adults could increase by as much as $3,600 for a 55-year old earning $25,000 a year, $7,000 for a 64-year old earning $25,000 a year and up to $8,400 for a 64-year old earning $15,000 a year. If you\'re a Republican looking at these numbers, you have to be concerned — just from a self-preservation standpoint. Republican leaders have made great pains to argue that the CBO\'s estimate of the millions who will lose insurance is faulty, or even that it\'s a necessary side effect of returning to a more free-market approach to health care. But the GOP\'s counter-argument is predicated on the idea that its alternative would at least allow people access to coverage. Paying such a substantial portion of one\'s income on health insurance doesn\'t meet that goal — if, in fact, the CBO\'s estimate is anywhere close to accurate. It also affects a group of voters who are integral to the Trump Coalition. Less-formally educated and lower-income white Americans were the backbone of the electoral shift that allowed President Trump to be elected, and he has promised them the world: Coverage that is even more affordable than the Affordable Care Act and "insurance for everybody." On top of all that, older people are much more likely to vote than younger people, especially in a midterm election like 2018. So basically, Trump\'s win was built on older, poorer people, for whom this law appears to drive up premiums and drive down the insurance rate, while benefiting the younger and wealthier. That\'s not something that will assure skeptical Republicans at all — even if they can get past that 24 million number.', 'answers': {'answer_start': [638], 'text': ['750 percent']}}
Running tokenizer on train dataset:   0%|          | 0/3200 [00:00<?, ? examples/s]Running tokenizer on train dataset:  31%|███▏      | 1000/3200 [00:01<00:03, 559.42 examples/s]Running tokenizer on train dataset:  62%|██████▎   | 2000/3200 [00:03<00:02, 535.86 examples/s]Running tokenizer on train dataset:  94%|█████████▍| 3000/3200 [00:05<00:00, 560.56 examples/s]Running tokenizer on train dataset: 100%|██████████| 3200/3200 [00:05<00:00, 568.10 examples/s]                                                                                               Running tokenizer on validation dataset:   0%|          | 0/400 [00:00<?, ? examples/s]Running tokenizer on validation dataset: 100%|██████████| 400/400 [00:00<00:00, 475.42 examples/s]                                                                                                  [INFO|trainer.py:2015] 2023-08-03 14:27:09,145 >> Loading model from ../../output/bert_large/checkpoint-4000.
[INFO|trainer.py:1681] 2023-08-03 14:27:11,412 >> ***** Running training *****
[INFO|trainer.py:1682] 2023-08-03 14:27:11,413 >>   Num examples = 6,624
[INFO|trainer.py:1683] 2023-08-03 14:27:11,413 >>   Num Epochs = 8
[INFO|trainer.py:1684] 2023-08-03 14:27:11,413 >>   Instantaneous batch size per device = 12
[INFO|trainer.py:1687] 2023-08-03 14:27:11,413 >>   Total train batch size (w. parallel, distributed & accumulation) = 12
[INFO|trainer.py:1688] 2023-08-03 14:27:11,413 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1689] 2023-08-03 14:27:11,413 >>   Total optimization steps = 4,416
[INFO|trainer.py:1690] 2023-08-03 14:27:11,414 >>   Number of trainable parameters = 334,094,338
[INFO|trainer.py:1710] 2023-08-03 14:27:11,415 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:1711] 2023-08-03 14:27:11,415 >>   Continuing training from epoch 7
[INFO|trainer.py:1712] 2023-08-03 14:27:11,415 >>   Continuing training from global step 4000
[INFO|trainer.py:1714] 2023-08-03 14:27:11,415 >>   Will skip the first 7 epochs then the first 136 batches in the first epoch.
[INFO|integrations.py:716] 2023-08-03 14:27:11,419 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: ym_k. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.7
wandb: Run data is saved locally in /mnt/hpc/work/y53kang/pan-code/semeval23/baselines/transformer-baseline-task-2/wandb/run-20230803_142712-z1jxk87w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-flower-16
wandb: ⭐️ View project at https://wandb.ai/ym_k/huggingface
wandb: 🚀 View run at https://wandb.ai/ym_k/huggingface/runs/z1jxk87w
  0%|          | 0/4416 [00:00<?, ?it/s] 91%|█████████ | 4001/4416 [00:01<00:00, 2605.12it/s] 91%|█████████ | 4015/4416 [00:14<00:00, 2605.12it/s] 91%|█████████ | 4016/4416 [00:14<00:02, 198.08it/s]  91%|█████████ | 4017/4416 [00:15<00:02, 182.10it/s] 91%|█████████▏| 4037/4416 [00:34<00:02, 182.10it/s] 91%|█████████▏| 4038/4416 [00:34<00:07, 53.60it/s]  91%|█████████▏| 4039/4416 [00:35<00:07, 51.17it/s] 92%|█████████▏| 4060/4416 [00:54<00:06, 51.17it/s] 92%|█████████▏| 4061/4416 [00:54<00:16, 21.49it/s] 92%|█████████▏| 4062/4416 [00:55<00:17, 20.74it/s] 92%|█████████▏| 4082/4416 [01:14<00:16, 20.74it/s] 92%|█████████▏| 4083/4416 [01:14<00:32, 10.38it/s] 92%|█████████▏| 4084/4416 [01:15<00:33, 10.06it/s] 93%|█████████▎| 4105/4416 [01:34<00:30, 10.06it/s] 93%|█████████▎| 4106/4416 [01:34<00:57,  5.39it/s] 93%|█████████▎| 4107/4416 [01:35<00:58,  5.25it/s] 93%|█████████▎| 4122/4416 [01:49<01:22,  3.55it/s] 93%|█████████▎| 4123/4416 [01:50<01:24,  3.45it/s] 94%|█████████▎| 4134/4416 [01:59<01:49,  2.58it/s] 94%|█████████▍| 4141/4416 [02:06<02:04,  2.21it/s] 94%|█████████▍| 4146/4416 [02:10<02:15,  1.99it/s] 94%|█████████▍| 4150/4416 [02:14<02:26,  1.82it/s] 94%|█████████▍| 4153/4416 [02:16<02:34,  1.70it/s] 94%|█████████▍| 4155/4416 [02:18<02:41,  1.62it/s] 94%|█████████▍| 4157/4416 [02:20<02:48,  1.54it/s] 94%|█████████▍| 4158/4416 [02:21<02:52,  1.49it/s] 94%|█████████▍| 4159/4416 [02:22<02:58,  1.44it/s] 94%|█████████▍| 4160/4416 [02:23<03:04,  1.39it/s] 94%|█████████▍| 4161/4416 [02:24<03:10,  1.34it/s] 94%|█████████▍| 4162/4416 [02:25<03:16,  1.29it/s] 94%|█████████▍| 4163/4416 [02:25<03:22,  1.25it/s] 94%|█████████▍| 4164/4416 [02:26<03:26,  1.22it/s] 94%|█████████▍| 4165/4416 [02:27<03:31,  1.19it/s] 94%|█████████▍| 4166/4416 [02:28<03:34,  1.17it/s] 94%|█████████▍| 4167/4416 [02:29<03:36,  1.15it/s] 94%|█████████▍| 4168/4416 [02:30<03:37,  1.14it/s] 94%|█████████▍| 4169/4416 [02:31<03:37,  1.13it/s] 94%|█████████▍| 4170/4416 [02:32<03:38,  1.13it/s] 94%|█████████▍| 4171/4416 [02:33<03:38,  1.12it/s] 94%|█████████▍| 4172/4416 [02:34<03:37,  1.12it/s] 94%|█████████▍| 4173/4416 [02:34<03:37,  1.12it/s] 95%|█████████▍| 4174/4416 [02:35<03:36,  1.12it/s] 95%|█████████▍| 4175/4416 [02:36<03:35,  1.12it/s] 95%|█████████▍| 4176/4416 [02:37<03:35,  1.12it/s] 95%|█████████▍| 4177/4416 [02:38<03:34,  1.12it/s] 95%|█████████▍| 4178/4416 [02:39<03:33,  1.12it/s] 95%|█████████▍| 4179/4416 [02:40<03:32,  1.11it/s] 95%|█████████▍| 4180/4416 [02:41<03:31,  1.11it/s] 95%|█████████▍| 4181/4416 [02:42<03:30,  1.11it/s] 95%|█████████▍| 4182/4416 [02:43<03:30,  1.11it/s] 95%|█████████▍| 4183/4416 [02:43<03:29,  1.11it/s] 95%|█████████▍| 4184/4416 [02:44<03:28,  1.11it/s] 95%|█████████▍| 4185/4416 [02:45<03:27,  1.11it/s] 95%|█████████▍| 4186/4416 [02:46<03:26,  1.11it/s] 95%|█████████▍| 4187/4416 [02:47<03:25,  1.11it/s] 95%|█████████▍| 4188/4416 [02:48<03:24,  1.12it/s] 95%|█████████▍| 4189/4416 [02:49<03:23,  1.11it/s] 95%|█████████▍| 4190/4416 [02:50<03:22,  1.11it/s] 95%|█████████▍| 4191/4416 [02:51<03:21,  1.11it/s] 95%|█████████▍| 4192/4416 [02:51<03:21,  1.11it/s] 95%|█████████▍| 4193/4416 [02:52<03:20,  1.11it/s] 95%|█████████▍| 4194/4416 [02:53<03:19,  1.11it/s] 95%|█████████▍| 4195/4416 [02:54<03:18,  1.11it/s] 95%|█████████▌| 4196/4416 [02:55<03:17,  1.11it/s] 95%|█████████▌| 4197/4416 [02:56<03:16,  1.12it/s] 95%|█████████▌| 4198/4416 [02:57<03:15,  1.11it/s] 95%|█████████▌| 4199/4416 [02:58<03:14,  1.11it/s] 95%|█████████▌| 4200/4416 [02:59<03:13,  1.11it/s] 95%|█████████▌| 4201/4416 [03:00<03:12,  1.12it/s] 95%|█████████▌| 4202/4416 [03:00<03:11,  1.12it/s] 95%|█████████▌| 4203/4416 [03:01<03:11,  1.12it/s] 95%|█████████▌| 4204/4416 [03:02<03:10,  1.11it/s] 95%|█████████▌| 4205/4416 [03:03<03:09,  1.12it/s] 95%|█████████▌| 4206/4416 [03:04<03:08,  1.11it/s] 95%|█████████▌| 4207/4416 [03:05<03:07,  1.12it/s] 95%|█████████▌| 4208/4416 [03:06<03:06,  1.12it/s] 95%|█████████▌| 4209/4416 [03:07<03:05,  1.12it/s] 95%|█████████▌| 4210/4416 [03:08<03:04,  1.12it/s] 95%|█████████▌| 4211/4416 [03:09<03:03,  1.12it/s] 95%|█████████▌| 4212/4416 [03:09<03:02,  1.12it/s] 95%|█████████▌| 4213/4416 [03:10<03:02,  1.12it/s] 95%|█████████▌| 4214/4416 [03:11<03:01,  1.12it/s] 95%|█████████▌| 4215/4416 [03:12<03:00,  1.11it/s] 95%|█████████▌| 4216/4416 [03:13<02:59,  1.12it/s] 95%|█████████▌| 4217/4416 [03:14<02:58,  1.12it/s] 96%|█████████▌| 4218/4416 [03:15<02:57,  1.12it/s] 96%|█████████▌| 4219/4416 [03:16<02:56,  1.12it/s] 96%|█████████▌| 4220/4416 [03:17<02:55,  1.12it/s] 96%|█████████▌| 4221/4416 [03:17<02:54,  1.12it/s] 96%|█████████▌| 4222/4416 [03:18<02:53,  1.11it/s] 96%|█████████▌| 4223/4416 [03:19<02:52,  1.12it/s] 96%|█████████▌| 4224/4416 [03:20<02:52,  1.11it/s] 96%|█████████▌| 4225/4416 [03:21<02:51,  1.11it/s] 96%|█████████▌| 4226/4416 [03:22<02:50,  1.11it/s] 96%|█████████▌| 4227/4416 [03:23<02:49,  1.11it/s] 96%|█████████▌| 4228/4416 [03:24<02:48,  1.12it/s] 96%|█████████▌| 4229/4416 [03:25<02:47,  1.12it/s] 96%|█████████▌| 4230/4416 [03:26<02:46,  1.12it/s] 96%|█████████▌| 4231/4416 [03:26<02:45,  1.12it/s] 96%|█████████▌| 4232/4416 [03:27<02:44,  1.12it/s] 96%|█████████▌| 4233/4416 [03:28<02:44,  1.11it/s] 96%|█████████▌| 4234/4416 [03:29<02:43,  1.11it/s] 96%|█████████▌| 4235/4416 [03:30<02:42,  1.11it/s] 96%|█████████▌| 4236/4416 [03:31<02:41,  1.11it/s] 96%|█████████▌| 4237/4416 [03:32<02:40,  1.11it/s] 96%|█████████▌| 4238/4416 [03:33<02:39,  1.11it/s] 96%|█████████▌| 4239/4416 [03:34<02:38,  1.11it/s] 96%|█████████▌| 4240/4416 [03:35<02:37,  1.11it/s] 96%|█████████▌| 4241/4416 [03:35<02:36,  1.11it/s] 96%|█████████▌| 4242/4416 [03:36<02:36,  1.11it/s] 96%|█████████▌| 4243/4416 [03:37<02:35,  1.12it/s] 96%|█████████▌| 4244/4416 [03:38<02:34,  1.11it/s] 96%|█████████▌| 4245/4416 [03:39<02:33,  1.11it/s] 96%|█████████▌| 4246/4416 [03:40<02:32,  1.11it/s] 96%|█████████▌| 4247/4416 [03:41<02:31,  1.11it/s] 96%|█████████▌| 4248/4416 [03:42<02:30,  1.11it/s] 96%|█████████▌| 4249/4416 [03:43<02:29,  1.11it/s] 96%|█████████▌| 4250/4416 [03:43<02:28,  1.12it/s] 96%|█████████▋| 4251/4416 [03:44<02:27,  1.11it/s] 96%|█████████▋| 4252/4416 [03:45<02:27,  1.11it/s] 96%|█████████▋| 4253/4416 [03:46<02:26,  1.11it/s] 96%|█████████▋| 4254/4416 [03:47<02:25,  1.11it/s] 96%|█████████▋| 4255/4416 [03:48<02:24,  1.11it/s] 96%|█████████▋| 4256/4416 [03:49<02:23,  1.11it/s] 96%|█████████▋| 4257/4416 [03:50<02:22,  1.11it/s] 96%|█████████▋| 4258/4416 [03:51<02:21,  1.11it/s] 96%|█████████▋| 4259/4416 [03:52<02:20,  1.11it/s] 96%|█████████▋| 4260/4416 [03:52<02:20,  1.11it/s] 96%|█████████▋| 4261/4416 [03:53<02:19,  1.11it/s] 97%|█████████▋| 4262/4416 [03:54<02:18,  1.11it/s] 97%|█████████▋| 4263/4416 [03:55<02:17,  1.11it/s] 97%|█████████▋| 4264/4416 [03:56<02:16,  1.11it/s] 97%|█████████▋| 4265/4416 [03:57<02:15,  1.11it/s] 97%|█████████▋| 4266/4416 [03:58<02:14,  1.11it/s] 97%|█████████▋| 4267/4416 [03:59<02:13,  1.11it/s] 97%|█████████▋| 4268/4416 [04:00<02:12,  1.11it/s] 97%|█████████▋| 4269/4416 [04:01<02:11,  1.11it/s] 97%|█████████▋| 4270/4416 [04:01<02:10,  1.12it/s] 97%|█████████▋| 4271/4416 [04:02<02:10,  1.11it/s] 97%|█████████▋| 4272/4416 [04:03<02:09,  1.12it/s] 97%|█████████▋| 4273/4416 [04:04<02:08,  1.12it/s] 97%|█████████▋| 4274/4416 [04:05<02:07,  1.12it/s] 97%|█████████▋| 4275/4416 [04:06<02:06,  1.12it/s] 97%|█████████▋| 4276/4416 [04:07<02:05,  1.11it/s] 97%|█████████▋| 4277/4416 [04:08<02:04,  1.12it/s] 97%|█████████▋| 4278/4416 [04:09<02:03,  1.12it/s] 97%|█████████▋| 4279/4416 [04:10<02:02,  1.12it/s] 97%|█████████▋| 4280/4416 [04:10<02:02,  1.11it/s] 97%|█████████▋| 4281/4416 [04:11<02:01,  1.11it/s] 97%|█████████▋| 4282/4416 [04:12<02:00,  1.11it/s] 97%|█████████▋| 4283/4416 [04:13<01:59,  1.11it/s] 97%|█████████▋| 4284/4416 [04:14<01:58,  1.12it/s] 97%|█████████▋| 4285/4416 [04:15<01:57,  1.11it/s] 97%|█████████▋| 4286/4416 [04:16<01:56,  1.11it/s] 97%|█████████▋| 4287/4416 [04:17<01:55,  1.11it/s] 97%|█████████▋| 4288/4416 [04:18<01:54,  1.11it/s] 97%|█████████▋| 4289/4416 [04:18<01:53,  1.11it/s] 97%|█████████▋| 4290/4416 [04:19<01:53,  1.11it/s] 97%|█████████▋| 4291/4416 [04:20<01:52,  1.11it/s] 97%|█████████▋| 4292/4416 [04:21<01:51,  1.11it/s] 97%|█████████▋| 4293/4416 [04:22<01:50,  1.11it/s] 97%|█████████▋| 4294/4416 [04:23<01:49,  1.11it/s] 97%|█████████▋| 4295/4416 [04:24<01:48,  1.12it/s] 97%|█████████▋| 4296/4416 [04:25<01:47,  1.11it/s] 97%|█████████▋| 4297/4416 [04:26<01:46,  1.12it/s] 97%|█████████▋| 4298/4416 [04:27<01:45,  1.11it/s] 97%|█████████▋| 4299/4416 [04:27<01:44,  1.11it/s] 97%|█████████▋| 4300/4416 [04:28<01:44,  1.11it/s] 97%|█████████▋| 4301/4416 [04:29<01:43,  1.12it/s] 97%|█████████▋| 4302/4416 [04:30<01:42,  1.11it/s] 97%|█████████▋| 4303/4416 [04:31<01:41,  1.11it/s] 97%|█████████▋| 4304/4416 [04:32<01:40,  1.11it/s] 97%|█████████▋| 4305/4416 [04:33<01:39,  1.12it/s] 98%|█████████▊| 4306/4416 [04:34<01:38,  1.11it/s] 98%|█████████▊| 4307/4416 [04:35<01:37,  1.11it/s] 98%|█████████▊| 4308/4416 [04:36<01:36,  1.11it/s] 98%|█████████▊| 4309/4416 [04:36<01:36,  1.11it/s] 98%|█████████▊| 4310/4416 [04:37<01:35,  1.11it/s] 98%|█████████▊| 4311/4416 [04:38<01:34,  1.12it/s] 98%|█████████▊| 4312/4416 [04:39<01:33,  1.12it/s] 98%|█████████▊| 4313/4416 [04:40<01:32,  1.12it/s] 98%|█████████▊| 4314/4416 [04:41<01:31,  1.11it/s] 98%|█████████▊| 4315/4416 [04:42<01:30,  1.11it/s] 98%|█████████▊| 4316/4416 [04:43<01:29,  1.11it/s] 98%|█████████▊| 4317/4416 [04:44<01:28,  1.11it/s] 98%|█████████▊| 4318/4416 [04:45<01:27,  1.11it/s] 98%|█████████▊| 4319/4416 [04:45<01:27,  1.11it/s] 98%|█████████▊| 4320/4416 [04:46<01:26,  1.11it/s] 98%|█████████▊| 4321/4416 [04:47<01:25,  1.11it/s] 98%|█████████▊| 4322/4416 [04:48<01:24,  1.11it/s] 98%|█████████▊| 4323/4416 [04:49<01:23,  1.12it/s] 98%|█████████▊| 4324/4416 [04:50<01:22,  1.11it/s] 98%|█████████▊| 4325/4416 [04:51<01:21,  1.11it/s] 98%|█████████▊| 4326/4416 [04:52<01:20,  1.11it/s] 98%|█████████▊| 4327/4416 [04:53<01:19,  1.12it/s] 98%|█████████▊| 4328/4416 [04:53<01:18,  1.11it/s] 98%|█████████▊| 4329/4416 [04:54<01:18,  1.11it/s] 98%|█████████▊| 4330/4416 [04:55<01:17,  1.11it/s] 98%|█████████▊| 4331/4416 [04:56<01:16,  1.11it/s] 98%|█████████▊| 4332/4416 [04:57<01:15,  1.11it/s] 98%|█████████▊| 4333/4416 [04:58<01:14,  1.11it/s] 98%|█████████▊| 4334/4416 [04:59<01:13,  1.11it/s] 98%|█████████▊| 4335/4416 [05:00<01:12,  1.11it/s] 98%|█████████▊| 4336/4416 [05:01<01:11,  1.11it/s] 98%|█████████▊| 4337/4416 [05:02<01:11,  1.11it/s] 98%|█████████▊| 4338/4416 [05:02<01:10,  1.11it/s] 98%|█████████▊| 4339/4416 [05:03<01:09,  1.11it/s] 98%|█████████▊| 4340/4416 [05:04<01:08,  1.11it/s] 98%|█████████▊| 4341/4416 [05:05<01:07,  1.11it/s] 98%|█████████▊| 4342/4416 [05:06<01:06,  1.11it/s] 98%|█████████▊| 4343/4416 [05:07<01:05,  1.11it/s] 98%|█████████▊| 4344/4416 [05:08<01:04,  1.11it/s] 98%|█████████▊| 4345/4416 [05:09<01:03,  1.11it/s] 98%|█████████▊| 4346/4416 [05:10<01:02,  1.11it/s] 98%|█████████▊| 4347/4416 [05:11<01:01,  1.11it/s] 98%|█████████▊| 4348/4416 [05:11<01:01,  1.11it/s] 98%|█████████▊| 4349/4416 [05:12<01:00,  1.11it/s] 99%|█████████▊| 4350/4416 [05:13<00:59,  1.11it/s] 99%|█████████▊| 4351/4416 [05:14<00:58,  1.11it/s] 99%|█████████▊| 4352/4416 [05:15<00:57,  1.11it/s] 99%|█████████▊| 4353/4416 [05:16<00:56,  1.11it/s] 99%|█████████▊| 4354/4416 [05:17<00:55,  1.11it/s] 99%|█████████▊| 4355/4416 [05:18<00:54,  1.11it/s] 99%|█████████▊| 4356/4416 [05:19<00:53,  1.11it/s] 99%|█████████▊| 4357/4416 [05:20<00:52,  1.11it/s] 99%|█████████▊| 4358/4416 [05:20<00:52,  1.11it/s] 99%|█████████▊| 4359/4416 [05:21<00:51,  1.12it/s] 99%|█████████▊| 4360/4416 [05:22<00:50,  1.11it/s] 99%|█████████▉| 4361/4416 [05:23<00:49,  1.11it/s] 99%|█████████▉| 4362/4416 [05:24<00:48,  1.11it/s] 99%|█████████▉| 4363/4416 [05:25<00:47,  1.11it/s] 99%|█████████▉| 4364/4416 [05:26<00:46,  1.11it/s] 99%|█████████▉| 4365/4416 [05:27<00:45,  1.11it/s] 99%|█████████▉| 4366/4416 [05:28<00:44,  1.11it/s] 99%|█████████▉| 4367/4416 [05:28<00:43,  1.12it/s] 99%|█████████▉| 4368/4416 [05:29<00:43,  1.12it/s] 99%|█████████▉| 4369/4416 [05:30<00:42,  1.11it/s] 99%|█████████▉| 4370/4416 [05:31<00:41,  1.11it/s] 99%|█████████▉| 4371/4416 [05:32<00:40,  1.11it/s] 99%|█████████▉| 4372/4416 [05:33<00:39,  1.11it/s] 99%|█████████▉| 4373/4416 [05:34<00:38,  1.11it/s] 99%|█████████▉| 4374/4416 [05:35<00:37,  1.11it/s] 99%|█████████▉| 4375/4416 [05:36<00:36,  1.11it/s] 99%|█████████▉| 4376/4416 [05:37<00:35,  1.11it/s] 99%|█████████▉| 4377/4416 [05:37<00:34,  1.11it/s] 99%|█████████▉| 4378/4416 [05:38<00:34,  1.11it/s] 99%|█████████▉| 4379/4416 [05:39<00:33,  1.11it/s] 99%|█████████▉| 4380/4416 [05:40<00:32,  1.11it/s] 99%|█████████▉| 4381/4416 [05:41<00:31,  1.11it/s] 99%|█████████▉| 4382/4416 [05:42<00:30,  1.11it/s] 99%|█████████▉| 4383/4416 [05:43<00:29,  1.11it/s] 99%|█████████▉| 4384/4416 [05:44<00:28,  1.11it/s] 99%|█████████▉| 4385/4416 [05:45<00:27,  1.11it/s] 99%|█████████▉| 4386/4416 [05:46<00:26,  1.11it/s] 99%|█████████▉| 4387/4416 [05:46<00:26,  1.11it/s] 99%|█████████▉| 4388/4416 [05:47<00:25,  1.11it/s] 99%|█████████▉| 4389/4416 [05:48<00:24,  1.11it/s] 99%|█████████▉| 4390/4416 [05:49<00:23,  1.11it/s] 99%|█████████▉| 4391/4416 [05:50<00:22,  1.11it/s] 99%|█████████▉| 4392/4416 [05:51<00:21,  1.11it/s] 99%|█████████▉| 4393/4416 [05:52<00:20,  1.11it/s]100%|█████████▉| 4394/4416 [05:53<00:19,  1.11it/s]100%|█████████▉| 4395/4416 [05:54<00:18,  1.11it/s]100%|█████████▉| 4396/4416 [05:55<00:17,  1.11it/s]100%|█████████▉| 4397/4416 [05:55<00:17,  1.11it/s]100%|█████████▉| 4398/4416 [05:56<00:16,  1.11it/s]100%|█████████▉| 4399/4416 [05:57<00:15,  1.11it/s]100%|█████████▉| 4400/4416 [05:58<00:14,  1.11it/s]100%|█████████▉| 4401/4416 [05:59<00:13,  1.11it/s]100%|█████████▉| 4402/4416 [06:00<00:12,  1.11it/s]100%|█████████▉| 4403/4416 [06:01<00:11,  1.11it/s]100%|█████████▉| 4404/4416 [06:02<00:10,  1.11it/s]100%|█████████▉| 4405/4416 [06:03<00:09,  1.11it/s]100%|█████████▉| 4406/4416 [06:03<00:08,  1.11it/s]100%|█████████▉| 4407/4416 [06:04<00:08,  1.11it/s]100%|█████████▉| 4408/4416 [06:05<00:07,  1.11it/s]100%|█████████▉| 4409/4416 [06:06<00:06,  1.11it/s]100%|█████████▉| 4410/4416 [06:07<00:05,  1.11it/s]100%|█████████▉| 4411/4416 [06:08<00:04,  1.11it/s]100%|█████████▉| 4412/4416 [06:09<00:03,  1.11it/s]100%|█████████▉| 4413/4416 [06:10<00:02,  1.11it/s]100%|█████████▉| 4414/4416 [06:11<00:01,  1.11it/s]100%|█████████▉| 4415/4416 [06:12<00:00,  1.11it/s]100%|██████████| 4416/4416 [06:12<00:00,  1.12it/s][INFO|trainer.py:1929] 2023-08-03 14:33:29,961 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 4416/4416 [06:12<00:00,  1.12it/s]100%|██████████| 4416/4416 [06:12<00:00, 11.84it/s]
[INFO|trainer.py:2809] 2023-08-03 14:33:29,990 >> Saving model checkpoint to ../../output/bert_large
[INFO|configuration_utils.py:460] 2023-08-03 14:33:29,996 >> Configuration saved in ../../output/bert_large/config.json
[INFO|modeling_utils.py:1874] 2023-08-03 14:33:46,481 >> Model weights saved in ../../output/bert_large/pytorch_model.bin
[INFO|tokenization_utils_base.py:2227] 2023-08-03 14:33:46,487 >> tokenizer config file saved in ../../output/bert_large/tokenizer_config.json
[INFO|tokenization_utils_base.py:2234] 2023-08-03 14:33:46,492 >> Special tokens file saved in ../../output/bert_large/special_tokens_map.json
{'train_runtime': 378.5606, 'train_samples_per_second': 139.983, 'train_steps_per_second': 11.665, 'train_loss': 0.003155372496964275, 'epoch': 8.0}
***** train metrics *****
  epoch                    =        8.0
  train_loss               =     0.0032
  train_runtime            = 0:06:18.56
  train_samples            =       6624
  train_samples_per_second =    139.983
  train_steps_per_second   =     11.665
08/03/2023 14:33:46 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:749] 2023-08-03 14:33:46,568 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.
[INFO|trainer.py:3083] 2023-08-03 14:33:46,570 >> ***** Running Evaluation *****
[INFO|trainer.py:3085] 2023-08-03 14:33:46,570 >>   Num examples = 864
[INFO|trainer.py:3088] 2023-08-03 14:33:46,570 >>   Batch size = 8
  0%|          | 0/108 [00:00<?, ?it/s]  2%|▏         | 2/108 [00:00<00:10, 10.01it/s]  4%|▎         | 4/108 [00:00<00:16,  6.25it/s]  5%|▍         | 5/108 [00:00<00:17,  5.80it/s]  6%|▌         | 6/108 [00:01<00:18,  5.53it/s]  6%|▋         | 7/108 [00:01<00:18,  5.37it/s]  7%|▋         | 8/108 [00:01<00:19,  5.25it/s]  8%|▊         | 9/108 [00:01<00:19,  5.17it/s]  9%|▉         | 10/108 [00:01<00:19,  5.12it/s] 10%|█         | 11/108 [00:02<00:19,  5.08it/s] 11%|█         | 12/108 [00:02<00:19,  5.05it/s] 12%|█▏        | 13/108 [00:02<00:18,  5.02it/s] 13%|█▎        | 14/108 [00:02<00:18,  4.99it/s] 14%|█▍        | 15/108 [00:02<00:18,  4.98it/s] 15%|█▍        | 16/108 [00:03<00:18,  4.98it/s] 16%|█▌        | 17/108 [00:03<00:18,  4.98it/s] 17%|█▋        | 18/108 [00:03<00:18,  4.98it/s] 18%|█▊        | 19/108 [00:03<00:17,  4.97it/s] 19%|█▊        | 20/108 [00:03<00:17,  4.98it/s] 19%|█▉        | 21/108 [00:04<00:17,  4.98it/s] 20%|██        | 22/108 [00:04<00:17,  4.98it/s] 21%|██▏       | 23/108 [00:04<00:17,  4.98it/s] 22%|██▏       | 24/108 [00:04<00:16,  4.97it/s] 23%|██▎       | 25/108 [00:04<00:16,  4.98it/s] 24%|██▍       | 26/108 [00:05<00:16,  4.98it/s] 25%|██▌       | 27/108 [00:05<00:16,  4.98it/s] 26%|██▌       | 28/108 [00:05<00:16,  4.97it/s] 27%|██▋       | 29/108 [00:05<00:15,  4.97it/s] 28%|██▊       | 30/108 [00:05<00:15,  4.97it/s] 29%|██▊       | 31/108 [00:06<00:15,  4.97it/s] 30%|██▉       | 32/108 [00:06<00:15,  4.98it/s] 31%|███       | 33/108 [00:06<00:15,  4.99it/s] 31%|███▏      | 34/108 [00:06<00:14,  4.99it/s] 32%|███▏      | 35/108 [00:06<00:14,  4.98it/s] 33%|███▎      | 36/108 [00:07<00:14,  4.98it/s] 34%|███▍      | 37/108 [00:07<00:14,  4.97it/s] 35%|███▌      | 38/108 [00:07<00:14,  4.97it/s] 36%|███▌      | 39/108 [00:07<00:13,  4.98it/s] 37%|███▋      | 40/108 [00:07<00:13,  4.98it/s] 38%|███▊      | 41/108 [00:08<00:13,  4.98it/s] 39%|███▉      | 42/108 [00:08<00:13,  4.97it/s] 40%|███▉      | 43/108 [00:08<00:13,  4.97it/s] 41%|████      | 44/108 [00:08<00:12,  4.97it/s] 42%|████▏     | 45/108 [00:08<00:12,  4.96it/s] 43%|████▎     | 46/108 [00:09<00:12,  4.97it/s] 44%|████▎     | 47/108 [00:09<00:12,  4.97it/s] 44%|████▍     | 48/108 [00:09<00:12,  4.97it/s] 45%|████▌     | 49/108 [00:09<00:11,  4.97it/s] 46%|████▋     | 50/108 [00:09<00:11,  4.97it/s] 47%|████▋     | 51/108 [00:10<00:11,  4.96it/s] 48%|████▊     | 52/108 [00:10<00:11,  4.96it/s] 49%|████▉     | 53/108 [00:10<00:11,  4.96it/s] 50%|█████     | 54/108 [00:10<00:10,  4.97it/s] 51%|█████     | 55/108 [00:10<00:10,  4.96it/s] 52%|█████▏    | 56/108 [00:11<00:10,  4.96it/s] 53%|█████▎    | 57/108 [00:11<00:10,  4.97it/s] 54%|█████▎    | 58/108 [00:11<00:10,  4.97it/s] 55%|█████▍    | 59/108 [00:11<00:09,  4.97it/s] 56%|█████▌    | 60/108 [00:11<00:09,  4.98it/s] 56%|█████▋    | 61/108 [00:12<00:09,  4.98it/s] 57%|█████▋    | 62/108 [00:12<00:09,  4.98it/s] 58%|█████▊    | 63/108 [00:12<00:09,  4.98it/s] 59%|█████▉    | 64/108 [00:12<00:08,  4.97it/s] 60%|██████    | 65/108 [00:12<00:08,  4.97it/s] 61%|██████    | 66/108 [00:13<00:08,  4.97it/s] 62%|██████▏   | 67/108 [00:13<00:08,  4.97it/s] 63%|██████▎   | 68/108 [00:13<00:08,  4.96it/s] 64%|██████▍   | 69/108 [00:13<00:07,  4.97it/s] 65%|██████▍   | 70/108 [00:13<00:07,  4.97it/s] 66%|██████▌   | 71/108 [00:14<00:07,  4.96it/s] 67%|██████▋   | 72/108 [00:14<00:07,  4.96it/s] 68%|██████▊   | 73/108 [00:14<00:07,  4.95it/s] 69%|██████▊   | 74/108 [00:14<00:06,  4.95it/s] 69%|██████▉   | 75/108 [00:14<00:06,  4.95it/s] 70%|███████   | 76/108 [00:15<00:06,  4.95it/s] 71%|███████▏  | 77/108 [00:15<00:06,  4.96it/s] 72%|███████▏  | 78/108 [00:15<00:06,  4.96it/s] 73%|███████▎  | 79/108 [00:15<00:05,  4.98it/s] 74%|███████▍  | 80/108 [00:15<00:05,  4.98it/s] 75%|███████▌  | 81/108 [00:16<00:05,  4.98it/s] 76%|███████▌  | 82/108 [00:16<00:05,  4.97it/s] 77%|███████▋  | 83/108 [00:16<00:05,  4.97it/s] 78%|███████▊  | 84/108 [00:16<00:04,  4.97it/s] 79%|███████▊  | 85/108 [00:16<00:04,  4.97it/s] 80%|███████▉  | 86/108 [00:17<00:04,  4.97it/s] 81%|████████  | 87/108 [00:17<00:04,  4.97it/s] 81%|████████▏ | 88/108 [00:17<00:04,  4.97it/s] 82%|████████▏ | 89/108 [00:17<00:03,  4.97it/s] 83%|████████▎ | 90/108 [00:17<00:03,  4.97it/s] 84%|████████▍ | 91/108 [00:18<00:03,  4.97it/s] 85%|████████▌ | 92/108 [00:18<00:03,  4.97it/s] 86%|████████▌ | 93/108 [00:18<00:03,  4.97it/s] 87%|████████▋ | 94/108 [00:18<00:02,  4.97it/s] 88%|████████▊ | 95/108 [00:18<00:02,  4.98it/s] 89%|████████▉ | 96/108 [00:19<00:02,  4.97it/s] 90%|████████▉ | 97/108 [00:19<00:02,  4.97it/s] 91%|█████████ | 98/108 [00:19<00:02,  4.98it/s] 92%|█████████▏| 99/108 [00:19<00:01,  4.97it/s] 93%|█████████▎| 100/108 [00:19<00:01,  4.97it/s] 94%|█████████▎| 101/108 [00:20<00:01,  4.97it/s] 94%|█████████▍| 102/108 [00:20<00:01,  4.97it/s] 95%|█████████▌| 103/108 [00:20<00:01,  4.97it/s] 96%|█████████▋| 104/108 [00:20<00:00,  4.97it/s] 97%|█████████▋| 105/108 [00:20<00:00,  4.97it/s] 98%|█████████▊| 106/108 [00:21<00:00,  4.96it/s] 99%|█████████▉| 107/108 [00:21<00:00,  4.96it/s]100%|██████████| 108/108 [00:21<00:00,  5.00it/s]08/03/2023 14:34:09 - INFO - utils_qa_hf - Post-processing 400 example predictions split into 864 features.

  0%|          | 0/400 [00:00<?, ?it/s][A
  4%|▎         | 14/400 [00:00<00:02, 137.00it/s][A
  7%|▋         | 28/400 [00:00<00:02, 138.64it/s][A
 10%|█         | 42/400 [00:00<00:02, 138.89it/s][A
 14%|█▍        | 56/400 [00:00<00:02, 123.81it/s][A
 18%|█▊        | 71/400 [00:00<00:02, 132.18it/s][A
 21%|██▏       | 85/400 [00:00<00:02, 126.85it/s][A
 25%|██▍       | 99/400 [00:00<00:02, 129.84it/s][A
 28%|██▊       | 113/400 [00:00<00:02, 124.08it/s][A
 32%|███▏      | 126/400 [00:01<00:02, 108.21it/s][A
 35%|███▌      | 140/400 [00:01<00:02, 109.13it/s][A
 40%|███▉      | 158/400 [00:01<00:01, 125.47it/s][A
 43%|████▎     | 173/400 [00:01<00:01, 127.49it/s][A
 47%|████▋     | 187/400 [00:01<00:01, 125.60it/s][A
 50%|█████     | 201/400 [00:01<00:01, 124.98it/s][A
 54%|█████▍    | 215/400 [00:01<00:01, 127.20it/s][A
 57%|█████▋    | 228/400 [00:01<00:01, 122.71it/s][A
 60%|██████    | 242/400 [00:01<00:01, 126.74it/s][A
 64%|██████▍   | 257/400 [00:02<00:01, 132.52it/s][A
 68%|██████▊   | 271/400 [00:02<00:01, 117.99it/s][A
 72%|███████▏  | 286/400 [00:02<00:00, 126.03it/s][A
 75%|███████▌  | 300/400 [00:02<00:00, 129.69it/s][A
 79%|███████▉  | 317/400 [00:02<00:00, 139.41it/s][A
 83%|████████▎ | 332/400 [00:02<00:00, 141.03it/s][A
 87%|████████▋ | 348/400 [00:02<00:00, 145.35it/s][A
 91%|█████████▏| 365/400 [00:02<00:00, 118.79it/s][A
 95%|█████████▌| 381/400 [00:03<00:00, 127.50it/s][A
100%|█████████▉| 398/400 [00:03<00:00, 137.29it/s][A100%|██████████| 400/400 [00:03<00:00, 127.78it/s]
08/03/2023 14:34:12 - INFO - utils_qa_hf - Saving predictions to ../../output/bert_large/eval_predictions.json.
08/03/2023 14:34:12 - INFO - utils_qa_hf - Saving nbest_preds to ../../output/bert_large/eval_nbest_predictions.json.
100%|██████████| 108/108 [00:27<00:00,  3.93it/s]
[INFO|modelcard.py:452] 2023-08-03 14:34:15,164 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Question Answering', 'type': 'question-answering'}}
***** eval metrics *****
  epoch                   =        8.0
  eval_exact_match        =       25.5
  eval_f1                 =    40.7097
  eval_runtime            = 0:00:21.72
  eval_samples            =        864
  eval_samples_per_second =     39.765
  eval_steps_per_second   =      4.971
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:               eval/exact_match ▁
wandb:                        eval/f1 ▁
wandb:                   eval/runtime ▁
wandb:        eval/samples_per_second ▁
wandb:          eval/steps_per_second ▁
wandb:                    train/epoch ▁▁
wandb:              train/global_step ▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:               eval/exact_match 25.5
wandb:                        eval/f1 40.70967
wandb:                   eval/runtime 21.7276
wandb:        eval/samples_per_second 39.765
wandb:          eval/steps_per_second 4.971
wandb:                    train/epoch 8.0
wandb:              train/global_step 4416
wandb:               train/total_flos 4.921404092684698e+16
wandb:               train/train_loss 0.00316
wandb:            train/train_runtime 378.5606
wandb: train/train_samples_per_second 139.983
wandb:   train/train_steps_per_second 11.665
wandb: 
wandb: 🚀 View run gallant-flower-16 at: https://wandb.ai/ym_k/huggingface/runs/z1jxk87w
wandb: ️⚡ View job at https://wandb.ai/ym_k/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg2OTkzNTMx/version_details/v5
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230803_142712-z1jxk87w/logs
08/03/2023 14:34:29 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False
08/03/2023 14:34:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=True,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=../../results/bert_large/runs/Aug03_14-34-29_gpu-pr1-02,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=../../results/bert_large,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=../../results/bert_large,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
[INFO|configuration_utils.py:713] 2023-08-03 14:34:29,686 >> loading configuration file ../../output/bert_large/checkpoint-4000/config.json
[INFO|configuration_utils.py:771] 2023-08-03 14:34:29,696 >> Model config BertConfig {
  "_name_or_path": "../../output/bert_large/checkpoint-4000",
  "architectures": [
    "BertForQuestionAnswering"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.32.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1842] 2023-08-03 14:34:29,700 >> loading file vocab.txt
[INFO|tokenization_utils_base.py:1842] 2023-08-03 14:34:29,700 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:1842] 2023-08-03 14:34:29,700 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1842] 2023-08-03 14:34:29,700 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1842] 2023-08-03 14:34:29,700 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:2635] 2023-08-03 14:34:29,813 >> loading weights file ../../output/bert_large/checkpoint-4000/pytorch_model.bin
[INFO|modeling_utils.py:3370] 2023-08-03 14:34:32,331 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.

[INFO|modeling_utils.py:3378] 2023-08-03 14:34:32,331 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at ../../output/bert_large/checkpoint-4000.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.
{'id': 10, 'title': 'This may be the most brutal number in the CBO report', 'question': 'Analysis: This may be the most brutal number in the CBO report', 'context': 'This may be the most brutal number in the CBO report - Plenty has been made of the big Congressional Budget Office finding that 24 million people could lose their insurance under Republicans\' Obamacare replacement over the next decade. That\'s higher than expected and poses a clear and massive hurdle for Republicans as they attempt to convince dozens of skeptical members. But there\'s another number that paints a particularly dire picture for the GOP\'s alternative — especially in light of President Trump\'s populist rhetoric. According to the CBO, 64-year olds making $26,500 per year would see their premiums increase by an estimated 750 percent by 2026. While they are on track to pay $1,700 under the current law, the CBO projects the American Health Care Act would force them to pay $14,600. Even if you grant that inflation will allow them to make slightly more money by 2026, that\'s still about half of their income going to health care. Here\'s how that looks as a percentage of income (h/t Philip Bump): As skeptics of the law noted, that suggests the reason premiums as a whole will eventually decline is because older, poorer people simply won\'t be able to afford it. The legislation reduces premiums substantially for younger people but increases them substantially for older people — and especially poorer, older people — according to the CBO. The CBO\'s estimate of the premium increases for older, poorer Americans is actually worse than a previous one from AARP. Here\'s what AARP, which has announced its opposition to the bill, said last week: ... Our estimates find that, taken together, premiums for older adults could increase by as much as $3,600 for a 55-year old earning $25,000 a year, $7,000 for a 64-year old earning $25,000 a year and up to $8,400 for a 64-year old earning $15,000 a year. If you\'re a Republican looking at these numbers, you have to be concerned — just from a self-preservation standpoint. Republican leaders have made great pains to argue that the CBO\'s estimate of the millions who will lose insurance is faulty, or even that it\'s a necessary side effect of returning to a more free-market approach to health care. But the GOP\'s counter-argument is predicated on the idea that its alternative would at least allow people access to coverage. Paying such a substantial portion of one\'s income on health insurance doesn\'t meet that goal — if, in fact, the CBO\'s estimate is anywhere close to accurate. It also affects a group of voters who are integral to the Trump Coalition. Less-formally educated and lower-income white Americans were the backbone of the electoral shift that allowed President Trump to be elected, and he has promised them the world: Coverage that is even more affordable than the Affordable Care Act and "insurance for everybody." On top of all that, older people are much more likely to vote than younger people, especially in a midterm election like 2018. So basically, Trump\'s win was built on older, poorer people, for whom this law appears to drive up premiums and drive down the insurance rate, while benefiting the younger and wealthier. That\'s not something that will assure skeptical Republicans at all — even if they can get past that 24 million number.', 'answers': {'answer_start': [638], 'text': ['750 percent']}}
Running tokenizer on prediction dataset:   0%|          | 0/400 [00:00<?, ? examples/s]Running tokenizer on prediction dataset: 100%|██████████| 400/400 [00:00<00:00, 461.30 examples/s]                                                                                                  08/03/2023 14:34:34 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:749] 2023-08-03 14:34:34,704 >> The following columns in the test set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.
[INFO|trainer.py:3083] 2023-08-03 14:34:34,709 >> ***** Running Prediction *****
[INFO|trainer.py:3085] 2023-08-03 14:34:34,709 >>   Num examples = 851
[INFO|trainer.py:3088] 2023-08-03 14:34:34,709 >>   Batch size = 8
  0%|          | 0/107 [00:00<?, ?it/s]  2%|▏         | 2/107 [00:00<00:10, 10.00it/s]  3%|▎         | 3/107 [00:00<00:14,  7.07it/s]  4%|▎         | 4/107 [00:00<00:16,  6.12it/s]  5%|▍         | 5/107 [00:00<00:18,  5.67it/s]  6%|▌         | 6/107 [00:01<00:18,  5.43it/s]  7%|▋         | 7/107 [00:01<00:18,  5.28it/s]  7%|▋         | 8/107 [00:01<00:19,  5.19it/s]  8%|▊         | 9/107 [00:01<00:19,  5.13it/s]  9%|▉         | 10/107 [00:01<00:19,  5.10it/s] 10%|█         | 11/107 [00:01<00:18,  5.07it/s] 11%|█         | 12/107 [00:02<00:18,  5.04it/s] 12%|█▏        | 13/107 [00:02<00:18,  5.03it/s] 13%|█▎        | 14/107 [00:02<00:18,  5.02it/s] 14%|█▍        | 15/107 [00:02<00:18,  5.02it/s] 15%|█▍        | 16/107 [00:02<00:18,  5.01it/s] 16%|█▌        | 17/107 [00:03<00:17,  5.01it/s] 17%|█▋        | 18/107 [00:03<00:17,  5.01it/s] 18%|█▊        | 19/107 [00:03<00:17,  5.01it/s] 19%|█▊        | 20/107 [00:03<00:17,  5.01it/s] 20%|█▉        | 21/107 [00:03<00:17,  5.01it/s] 21%|██        | 22/107 [00:04<00:16,  5.00it/s] 21%|██▏       | 23/107 [00:04<00:16,  5.00it/s] 22%|██▏       | 24/107 [00:04<00:16,  5.00it/s] 23%|██▎       | 25/107 [00:04<00:16,  5.00it/s] 24%|██▍       | 26/107 [00:04<00:16,  5.00it/s] 25%|██▌       | 27/107 [00:05<00:15,  5.00it/s] 26%|██▌       | 28/107 [00:05<00:15,  5.00it/s] 27%|██▋       | 29/107 [00:05<00:15,  5.00it/s] 28%|██▊       | 30/107 [00:05<00:15,  5.00it/s] 29%|██▉       | 31/107 [00:05<00:15,  5.01it/s] 30%|██▉       | 32/107 [00:06<00:14,  5.00it/s] 31%|███       | 33/107 [00:06<00:14,  5.00it/s] 32%|███▏      | 34/107 [00:06<00:14,  5.00it/s] 33%|███▎      | 35/107 [00:06<00:14,  5.00it/s] 34%|███▎      | 36/107 [00:06<00:14,  5.00it/s] 35%|███▍      | 37/107 [00:07<00:13,  5.01it/s] 36%|███▌      | 38/107 [00:07<00:13,  5.00it/s] 36%|███▋      | 39/107 [00:07<00:13,  5.00it/s] 37%|███▋      | 40/107 [00:07<00:13,  5.00it/s] 38%|███▊      | 41/107 [00:07<00:13,  4.99it/s] 39%|███▉      | 42/107 [00:08<00:13,  4.99it/s] 40%|████      | 43/107 [00:08<00:12,  5.00it/s] 41%|████      | 44/107 [00:08<00:12,  5.00it/s] 42%|████▏     | 45/107 [00:08<00:12,  5.00it/s] 43%|████▎     | 46/107 [00:08<00:12,  5.00it/s] 44%|████▍     | 47/107 [00:09<00:11,  5.00it/s] 45%|████▍     | 48/107 [00:09<00:11,  5.00it/s] 46%|████▌     | 49/107 [00:09<00:11,  5.00it/s] 47%|████▋     | 50/107 [00:09<00:11,  4.99it/s] 48%|████▊     | 51/107 [00:09<00:11,  4.98it/s] 49%|████▊     | 52/107 [00:10<00:11,  4.98it/s] 50%|████▉     | 53/107 [00:10<00:10,  4.98it/s] 50%|█████     | 54/107 [00:10<00:10,  4.97it/s] 51%|█████▏    | 55/107 [00:10<00:10,  4.98it/s] 52%|█████▏    | 56/107 [00:11<00:10,  4.98it/s] 53%|█████▎    | 57/107 [00:11<00:10,  4.98it/s] 54%|█████▍    | 58/107 [00:11<00:09,  4.98it/s] 55%|█████▌    | 59/107 [00:11<00:09,  4.99it/s] 56%|█████▌    | 60/107 [00:11<00:09,  4.99it/s] 57%|█████▋    | 61/107 [00:12<00:09,  5.00it/s] 58%|█████▊    | 62/107 [00:12<00:09,  4.99it/s] 59%|█████▉    | 63/107 [00:12<00:08,  4.99it/s] 60%|█████▉    | 64/107 [00:12<00:08,  4.99it/s] 61%|██████    | 65/107 [00:12<00:08,  4.99it/s] 62%|██████▏   | 66/107 [00:13<00:08,  5.00it/s] 63%|██████▎   | 67/107 [00:13<00:08,  4.99it/s] 64%|██████▎   | 68/107 [00:13<00:07,  4.98it/s] 64%|██████▍   | 69/107 [00:13<00:07,  4.98it/s] 65%|██████▌   | 70/107 [00:13<00:07,  4.98it/s] 66%|██████▋   | 71/107 [00:14<00:07,  4.98it/s] 67%|██████▋   | 72/107 [00:14<00:07,  4.97it/s] 68%|██████▊   | 73/107 [00:14<00:06,  4.97it/s] 69%|██████▉   | 74/107 [00:14<00:06,  4.98it/s] 70%|███████   | 75/107 [00:14<00:06,  4.98it/s] 71%|███████   | 76/107 [00:15<00:06,  4.99it/s] 72%|███████▏  | 77/107 [00:15<00:06,  4.99it/s] 73%|███████▎  | 78/107 [00:15<00:05,  4.99it/s] 74%|███████▍  | 79/107 [00:15<00:05,  5.00it/s] 75%|███████▍  | 80/107 [00:15<00:05,  4.99it/s] 76%|███████▌  | 81/107 [00:16<00:05,  4.98it/s] 77%|███████▋  | 82/107 [00:16<00:05,  4.98it/s] 78%|███████▊  | 83/107 [00:16<00:04,  4.98it/s] 79%|███████▊  | 84/107 [00:16<00:04,  4.99it/s] 79%|███████▉  | 85/107 [00:16<00:04,  4.99it/s] 80%|████████  | 86/107 [00:17<00:04,  4.98it/s] 81%|████████▏ | 87/107 [00:17<00:04,  4.98it/s] 82%|████████▏ | 88/107 [00:17<00:03,  4.97it/s] 83%|████████▎ | 89/107 [00:17<00:03,  4.97it/s] 84%|████████▍ | 90/107 [00:17<00:03,  4.97it/s] 85%|████████▌ | 91/107 [00:18<00:03,  4.97it/s] 86%|████████▌ | 92/107 [00:18<00:03,  4.98it/s] 87%|████████▋ | 93/107 [00:18<00:03,  4.55it/s] 88%|████████▊ | 94/107 [00:18<00:02,  4.67it/s] 89%|████████▉ | 95/107 [00:19<00:04,  2.54it/s] 90%|████████▉ | 96/107 [00:19<00:03,  2.98it/s] 91%|█████████ | 97/107 [00:19<00:02,  3.39it/s] 92%|█████████▏| 98/107 [00:20<00:02,  3.75it/s] 93%|█████████▎| 99/107 [00:20<00:01,  4.05it/s] 93%|█████████▎| 100/107 [00:20<00:01,  4.29it/s] 94%|█████████▍| 101/107 [00:20<00:01,  4.47it/s] 95%|█████████▌| 102/107 [00:20<00:01,  4.61it/s] 96%|█████████▋| 103/107 [00:21<00:00,  4.71it/s] 97%|█████████▋| 104/107 [00:21<00:00,  4.78it/s] 98%|█████████▊| 105/107 [00:21<00:00,  4.85it/s] 99%|█████████▉| 106/107 [00:21<00:00,  4.90it/s]08/03/2023 14:34:58 - INFO - utils_qa_hf - Post-processing 400 example predictions split into 851 features.

  0%|          | 0/400 [00:00<?, ?it/s][A
  4%|▍         | 15/400 [00:00<00:02, 141.23it/s][A
  8%|▊         | 30/400 [00:00<00:02, 129.19it/s][A
 11%|█         | 43/400 [00:00<00:02, 128.73it/s][A
 14%|█▍        | 56/400 [00:00<00:03, 111.47it/s][A
 17%|█▋        | 69/400 [00:00<00:02, 116.13it/s][A
 20%|██        | 81/400 [00:00<00:03, 96.27it/s] [A
 24%|██▎       | 94/400 [00:00<00:02, 104.84it/s][A
 27%|██▋       | 108/400 [00:00<00:02, 113.81it/s][A
 30%|███       | 120/400 [00:01<00:02, 110.09it/s][A
 35%|███▌      | 141/400 [00:01<00:01, 136.42it/s][A
 39%|███▉      | 156/400 [00:01<00:01, 137.37it/s][A
 43%|████▎     | 171/400 [00:01<00:01, 134.44it/s][A
 47%|████▋     | 187/400 [00:01<00:01, 132.26it/s][A
 50%|█████     | 201/400 [00:01<00:01, 133.65it/s][A
 54%|█████▍    | 215/400 [00:01<00:01, 123.97it/s][A
 57%|█████▊    | 230/400 [00:01<00:01, 128.78it/s][A
 62%|██████▏   | 247/400 [00:01<00:01, 139.90it/s][A
 66%|██████▌   | 262/400 [00:02<00:01, 120.14it/s][A
 69%|██████▉   | 275/400 [00:02<00:01, 110.06it/s][A
 72%|███████▏  | 287/400 [00:02<00:01, 99.01it/s] [A
 74%|███████▍  | 298/400 [00:02<00:01, 101.06it/s][A
 79%|███████▉  | 315/400 [00:02<00:00, 117.58it/s][A
 82%|████████▏ | 329/400 [00:02<00:00, 115.75it/s][A
 85%|████████▌ | 341/400 [00:02<00:00, 114.11it/s][A
 90%|█████████ | 361/400 [00:02<00:00, 135.13it/s][A
 95%|█████████▌| 381/400 [00:03<00:00, 152.30it/s][A
 99%|█████████▉| 397/400 [00:03<00:00, 131.96it/s][A100%|██████████| 400/400 [00:03<00:00, 122.22it/s]
08/03/2023 14:35:02 - INFO - utils_qa_hf - Saving predictions to ../../results/bert_large/predict_predictions.json.
08/03/2023 14:35:02 - INFO - utils_qa_hf - Saving nbest_preds to ../../results/bert_large/predict_nbest_predictions.json.
[INFO|modelcard.py:452] 2023-08-03 14:35:02,254 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Question Answering', 'type': 'question-answering'}}
100%|██████████| 107/107 [00:26<00:00,  4.01it/s]
***** predict metrics *****
  predict_samples         =        851
  test_exact_match        =        4.0
  test_f1                 =        0.0
  test_runtime            = 0:00:22.67
  test_samples_per_second =     37.524
  test_steps_per_second   =      4.718
