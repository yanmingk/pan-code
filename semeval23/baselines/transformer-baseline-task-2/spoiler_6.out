/var/spool/slurmd/job00006/slurm_script: line 15: nvidia-smi: command not found
Found existing installation: transformers 4.32.0.dev0
Uninstalling transformers-4.32.0.dev0:
  Would remove:
    /mnt/hpc/work/y53kang/.conda/envs/spoiler/bin/transformers-cli
    /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/transformers-4.32.0.dev0.dist-info/*
    /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/transformers/*
Proceed (Y/n)? ERROR: Exception:
Traceback (most recent call last):
  File "/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/pip/_internal/cli/base_command.py", line 169, in exc_logging_wrapper
    status = run_func(*args)
  File "/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/pip/_internal/commands/uninstall.py", line 105, in run
    uninstall_pathset = req.uninstall(
  File "/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/pip/_internal/req/req_install.py", line 680, in uninstall
    uninstalled_pathset.remove(auto_confirm, verbose)
  File "/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/pip/_internal/req/req_uninstall.py", line 375, in remove
    if auto_confirm or self._allowed_to_proceed(verbose):
  File "/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/pip/_internal/req/req_uninstall.py", line 415, in _allowed_to_proceed
    return ask("Proceed (Y/n)? ", ("y", "n", "")) != "n"
  File "/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages/pip/_internal/utils/misc.py", line 192, in ask
    response = input(message)
EOFError: EOF when reading a line
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-6o1eyrnt
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-6o1eyrnt
  Resolved https://github.com/huggingface/transformers to commit d27e4c18fe2970abcb9a48dcb8a824e48083b15f
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: filelock in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (3.12.2)
Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (0.15.1)
Requirement already satisfied: numpy>=1.17 in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (6.0)
Requirement already satisfied: regex!=2019.12.17 in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (2023.6.3)
Requirement already satisfied: requests in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (2.31.0)
Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (0.13.3)
Requirement already satisfied: safetensors>=0.3.1 in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (0.3.1)
Requirement already satisfied: tqdm>=4.27 in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (4.65.0)
Requirement already satisfied: fsspec in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.32.0.dev0) (2023.6.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.32.0.dev0) (4.6.3)
Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0) (3.1.0)
Requirement already satisfied: idna<4,>=2.5 in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /mnt/hpc/work/y53kang/.conda/envs/spoiler/lib/python3.10/site-packages (from requests->transformers==4.32.0.dev0) (2023.5.7)
08/01/2023 21:42:41 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False
08/01/2023 21:42:41 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=../../output/bert_large/runs/Aug01_21-42-40_gpu-pr1-02,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=8.0,
optim=adamw_torch,
optim_args=None,
output_dir=../../output/bert_large,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=12,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=../../output/bert_large,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571/571 [00:00<00:00, 3.57MB/s]
[INFO|configuration_utils.py:715] 2023-08-01 21:42:41,737 >> loading configuration file config.json from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json
[INFO|configuration_utils.py:771] 2023-08-01 21:42:41,748 >> Model config BertConfig {
  "_name_or_path": "bert-large-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.32.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]Downloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.0/28.0 [00:00<00:00, 210kB/s]
[INFO|configuration_utils.py:715] 2023-08-01 21:42:41,847 >> loading configuration file config.json from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json
[INFO|configuration_utils.py:771] 2023-08-01 21:42:41,848 >> Model config BertConfig {
  "_name_or_path": "bert-large-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.32.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading (‚Ä¶)solve/main/vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00<00:00, 9.52MB/s]
Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]Downloading (‚Ä¶)/main/tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 466k/466k [00:00<00:00, 11.5MB/s]
[INFO|tokenization_utils_base.py:1844] 2023-08-01 21:42:42,303 >> loading file vocab.txt from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/vocab.txt
[INFO|tokenization_utils_base.py:1844] 2023-08-01 21:42:42,304 >> loading file tokenizer.json from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/tokenizer.json
[INFO|tokenization_utils_base.py:1844] 2023-08-01 21:42:42,304 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1844] 2023-08-01 21:42:42,304 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1844] 2023-08-01 21:42:42,304 >> loading file tokenizer_config.json from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/tokenizer_config.json
[INFO|configuration_utils.py:715] 2023-08-01 21:42:42,307 >> loading configuration file config.json from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/config.json
[INFO|configuration_utils.py:771] 2023-08-01 21:42:42,307 >> Model config BertConfig {
  "_name_or_path": "bert-large-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.32.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

Downloading model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]Downloading model.safetensors:   1%|          | 10.5M/1.34G [00:00<00:14, 95.3MB/s]Downloading model.safetensors:   2%|‚ñè         | 31.5M/1.34G [00:00<00:12, 105MB/s] Downloading model.safetensors:   4%|‚ñç         | 52.4M/1.34G [00:00<00:11, 112MB/s]Downloading model.safetensors:   5%|‚ñå         | 73.4M/1.34G [00:00<00:11, 114MB/s]Downloading model.safetensors:   7%|‚ñã         | 94.4M/1.34G [00:00<00:11, 113MB/s]Downloading model.safetensors:   9%|‚ñä         | 115M/1.34G [00:01<00:10, 116MB/s] Downloading model.safetensors:  10%|‚ñà         | 136M/1.34G [00:01<00:10, 115MB/s]Downloading model.safetensors:  12%|‚ñà‚ñè        | 157M/1.34G [00:01<00:10, 116MB/s]Downloading model.safetensors:  13%|‚ñà‚ñé        | 178M/1.34G [00:01<00:10, 116MB/s]Downloading model.safetensors:  15%|‚ñà‚ñç        | 199M/1.34G [00:01<00:09, 116MB/s]Downloading model.safetensors:  16%|‚ñà‚ñã        | 220M/1.34G [00:01<00:09, 116MB/s]Downloading model.safetensors:  18%|‚ñà‚ñä        | 241M/1.34G [00:02<00:09, 115MB/s]Downloading model.safetensors:  19%|‚ñà‚ñâ        | 262M/1.34G [00:02<00:09, 116MB/s]Downloading model.safetensors:  21%|‚ñà‚ñà        | 283M/1.34G [00:02<00:09, 117MB/s]Downloading model.safetensors:  23%|‚ñà‚ñà‚ñé       | 304M/1.34G [00:02<00:08, 116MB/s]Downloading model.safetensors:  24%|‚ñà‚ñà‚ñç       | 325M/1.34G [00:02<00:08, 117MB/s]Downloading model.safetensors:  26%|‚ñà‚ñà‚ñå       | 346M/1.34G [00:03<00:08, 117MB/s]Downloading model.safetensors:  27%|‚ñà‚ñà‚ñã       | 367M/1.34G [00:03<00:08, 115MB/s]Downloading model.safetensors:  29%|‚ñà‚ñà‚ñâ       | 388M/1.34G [00:03<00:08, 115MB/s]Downloading model.safetensors:  30%|‚ñà‚ñà‚ñà       | 409M/1.34G [00:03<00:08, 116MB/s]Downloading model.safetensors:  32%|‚ñà‚ñà‚ñà‚ñè      | 430M/1.34G [00:03<00:08, 114MB/s]Downloading model.safetensors:  34%|‚ñà‚ñà‚ñà‚ñé      | 451M/1.34G [00:03<00:07, 116MB/s]Downloading model.safetensors:  35%|‚ñà‚ñà‚ñà‚ñå      | 472M/1.34G [00:04<00:07, 114MB/s]Downloading model.safetensors:  37%|‚ñà‚ñà‚ñà‚ñã      | 493M/1.34G [00:04<00:07, 116MB/s]Downloading model.safetensors:  38%|‚ñà‚ñà‚ñà‚ñä      | 514M/1.34G [00:04<00:07, 115MB/s]Downloading model.safetensors:  40%|‚ñà‚ñà‚ñà‚ñâ      | 535M/1.34G [00:04<00:07, 114MB/s]Downloading model.safetensors:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 556M/1.34G [00:04<00:06, 115MB/s]Downloading model.safetensors:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 577M/1.34G [00:05<00:06, 116MB/s]Downloading model.safetensors:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 598M/1.34G [00:05<00:06, 116MB/s]Downloading model.safetensors:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 619M/1.34G [00:05<00:06, 115MB/s]Downloading model.safetensors:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 640M/1.34G [00:05<00:06, 116MB/s]Downloading model.safetensors:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 661M/1.34G [00:05<00:06, 113MB/s]Downloading model.safetensors:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 682M/1.34G [00:05<00:05, 114MB/s]Downloading model.safetensors:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 703M/1.34G [00:06<00:05, 113MB/s]Downloading model.safetensors:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 724M/1.34G [00:06<00:05, 114MB/s]Downloading model.safetensors:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 744M/1.34G [00:06<00:05, 115MB/s]Downloading model.safetensors:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 765M/1.34G [00:06<00:05, 116MB/s]Downloading model.safetensors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 786M/1.34G [00:06<00:04, 116MB/s]Downloading model.safetensors:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 807M/1.34G [00:07<00:04, 117MB/s]Downloading model.safetensors:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 828M/1.34G [00:07<00:04, 117MB/s]Downloading model.safetensors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 849M/1.34G [00:07<00:04, 117MB/s]Downloading model.safetensors:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 870M/1.34G [00:07<00:04, 117MB/s]Downloading model.safetensors:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 891M/1.34G [00:07<00:03, 117MB/s]Downloading model.safetensors:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 912M/1.34G [00:07<00:03, 116MB/s]Downloading model.safetensors:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 933M/1.34G [00:08<00:03, 117MB/s]Downloading model.safetensors:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 954M/1.34G [00:08<00:03, 117MB/s]Downloading model.safetensors:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 975M/1.34G [00:08<00:03, 115MB/s]Downloading model.safetensors:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 996M/1.34G [00:08<00:02, 117MB/s]Downloading model.safetensors:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1.02G/1.34G [00:08<00:02, 116MB/s]Downloading model.safetensors:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1.04G/1.34G [00:08<00:02, 117MB/s]Downloading model.safetensors:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1.06G/1.34G [00:09<00:02, 117MB/s]Downloading model.safetensors:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1.08G/1.34G [00:09<00:02, 117MB/s]Downloading model.safetensors:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1.10G/1.34G [00:09<00:02, 117MB/s]Downloading model.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 1.12G/1.34G [00:09<00:01, 117MB/s]Downloading model.safetensors:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1.14G/1.34G [00:09<00:01, 115MB/s]Downloading model.safetensors:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 1.16G/1.34G [00:10<00:01, 107MB/s]Downloading model.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1.18G/1.34G [00:10<00:02, 79.9MB/s]Downloading model.safetensors:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 1.21G/1.34G [00:10<00:01, 89.8MB/s]Downloading model.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 1.23G/1.34G [00:10<00:01, 96.1MB/s]Downloading model.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1.25G/1.34G [00:11<00:00, 101MB/s] Downloading model.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 1.27G/1.34G [00:11<00:00, 104MB/s]Downloading model.safetensors:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 1.29G/1.34G [00:11<00:00, 108MB/s]Downloading model.safetensors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 1.31G/1.34G [00:11<00:00, 108MB/s]Downloading model.safetensors:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1.33G/1.34G [00:11<00:00, 111MB/s]Downloading model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.34G/1.34G [00:11<00:00, 111MB/s]Downloading model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.34G/1.34G [00:11<00:00, 113MB/s]
[INFO|modeling_utils.py:2638] 2023-08-01 21:43:02,139 >> loading weights file model.safetensors from cache at /work/y53kang/.cache/huggingface/hub/models--bert-large-uncased/snapshots/80792f8e8216b29f3c846b653a0ff0a37c210431/model.safetensors
[INFO|modeling_utils.py:3360] 2023-08-01 21:43:16,246 >> Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:3372] 2023-08-01 21:43:16,247 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on train dataset:   0%|          | 0/3200 [00:00<?, ? examples/s]Running tokenizer on train dataset:  31%|‚ñà‚ñà‚ñà‚ñè      | 1000/3200 [00:01<00:03, 551.82 examples/s]Running tokenizer on train dataset:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2000/3200 [00:03<00:02, 532.10 examples/s]Running tokenizer on train dataset:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3000/3200 [00:05<00:00, 559.41 examples/s]Running tokenizer on train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3200/3200 [00:05<00:00, 567.08 examples/s]                                                                                               Running tokenizer on validation dataset:   0%|          | 0/400 [00:00<?, ? examples/s]Running tokenizer on validation dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:00<00:00, 473.79 examples/s]                                                                                                  Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.53k/4.53k [00:00<00:00, 36.2MB/s]
Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]Downloading extra modules: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.32k/3.32k [00:00<00:00, 30.7MB/s]
[INFO|trainer.py:1681] 2023-08-01 21:43:23,705 >> ***** Running training *****
[INFO|trainer.py:1682] 2023-08-01 21:43:23,705 >>   Num examples = 6,624
[INFO|trainer.py:1683] 2023-08-01 21:43:23,705 >>   Num Epochs = 8
[INFO|trainer.py:1684] 2023-08-01 21:43:23,705 >>   Instantaneous batch size per device = 12
[INFO|trainer.py:1687] 2023-08-01 21:43:23,705 >>   Total train batch size (w. parallel, distributed & accumulation) = 12
[INFO|trainer.py:1688] 2023-08-01 21:43:23,705 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1689] 2023-08-01 21:43:23,705 >>   Total optimization steps = 4,416
[INFO|trainer.py:1690] 2023-08-01 21:43:23,706 >>   Number of trainable parameters = 334,094,338
[INFO|integrations.py:716] 2023-08-01 21:43:23,713 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: ym_k. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.7
wandb: Run data is saved locally in /mnt/hpc/work/y53kang/pan-code/semeval23/baselines/transformer-baseline-task-2/wandb/run-20230801_214325-zet5e9ey
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-shadow-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ym_k/huggingface
wandb: üöÄ View run at https://wandb.ai/ym_k/huggingface/runs/zet5e9ey
  0%|          | 0/4416 [00:00<?, ?it/s]  0%|          | 1/4416 [02:34<189:29:56, 154.52s/it]slurmstepd: error: *** JOB 6 ON gpu-pr1-02 CANCELLED AT 2023-08-01T21:46:40 ***
